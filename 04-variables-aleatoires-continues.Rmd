# Variables aléatoires continues

## densités de probabilités

```{definition}
Une variable aléatoire réelle $X$ est dite **continue** s'il existe une fonction f positive telle que 
$$F_X(x) = \int_{-\infty}^x f(t) \mbox{d}t$$
Une telle fonction est une **densité** de $X$. Cette densité n'est pas unique.
Une telle densité est généralement notée $f_X$.

Lorsque $F_X$ est dérivable, $F_X'$ est une densité de $X$. 
```

```{lemma}
Si $X$ admet $f$ comme densité, alors:
  
- $\int_{-\infty}^{+\infty} f(x) \mbox{d}x = 1$;

- $\mathbb{P}(X=x)=0, \forall x \in \mathbb{R}$;

- $\mathbb{P}(X \in \mathcal{E}) = \int_{\mathcal{E}} f(x) \mbox{d}x$ pour tout évènement $\mathcal{E}$;

- $X$ et $-X$ ont les mêmes distributions si et seulement si $f_X$ est paire;

- Si $f$ et $g$ sont des densités, il en est de même pour $\lambda f + (1-\lambda)g$ pour $0 \leq \lambda \leq 1$.
```

```{example}
Soient $a,b$ deux réels avec $a<b$. La **loi uniforme** sur l'intervalle $[a,b]$ est définie par sa densité de probabilité
$$
f(x) = \left\lbrace
\begin{array}{ccc}
\frac{1}{b-a}  & \mbox{si} & a \leq x \leq b\\
0 & \mbox{sinon} &\\
\end{array}\right.
$$
On note cette loi $\mathcal{U}(a,b)$.
```

```{example}
La **loi exponentielle** modélise la durée de vie d'un phénomène sans mémoire : la probabilité que le phénomène dure au moins $s+t$ sachant qu'il a déjà duré $t$ est la même que la probabilié de durer $s$ à partir de la mise en fonction initiale.

Soit $\lambda > 0$. La densité associée à cette loi est :
 $$
f(t) = \left\lbrace
\begin{array}{ccc}
\lambda e^{-\lambda t}  & \mbox{si} & t>0\\
0 & \mbox{sinon} &\\
\end{array}\right.
$$
On note la loi exponentielle de paramètre $\lambda$ : $\mathcal{E}(\lambda)$.
```
```{lemma}
Soient $X,Y$ deux variables aléatoires qui suivent deux lois exponentielles, de paramètres $\lambda, \mu$, alors $\min (X,Y)$ suit encore une loi exponentielle, de paramètre $\lambda + \mu$.

```

```{example}
La loi **Gamma** (qui généralise la loi exponentielle) modélise également des durées; elle est caractérisée par deux paramètres $k,\theta$ positifs.

Sa densité est
$$ f(t) = \left\lbrace
\begin{array}{ccc}
\frac{t^{k-1} \exp \left(-\frac{t}{\theta} \right)}{\theta^k \Gamma (k)}   & \mbox{si} & x>0\\
0 & \mbox{sinon} & \\
\end{array}\right.
$$
où $\Gamma$ désigne la fonction Gamma d'Euler ($\Gamma : z \mapsto \int_0^{+\infty} t^{z-1} e^{-t} \mbox{d}t$).  

On note cette  loi $\Gamma(k,\theta)$. 
```

```{lemma}
Soient $X_1, \ldots, X_n$ des variables aléatoires indépendantes, de loi $\Gamma(k_1,\theta),\ldots,\Gamma(k_n,\theta)$. Alors $X_1+\ldots+X_n$ suit une loi $\Gamma(\sum_{i=1}^n k_i,\theta)$.
```

```{lemma}
Soit $X  \sim \Gamma(k,\theta)$ et $t>0$. Alors $tX \sim \Gamma(k,t \theta)$.
```


```{example}
La **loi normale** est définie suivant deux paramètres ($\mu \in \mathbb{R}, \sigma >0$) par sa densité:
  
  $$
  f(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)
  $$
  On note la loi $\mathcal{N}(\mu,\sigma^2)$.
```

```{example}
Soit $X$ une variable aléatoire qui suit une loi normale $\mathcal{N}(\mu,\sigma^2)$. Alors $\exp(X)$ suit une loi **log-normale**.
```

## Indépendance

```{definition}
Soient  $X,Y$ deux variables aléatoires réelles.
On dit que les variables aléatoires $X$ et $Y$ sont **indépendantes** si les évènements $\lbrace X  \leq x \rbrace$ et $\lbrace Y \leq y \rbrace$ sont indépendants pour tous $x,y$ réels.
```

```{theorem}
Soient $X,Y$ deux variables aléatoires, $f,g$ deux fonctions telles que $f(X), g(Y)$ soient des variables aléatoires.

Si $X$ et $Y$ sont indépendantes, il en est de même pour $f(X)$ et $g(Y)$.```

## Espérance

```{definition}
**L'espérance** d'une variable aléatoire $X$ de densité $f$ est définie par
$$\mathbb{E}X = \int_{-\infty}^{+\infty} x f(x) \mbox{d}x$$
  lorsque cette intégrale existe.
```
```{theorem}
Si $X$ et $g(X)$ sont des variables aléatoires, alors
$$\mathbb{E}g(X) = \int_{-\infty}^{+\infty} g(x) f(x) \mbox{d}x$$
```
```{lemma}
Si $X$ admet une densité nulle sur $\mathbb{R}^-$ et une fonction de répartition $F$, alors
$$\mathbb{E}X = \int_0^{+\infty} (1-F(x)) \mbox{d}x = \int_0^{+\infty} \mathbb{P}(X>x) \mbox{d}x$$
```
Les autres propriétés de l'espérance, vues dans le chapitre sur les variables aléatoires discrètes, s'étendent sans difficultés aux variables continues (moments, moments centrés, variance, covariance, corrélation).

## Exemples de variables aléatoires continues

```{theorem}
L'espérance et la variance des lois continues de référence sont recensés dans le tableau ci-après :

|Loi                        | Espérance        | Variance|
|:---------------------------:|:------------------:|:---------:|
|$\mathcal{U}(a,b)$         | $\frac{a+b}{2}$  |$\frac{(b-a)^2}{12}$|
|$\mathcal{E}(\lambda)$     |$\frac{1}{\lambda}$|$\frac{1}{\lambda^2}$|
|$\Gamma(k,\theta)$         |$k \theta$        |$k \theta^2$|
|$\mathcal{N}(\mu,\sigma^2)$|$\mu$             |$\sigma^2$|

```

## Dépendance

```{definition}
Soient $X,Y$ deux variables aléatoires réelles.  Ces variables sont **conjointement continues** s'il existe une fonction positive $f$ telle que:
  
  $$F(x,y) = \mathbb{P}(X \leq x , Y \leq y) = \int_{-\infty}^x \int_{-\infty}^y f(u,v) \mbox{d}u \mbox{d}v$$
  f est une densité du couple $(X,Y)$. Si $F$ est dérivable, on peut prendre
  $$f = \frac{\partial^2 F}{\partial x \partial y}$$
```

```{lemma}
On a
$$
  \mathbb{P}(a \leq X \leq b ; c \leq Y \leq d) = F(b,d) - F(a,d) - F(b,c)+F(a,c)
  = \int_{a}^b \int_{c}^d f(u,v) \mbox{d}u \mbox{d}v
$$
  Plus généralement, si $\mathcal{E}$ est un évènement, on a:
  
  $$
  \mathbb{P}\left( (X,Y) \in \mathcal{E} \right) = \iint_{\mathcal{E}} f(u,v) \mbox{d}u \mbox{d}v
  $$
```

```{definition}
Les **fonctions de répartition marginales** de $X$ et $Y$ sont définies par
$$
  F_X(x) = \mathbb{P}(X  \leq x) = \int_{-\infty}^x \int_{-\infty}^{+\infty} f(u,v) \mbox{d}u \mbox{d}v ; F_Y(y) = \mathbb{P}(Y  \leq y) = \int_{-\infty}^{+\infty} \int_{-\infty}^y  f(u,v) \mbox{d}u \mbox{d}v 
  $$
  On en déduit les **densités marginales** selon $X$ et $Y$ :
  $$
  f_X(x) = \int_{-\infty}^{+\infty} f(x,y) \mbox{d}y ; f_Y(y) = \int_{-\infty}^{+\infty} f(x,y) \mbox{d}x
  $$
```

```{theorem}
Soit $g : \mathbb{R}^2 \to \mathbb{R}$ telle que $g(X,Y)$ soit une variable aléatoire. Alors, 
$$
  \mathbb{E} g(X,Y) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(u,v) f(u,v) \mbox{d}u \mbox{d}v
  $$ dès que cette intégrale est définie.
```

```{theorem}
Les variables $X,Y$ sont indépendantes si et seulement si
$$F_{X,Y} = F_X F_Y \iff f_{X,Y} = f_x f_y.$$
```

```{theorem}
Soit $X,Y$ deux variables aléatoires continues. Alors 
$$\left( \mathbb{E}XY \right)^2 \leq \left( \mathbb{E} X^2\right) \left( \mathbb{E} Y^2\right)$$
  avec égalité si et seulement si il existe des réels $\lambda,\mu$ non tous nuls tels que 
$$\mathbb{P}(\lambda X + \mu Y = 0) = 1.$$

```
## Loi conditionnelle et espérance conditionnelle

Soient $X,Y$ deux variables aléatoires continues, de densité jointe $f$.
```{definition}
La **fonction de répartition conditionnelle** de $Y$ sachant $X=x$ est définie par

$$
F_{Y|X}(y|x) = \int_{-\infty}^y \frac{f(x,v)}{f_X(x)} \mbox{d}v  
$$  pour tout $x$ tel que $f_X(x)>0$. Elle est également notée
$$\mathbb{P}(Y \leq y | X = x)$$
```

```{definition}
La **densité conditionnelle** de $Y$ sachant $X=x$ est définie par

$$
f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)} 
$$  pour tout $x$ tel que $f_X(x)>0$. Encore,
$$
  f_{Y|X} = \frac{f_{X,Y}}{f_X} 
$$
```

```{theorem}
**L'espérance conditionnelle** de $Y$ sachant $X$ est définie comme
$$\mathbb{E}(Y|X=x) = \int_{-\infty}^{+\infty} y f_{Y|X}(y|x) \mbox{d}y ;$$
Elle vérifie la relation
$$
\mathbb{E} \left(\mathbb{E}(Y|X) \right) = \mathbb{E} Y .
$$
Plus généralement, pour toute fonction $g$ pour laquelle l'expression a un sens, on a :
  $$
\mathbb{E} g(X) \left(\mathbb{E}(Y|X) \right) = \mathbb{E} Y g(X) .
$$
```
## Fonctions de variables aléatoires

## Somme de variables aléatoires

## La loi normale multivariée, applications

<!-- -> khi2(d) = loi Gamma(1/2,d/2) -->
 <!-- = 4.9+4.10 -->

