# Variables aléatoires continues

## densités de probabilités

```{definition}
Une variable aléatoire réelle $X$ est dite **continue** s'il existe une fonction f positive telle que 
$$F_X(x) = \int_{-\infty}^x f(t) \mbox{d}t$$
Une telle fonction est une **densité** de $X$. Cette densité n'est pas unique.
Une telle densité est généralement notée $f_X$.

Lorsque $F_X$ est dérivable, $F_X'$ est une densité de $X$. 
```

```{lemma}
Si $X$ admet $f$ comme densité, alors:
  
- $\int_{-\infty}^{+\infty} f(x) \mbox{d}x = 1$;

- $\mathbb{P}(X=x)=0, \forall x \in \mathbb{R}$;

- $\mathbb{P}(X \in \mathcal{E}) = \int_{\mathcal{E}} f(x) \mbox{d}x$ pour tout évènement $\mathcal{E}$;

- $X$ et $-X$ ont les mêmes distributions si et seulement si $f_X$ est paire;

- Si $f$ et $g$ sont des densités, il en est de même pour $\lambda f + (1-\lambda)g$ pour $0 \leq \lambda \leq 1$.
```

```{example}
Soient $a,b$ deux réels avec $a<b$. La **loi uniforme** sur l'intervalle $[a,b]$ est définie par sa densité de probabilité
$$
f(x) = \left\lbrace
\begin{array}{ccc}
\frac{1}{b-a}  & \mbox{si} & a \leq x \leq b\\
0 & \mbox{sinon} &\\
\end{array}\right.
$$
On note cette loi $\mathcal{U}(a,b)$.
```

```{example}
La **loi exponentielle** modélise la durée de vie d'un phénomène sans mémoire : la probabilité que le phénomène dure au moins $s+t$ sachant qu'il a déjà duré $t$ est la même que la probabilié de durer $s$ à partir de la mise en fonction initiale.

Soit $\lambda > 0$. La densité associée à cette loi est :
 $$
f(t) = \left\lbrace
\begin{array}{ccc}
\lambda e^{-\lambda t}  & \mbox{si} & t>0\\
0 & \mbox{sinon} &\\
\end{array}\right.
$$
On note la loi exponentielle de paramètre $\lambda$ : $\mathcal{E}(\lambda)$.
```
```{lemma}
Soient $X,Y$ deux variables aléatoires qui suivent deux lois exponentielles, de paramètres $\lambda, \mu$, alors $\min (X,Y)$ suit encore une loi exponentielle, de paramètre $\lambda + \mu$.

```

```{example}
La loi **Gamma** (qui généralise la loi exponentielle) modélise également des durées; elle est caractérisée par deux paramètres $k,\theta$ positifs.

Sa densité est
$$ f(t) = \left\lbrace
\begin{array}{ccc}
\frac{t^{k-1} \exp \left(-\frac{t}{\theta} \right)}{\theta^k \Gamma (k)}   & \mbox{si} & x>0\\
0 & \mbox{sinon} & \\
\end{array}\right.
$$
où $\Gamma$ désigne la fonction Gamma d'Euler ($\Gamma : z \mapsto \int_0^{+\infty} t^{z-1} e^{-t} \mbox{d}t$).  

On note cette  loi $\Gamma(k,\theta)$. 
```

```{lemma}
Soient $X_1, \ldots, X_n$ des variables aléatoires indépendantes, de loi $\Gamma(k_1,\theta),\ldots,\Gamma(k_n,\theta)$. Alors $X_1+\ldots+X_n$ suit une loi $\Gamma(\sum_{i=1}^n k_i,\theta)$.
```

```{lemma}
Soit $X  \sim \Gamma(k,\theta)$ et $t>0$. Alors $tX \sim \Gamma(k,t \theta)$.
```


```{example}
La **loi normale** est définie suivant deux paramètres ($\mu \in \mathbb{R}, \sigma >0$) par sa densité:
  
  $$
  f(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)
  $$
  On note la loi $\mathcal{N}(\mu,\sigma^2)$.
```

```{example}
Soit $X$ une variable aléatoire qui suit une loi normale $\mathcal{N}(\mu,\sigma^2)$. Alors $\exp(X)$ suit une loi **log-normale**.
```

## Indépendance

```{definition}
Soient  $X,Y$ deux variables aléatoires réelles.
On dit que les variables aléatoires $X$ et $Y$ sont **indépendantes** si les évènements $\lbrace X  \leq x \rbrace$ et $\lbrace Y \leq y \rbrace$ sont indépendants pour tous $x,y$ réels.
```

```{theorem}
Soient $X,Y$ deux variables aléatoires, $f,g$ deux fonctions telles que $f(X), g(Y)$ soient des variables aléatoires.

Si $X$ et $Y$ sont indépendantes, il en est de même pour $f(X)$ et $g(Y)$.```

## Espérance

```{definition}
**L'espérance** d'une variable aléatoire $X$ de densité $f$ est définie par
$$\mathbb{E}X = \int_{-\infty}^{+\infty} x f(x) \mbox{d}x$$
  lorsque cette intégrale existe.
```
```{theorem}
Si $X$ et $g(X)$ sont des variables aléatoires, alors
$$\mathbb{E}g(X) = \int_{-\infty}^{+\infty} g(x) f(x) \mbox{d}x$$
```
```{lemma}
Si $X$ admet une densité nulle sur $\mathbb{R}^-$ et une fonction de répartition $F$, alors
$$\mathbb{E}X = \int_0^{+\infty} (1-F(x)) \mbox{d}x = \int_0^{+\infty} \mathbb{P}(X>x) \mbox{d}x$$
```
Les autres propriétés de l'espérance, vues dans le chapitre sur les variables aléatoires discrètes, s'étendent sans difficultés aux variables continues (moments, moments centrés, variance, covariance, corrélation).

## Exemples de variables aléatoires continues

```{theorem}
L'espérance et la variance des lois continues de référence sont recensés dans le tableau ci-après :

|Loi                        | Espérance        | Variance|
|:---------------------------:|:------------------:|:---------:|
|$\mathcal{U}(a,b)$         | $\frac{a+b}{2}$  |$\frac{(b-a)^2}{12}$|
|$\mathcal{E}(\lambda)$     |$\frac{1}{\lambda}$|$\frac{1}{\lambda^2}$|
|$\Gamma(k,\theta)$         |$k \theta$        |$k \theta^2$|
|$\mathcal{N}(\mu,\sigma^2)$|$\mu$             |$\sigma^2$|

```

## Dépendance

```{definition}
Soient $X,Y$ deux variables aléatoires réelles.  Ces variables sont **conjointement continues** s'il existe une fonction positive $f$ telle que:
  
  $$F(x,y) = \mathbb{P}(X \leq x , Y \leq y) = \int_{-\infty}^x \int_{-\infty}^y f(u,v) \mbox{d}u \mbox{d}v$$
  f est une densité du couple $(X,Y)$. Si $F$ est dérivable, on peut prendre
  $$f = \frac{\partial^2 F}{\partial x \partial y}$$
```

```{lemma}
On a
$$
  \mathbb{P}(a \leq X \leq b ; c \leq Y \leq d) = F(b,d) - F(a,d) - F(b,c)+F(a,c)
  = \int_{a}^b \int_{c}^d f(u,v) \mbox{d}u \mbox{d}v
$$
  Plus généralement, si $\mathcal{E}$ est un évènement, on a:
  
  $$
  \mathbb{P}\left( (X,Y) \in \mathcal{E} \right) = \iint_{\mathcal{E}} f(u,v) \mbox{d}u \mbox{d}v
  $$
```

```{definition}
Les **fonctions de répartition marginales** de $X$ et $Y$ sont définies par
$$
  F_X(x) = \mathbb{P}(X  \leq x) = \int_{-\infty}^x \int_{-\infty}^{+\infty} f(u,v) \mbox{d}u \mbox{d}v ; F_Y(y) = \mathbb{P}(Y  \leq y) = \int_{-\infty}^{+\infty} \int_{-\infty}^y  f(u,v) \mbox{d}u \mbox{d}v 
  $$
  On en déduit les **densités marginales** selon $X$ et $Y$ :
  $$
  f_X(x) = \int_{-\infty}^{+\infty} f(x,y) \mbox{d}y ; f_Y(y) = \int_{-\infty}^{+\infty} f(x,y) \mbox{d}x
  $$
```

```{theorem}
Soit $g : \mathbb{R}^2 \to \mathbb{R}$ telle que $g(X,Y)$ soit une variable aléatoire. Alors, 
$$
  \mathbb{E} g(X,Y) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(u,v) f(u,v) \mbox{d}u \mbox{d}v
  $$ dès que cette intégrale est définie.
```

```{theorem}
Les variables $X,Y$ sont indépendantes si et seulement si
$$F_{X,Y} = F_X F_Y \iff f_{X,Y} = f_x f_y.$$
```

```{theorem}
Soit $X,Y$ deux variables aléatoires continues. Alors 
$$\left( \mathbb{E}XY \right)^2 \leq \left( \mathbb{E} X^2\right) \left( \mathbb{E} Y^2\right)$$
  avec égalité si et seulement si il existe des réels $\lambda,\mu$ non tous nuls tels que 
$$\mathbb{P}(\lambda X + \mu Y = 0) = 1.$$

```
## Loi conditionnelle et espérance conditionnelle

Soient $X,Y$ deux variables aléatoires continues, de densité jointe $f$.
```{definition}
La **fonction de répartition conditionnelle** de $Y$ sachant $X=x$ est définie par

$$
F_{Y|X}(y|x) = \int_{-\infty}^y \frac{f(x,v)}{f_X(x)} \mbox{d}v  
$$  pour tout $x$ tel que $f_X(x)>0$. Elle est également notée
$$\mathbb{P}(Y \leq y | X = x)$$
```

```{definition}
La **densité conditionnelle** de $Y$ sachant $X=x$ est définie par

$$
f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)} 
$$  pour tout $x$ tel que $f_X(x)>0$. Encore,
$$
  f_{Y|X} = \frac{f_{X,Y}}{f_X} 
$$
```

```{theorem}
**L'espérance conditionnelle** de $Y$ sachant $X$ est définie comme
$$\mathbb{E}(Y|X=x) = \int_{-\infty}^{+\infty} y f_{Y|X}(y|x) \mbox{d}y ;$$
Elle vérifie la relation
$$
\mathbb{E} \left(\mathbb{E}(Y|X) \right) = \mathbb{E} Y .
$$
Plus généralement, pour toute fonction $g$ pour laquelle l'expression a un sens, on a :
  $$
\mathbb{E}  \left(g(X) \mathbb{E}(Y|X) \right) = \mathbb{E} Y g(X) .
$$
```
## Fonctions de variables aléatoires

```{theorem}
Soit $X$ une variable aléatoire avec une densité $f$, et soit $g$ une fonction réelle telle que $Y = g(X)$ soit encore une variable aléatoire.
La fonction de répartition de $Y$ est définie par :
  $$
  \mathbb{P}(Y \leq y) = \mathbb{P} \left(X \in g^{-1}(]-\infty , y]) \right) .
  $$
```
Plus généralement, on a le théorème suivant:
```{theorem}
Soient $\mathcal{U}, \mathcal{V}$ deux ouverts de $\mathbb{R}^d$,
Soit $X$ une variable aléatoire à valeurs dans $\mathcal{U}$  de densité $f$ et soit $T$ un difféomorphisme de classe $\mathcal{C}^1 : \mathcal {U} \to \mathcal{V}$.
La densité de la variable aléatoire $Y = \varphi(X)$ est définie par :

$$
f_Y(y) = \left\lbrace
\begin{array}{ccc}
f_X(\varphi^{-1}(y)) |J_{\varphi^{-1}}(y)|  & \mbox{si} & y \in \mathcal{V}\\
0 & \mbox{sinon} &\\
\end{array}\right.
$$
en notant $|J_{\varphi^{-1}}(y)|$ le déterminant du jacobien de $\varphi^{-1}$, c'est-à-dire de la matrice de terme général $\left(\frac{\partial \varphi^{-1}_j}{ \partial y_i} \right)_{i,j}$.
```

```{lemma}
Pour $d=2$, le changement de variables "passage en coordonnées polaires" transforme une densité $f(x,y)$ en $f(r \cos \theta, r \sin \theta) r$.
```

## Somme de variables aléatoires

```{theorem}
Si $X,Y$ ont pour densité jointe $f$, alors la densité de $Z = X+Y$ est définie par:
  
  $$
  f_Z(z) = \int_{-\infty}^{+\infty} f(x,z-x) \mbox{d}x
  $$
  Si, $X$ et $Y$ sont indépendantes, le résultat devient :
   $$
  f_Z(z) = \int_{-\infty}^{+\infty} f_X(z-y) f_Y(y) \mbox{d}y = \int_{-\infty}^{+\infty} f_X(x) f_Y(z-x) \mbox{d}x
  $$
  On dit que $f_Z$ est le **produit de convolution** de $f_X$ et $f_Y$, noté
$f_X \star f_Y$.
```

```{lemma}
Soit $X \sim \mathcal{N}(\mu_1,\sigma_1^2)$ et $Y \sim \mathcal{N}(\mu_2,\sigma_2^2)$. Supposons que $X$ et $Y$ sont indépendantes. Alors
$X+Y \sim \mathcal{N}(\mu_1 + \mu_2,\sigma_1^2 + \sigma_2^2)$.
```

## La loi normale multivariée

```{definition}
Le vecteur $X = (X_1, \ldots, X_n)$ suit une **loi normale multivariée** (notée $\mathcal{N}(\mu,\Sigma)$) s'il existe un vecteur $\mu = (\mu_1,\ldots,\mu_n)$ de réels et une matrice symétrique définie positive $\Sigma$ telle que la densité jointe de $X$ soit:

$$f_X(x) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp \left( 
-\frac12 (x-\mu)\Sigma^{-1}(x-\mu)'
\right)$$
  
De façon équivalente, on dit que le vecteur $X$ suit une loi normale multivariée si et seulement si pour tout vecteur $a \in \mathbb{R}^n$, $Xa'$ suit une loi normale.  
```

```{theorem}
Si $X$ suit une loi $\mathcal{N}(\mu,\Sigma)$, alors

- $\mathbb{E}X = \mu$
  
-  la matrice $\Sigma$ est la matrice de covariance de $X$:
  $$\Sigma_{i,j} = \text{cov} (X_i,X_j)$$
  
- Si $A$ est une matrice de rang $m \leq n$, alors $XA$ suit la loi $\mathcal{N}(\mu A,A' \Sigma A)$
```

## Lois issues de la loi normale

On dispose de variables aléatoires correspondants aux résultats d'un même expérience aléatoire $X_1,\ldots,X_n$ dont on suppose qu'elle sont de loi normale $\mathcal{N}(\mu,\sigma^2)$ pour des quantités $\mu,\sigma$ non observée. On cherche à estimer ces quantités. Pour celà, on va considérer la moyenne empirique 
$$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$$
comme estimateur de $\mu$ et la variance empirique
$$S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2$$
comme estimateur de $\sigma^2$.

```{theorem}
Soit $X_1,\ldots,X_n$ des variables aléatoires indépendantes, de même loi $\mathcal{N}(\mu,\sigma^2)$.

- $\overline{X}$ et $S^2$ sont indépendants;

- $\overline{X} \sim \mathcal{N}(\mu,\sigma^2 / n)$

- $(n-1)S^2/\sigma^2 \sim \Gamma(\frac12, \frac{n-1}{2})$. Cette dernière loi s'appelle la **loi du $\chi^2$ à $n-1$ degrés de liberté**, notée $\chi^2(n-1)$ ;
c'est la loi de la somme des carrés de $n-1$ lois normales centrées réduites indépendantes.
- Les variables aléatoires
$$ U = \frac{n-1}{\sigma^2}S^2 \sim \chi^2(n-1)$$ et
$$V = \frac{\sqrt{n}}{\sigma}(\overline{X}-\mu) \sim \mathcal{N}(0,1).$$
Elles ne dépendent pas de $\sigma$. Le ratio 
$$T = \frac{V}{\sqrt{U/(n-1)}}$$
  dont le numérateur suit $\mathcal{N}(0,1)$ et le dénominateur est la racine d'un $\chi^2(n-1)$, divisé par $n-1$ suit une **loi de Student**, de paramètre $n-1$, notée $t(n-1)$.
  
- Soient $U,V$ deux variables aléatoires qui suivent respectivement les lois $\chi^2(r)$ et $\chi^2(s)$, alors
$$F = \frac{U/r}{V/s}$$
suit une **loi de Fisher** de paramètres $(r,s)$, notée $F(r,s)$. Dans ce cas, $1/F$ suit la loi $F(s,r)$ et si $T \sim t(r)$, $T^2 \sim F(1,r)$
```

