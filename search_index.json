[
["index.html", "Probabilités et statistiques Introduction", " Probabilités et statistiques Pierre-Damien Olive Mis à jour le 2019-06-18 Introduction Le site avec la dernière version du cours est disponible sur : https://olivepierre.github.io/probaStats/ "],
["evenements-et-probabilites.html", "Chapitre 1 Évènements et probabilités 1.1 Évènements 1.2 Probabilités 1.3 Probabilités conditionnelles 1.4 Indépendance", " Chapitre 1 Évènements et probabilités 1.1 Évènements 1.2 Probabilités 1.3 Probabilités conditionnelles 1.4 Indépendance "],
["variables-aleatoires.html", "Chapitre 2 Variables aléatoires 2.1 Variables aléatoires 2.2 Variables discrètes et continues 2.3 Vecteurs aléatoires", " Chapitre 2 Variables aléatoires 2.1 Variables aléatoires 2.2 Variables discrètes et continues 2.3 Vecteurs aléatoires "],
["variables-aleatoires-discretes.html", "Chapitre 3 Variables aléatoires discrètes 3.1 Fonctions de masse 3.2 Indépendance 3.3 Espérance 3.4 Cas particuliers de variables discrètes aléatoires 3.5 Dépendance 3.6 Loi conditionnelle et espérance conditionnelle 3.7 Somme de variables aléatoires discrètes", " Chapitre 3 Variables aléatoires discrètes 3.1 Fonctions de masse Définition 3.1 La fonction de masse d’une variable aléatoire discrète \\(X\\) est la fonction \\(f : \\mathbb{R} \\mapsto \\left[0 ,1 \\right]\\) définie par: \\[ f(x) = \\mathbb{P}(X=x) . \\] Fonctions de répartitions et fonctions de masse sont reliées par les relations: \\[ F(x) = \\sum_{x_i \\leq x} f(x_i) ; f(x) = F(x) - F(x^{-}) . \\] Lemme 3.1 Une fonction \\(f : \\mathbb{R} \\mapsto \\left[0 ,1 \\right]\\) est une fonction de masse si et seulement si l’ensemble \\(\\lbrace x : f(x) &gt;0 \\rbrace\\) est dénombrable et \\(\\sum_{i} f(x_i) = 1\\), où les \\(x_1, \\ldots, x_n, \\ldots\\) sont les valeurs de \\(x\\) tel que \\(f(x)&gt;0\\). Exemple 3.1 On choisit un nombre entier entre \\(1\\) et \\(n\\), de façon équiprobable. Un tel choix définit une variable aléatoire discrète, de loi uniforme. Exemple 3.2 On lance une pièce \\(n\\) fois, et on tombe sur pile avec une probabilité \\(p\\). L’ensemble des évènements est \\(\\Omega = \\lbrace \\mbox{pile}, \\mbox{face} \\rbrace\\). Le nombre de fois que l’on obtient “pile” est une variable aléatoire discrète dont la fonction de masse vérifie: \\[ f(x) = \\left\\lbrace \\begin{array}{ccc} 0 &amp; \\mbox{si} &amp; x\\notin \\lbrace 0, \\ldots , n \\rbrace\\\\ \\binom{n}{k} p^k (1-p)^{n-k} &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] On dit que cette variable aléatoire suit une loi binômiale (notée \\(\\mathcal{B}(n,p)\\)). Lorsque \\(n=1\\), on dit que la variable aléatoire suit une loi de Bernoulli (notée \\(\\mathcal{B}(p)\\)). Exemple 3.3 On compte le nombre d’épreuves de Bernoulli nécessaires pour arriver sur “pile” pour la première fois. Ce nombre de lancer est une variables aléatoire discrète, à valeur dans \\(\\mathbb{N}^*\\) et de fonction de masse: \\[ f(k) = (1-p)^{k-1} p \\forall k \\in \\mathbb{N}^* \\] La loi correspondante s’appelle la loi géométrique, de paramètre \\(p\\), notée \\(\\mathcal{G}(p)\\). Exemple 3.4 Une variable aléatoire prenant ses valeurs dans \\(\\mathbb{N}\\) avec une fonction de masse \\[ f(k) = \\frac{\\lambda^k}{k!} e^{-\\lambda} \\forall k \\in \\mathbb{N}\\] suit une loi de Poisson de paramètre \\(\\lambda &gt;0\\), notée \\(\\mathcal{P}(\\lambda)\\). Exemple 3.5 Soit \\(0&lt;p&lt;1\\). On tire simultanément \\(n\\) boules dans une urne contenant \\(A\\) boules, dont \\(pA\\) gagnantes et \\((1-p)A\\) perdantes. On compte le nombre de boules gagnantes extraites. La loi de cette variable aléatoire discrète est la loi hypergéométrique, de fonction de masse: \\[ f(k) = \\frac{\\binom{pA}{k} \\binom{qA}{n-k}}{\\binom{A}{n}} \\] 3.2 Indépendance Définition 3.2 Deux variables aléatoires discrètes \\(X\\) et \\(Y\\) sont indépendantes si les évènements \\(\\lbrace X=x \\rbrace\\) et \\(\\lbrace Y=y \\rbrace\\) sont indépendants pour tout \\(x\\) et \\(y\\) : \\[\\mathbb{P}(X=x,Y=y) = \\mathbb{P}(X=x) \\mathbb{P}(Y=y) .\\] On peut étendre cette définition à un ensemble de variables aléatoires \\((X_i)_{i \\in I}\\).Dans ce cas, l’égalité ci-dessus devient: \\[ \\mathbb{P}(X_i = x_i \\forall i \\in J) = \\prod_{i \\in J} \\mathbb{P}(X_i = x_i)\\] pour toute partie finie \\(J\\) de \\(I\\) Théorème 3.1 Soient \\(X,Y\\) deux variables aléatoires discrètes indépendantes, \\(f,g\\) deux fonctions réelles. Alors les variables aléatoires discrètes \\(f(X)\\) et \\(g(Y)\\) sont encore indépendantes. 3.3 Espérance Définition 3.3 L’espérance d’une variables aléatoire discrète \\(X\\) avec une fonction de masse \\(f\\) est définie comme \\[ \\mathbb{E} X = \\sum_{x : f(x)&gt;0} x f(x) \\] lorsque cette somme est absolument convergente; la convergence absolue est requise afin que cette somme ait une même valeur quelque soit l’ordre dans lequel on somme les \\(x\\) (cf. cours analyse sur les familles sommables). En particulier, lorsque \\(A\\) est un évènement, on a \\[\\mathbb{E} \\mathbf{1}_A = \\mathbb{P}(A).\\] Afin de calculer les espérances de fonctions de variables aléatoires, on peut envisager de calculer la fonction de masse de cette nouvelle variable aléatoire. Cette approche est délicate et peu pertinente grâce au lemme: Lemme 3.2 Soit \\(X\\) une variable aléatoire discrète, \\(f\\) sa fonction de masse. Soit \\(g\\) une fonction réelle. Alors l’espérance de \\(g(X)\\) est définie par \\[ \\mathbb{E} g(X) = \\sum_{x} g(x) f(x) \\] lorsque cette somme est absolument convergente. On peut ainsi calculer les moments pour tout ordre (\\(m_k = \\mathbb{E} X^k\\)) ainsi que les moments centrés (\\(\\sigma_k = \\mathbb{E} (X - m_1)^k\\)). En particulier, on définit : Définition 3.4 Le moment d’ordre 1 s’appelle l’espérance de \\(X\\), le moment centré d’ordre 2 de \\(X\\) s’appelle la variance de \\(X\\) (notée \\(\\mathbb{V}X\\)) : c’est une quantité positive! On appelle \\(\\sigma = \\sqrt{\\sigma_2}\\) l’écart-type de \\(X\\). Lemme 3.3 On peut exprimer les moments centrés en fonction de moments d’ordres inférieurs ; en particulier, \\[\\sigma_2 = m_2 - m_1^2 \\] Théorème 3.2 L’opérateur \\(\\mathbb{E}\\) est une forme linéaire ( pour \\(\\lambda \\in \\mathbb{R}\\), \\(X,Y\\) deux variables aléatoires admettant une espérance, \\(\\mathbb{E} (X+\\lambda Y) = \\mathbb{X} + \\lambda \\mathbb{E} Y\\)), positive (\\(X \\geq 0\\) entraine \\(\\mathbb{E}X \\geq 0\\)) vérifiant \\(\\mathbb{E}1 = 1\\). Par contre, il n’est pas vrai (en général) que \\[\\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Lemme 3.4 Si \\(X,Y\\) sont des variables aléatoires indépendantes, alors \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Définition 3.5 On dit que les variables aléatoires \\(X,Y\\) sont non corrélées si \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Ainsi, les variables indépendantes sont non-corrélées, mais l’inverse n’est pas vrai ! Théorème 3.3 Soient \\(X,Y\\) deux variables aléatoires, \\(\\lambda \\in \\mathbb{R}\\). Alors \\(\\mathbb{V} \\lambda X = \\lambda^2 \\mathbb{V}X\\) et \\(\\mathbb{V}(X+Y) = \\mathbb{V}X+\\mathbb{V}Y\\) si ces variables sont non-corrélées. 3.4 Cas particuliers de variables discrètes aléatoires Théorème 3.4 Soit \\(X\\) une variable aléatoire suivant une loi uniforme. Alors \\[\\mathbb{E}X = \\frac{n+1}{2} , \\mathbb{V}X = \\frac{n^2-1}{12}\\] Théorème 3.5 Soit \\(X\\) une variable aléatoire suivant une loi de Bernoulli. Alors \\[\\mathbb{E}X = p , \\mathbb{V}X = p(1-p)\\] Théorème 3.6 Soit \\(X\\) une variable aléatoire suivant une loi binômiale \\(\\mathcal{B}(n,p)\\). Alors \\[\\mathbb{E}X = np , \\mathbb{V}X = np(1-p)\\] Théorème 3.7 Soit \\(X\\) une variable aléatoire suivant une loi géométrique \\(\\mathcal{G}(p)\\). Alors \\[\\mathbb{E}X = \\frac{1}{p} , \\mathbb{V}X = \\frac{1-p}{p^2}\\] Théorème 3.8 Soit \\(X\\) une variable aléatoire suivant une loi de Poisson \\(\\mathcal{P}(\\lambda)\\). Alors \\[\\mathbb{E}X = \\lambda , \\mathbb{V}X = \\lambda\\] 3.5 Dépendance Définition 3.6 Soient \\(X,Y\\) deux variables aléatoires discrètes. La fonction de répartition jointe est définie par \\[F(x,y) = \\mathbb{P}(X \\leq x , Y \\leq y).\\] La fonction de masse jointe est donnée par \\[ f(x,y) = \\mathbb{P}(X=x, Y=y).\\] Lemme 3.5 Les variables aléatoires discrètes \\(X,Y\\) sont indépendantes si et seulement si \\[ f_{X,Y}(x,y) = f_X(x) f_Y(y). \\] Plus généralement, \\(X,Y\\) sont indépendantes si et seulement si il existe des fonctions réelles \\(f,g\\) telles que \\[ f_{X,Y}(x,y) = f(x) g(y). \\] A partir de la connaissance de \\(f_{X,Y}\\), il est possible de calculer \\(f_X\\) ou \\(f_Y\\) grâce au théorème suivant : Théorème 3.9 On a: \\[f_X(x) = \\sum_y f_{X,Y}(x,y)\\] \\[f_Y(y) = \\sum_x f_{X,Y}(x,y)\\] Le calcul de l’espérance d’une fonction de variables aléatoires est rendu facile par le Lemme 3.6 Soit \\(g\\) une fonction telle que \\(g(X,Y)\\) soit une variable aléatoire. Alors \\[\\mathbb{E} g(X,Y) = \\sum_{x,y} g(x,y)f_{X,Y}(x,y)\\] si cette somme existe. Définition 3.7 Soient \\(X,Y\\) deux variables aléatoires. Leur covariance est définie par: \\[cov(X,Y) = \\mathbb{E} \\left[ (X- \\mathbb{E}X) (Y - \\mathbb{E}Y)\\right] = \\mathbb{E}XY - \\mathbb{E}X \\mathbb{E}Y.\\] Le coefficient de corrélation est : \\[cor(X,Y) = \\frac{cov(X,Y)}{\\sqrt{\\mathbb{V}X \\mathbb{V}Y}}.\\] Lemme 3.7 Le coefficient de corrélation est en module inférieur à 1 et égal à \\(\\pm 1\\) si seulement si il existe des réels \\(a,b,c\\) tels que \\[\\mathbb{P}(aX+bY+c=0)=1.\\] Théorème 3.10 Soit \\(X,Y\\) deux variables aléatoires discrètes. Alors \\[\\left( \\mathbb{E}XY \\right)^2 \\leq \\left( \\mathbb{E} X^2\\right) \\left( \\mathbb{E} Y^2\\right)\\] avec égalité si et seulement si il existe des réels \\(\\lambda,\\mu\\) non tous nuls tels que \\[\\mathbb{P}(\\lambda X + \\mu Y = 0) = 1.\\] 3.6 Loi conditionnelle et espérance conditionnelle Soient \\(X,Y\\) deux variables aléatoires discrètes. Définition 3.8 La fonction de répartion conditionnelle de \\(Y\\) sachant \\(X=x\\), notée \\(F_{Y|X}(\\bullet | x)\\) est définie par \\[ F_{Y|X}(y | x) = \\mathbb{P}(Y \\leq y | X=x) \\] pour tout \\(x\\) tel que \\(\\mathbb{P}(X=x)&gt;0\\). La fonction de masse conditionnelle de \\(Y\\) sachant \\(X=x\\), notée \\(f_{Y|X}(\\bullet | x)\\) est définie par \\[ f_{Y|X}(y | x) = \\mathbb{P}(Y = y | X=x) \\] pour tout \\(x\\) tel que \\(\\mathbb{P}(X=x)&gt;0\\). On a donc \\[ f_{Y|X} = f_{X,Y} / f_X \\] lorsque cette expression est définie. Encore, \\(f_{Y|X} = f_Y\\) si et seulement si \\(X\\) et \\(Y\\) sont indépendantes. Définition 3.9 L’espérance conditionnelle de \\(Y\\) par rapport à \\(X\\) est définie comme \\[\\mathbb{E}(Y | X=x) = \\sum_y y f_{Y|X}(y | x)\\] Théorème 3.11 On a : \\[\\mathbb{E} Y = \\mathbb{E} \\left( \\mathbb{E}(Y | X)\\right)\\] Théorème 3.12 Pour toute fonction \\(g\\) pour laquelle les sommes sont définies, on a : \\[\\mathbb{E} \\left(Y g(X)\\right) = \\mathbb{E} \\left( g(X) \\mathbb{E}(Y | X)\\right)\\] Définition 3.10 La variance conditionnelle de \\(Y\\) sachant \\(X\\) est définie par \\[\\mathbb{V}(Y | X) = \\mathbb{E} \\left( [Y- \\mathbb{E}(Y | X)]^2 | X\\right).\\]) La variance conditionnelle est liée à l’espérance conditionnelle par le théorème : Théorème 3.13 \\[\\mathbb{V}(Y)=\\mathbb{E}(\\mathbb{V}[Y| X])+\\mathbb{V}(\\mathbb{E}[Y|X])\\] 3.7 Somme de variables aléatoires discrètes Soient \\(X,Y\\) deux variables aléatoires discrètes. Théorème 3.14 La loi de \\(X+Y\\) est définie par \\[ \\mathbb{P}(X+Y=z) = \\sum_x f_{X,Y} (x,z-x) \\] Si, en outre, \\(X\\) et \\(Y\\) sont indépendantes, alors \\[ \\mathbb{P}(X+Y=z) = \\sum_x f_X(x) f_Y(z-x) = \\sum_y f_X(z-y) f_Y(y) \\] La fonction masse de \\(X+Y\\) s’appelle alors le produit de convolution des fonctions masses de \\(X\\) et \\(Y\\), et est noté \\(f_X \\star f_Y\\). "],
["variables-aleatoires-continues.html", "Chapitre 4 Variables aléatoires continues 4.1 densités de probabilités 4.2 Indépendance 4.3 Espérance 4.4 Exemples de variables aléatoires continues 4.5 Dépendance 4.6 Loi conditionnelle et espérance conditionnelle 4.7 Fonctions de variables aléatoires 4.8 Somme de variables aléatoires 4.9 La loi normale multivariée, applications", " Chapitre 4 Variables aléatoires continues 4.1 densités de probabilités 4.2 Indépendance 4.3 Espérance 4.4 Exemples de variables aléatoires continues 4.5 Dépendance 4.6 Loi conditionnelle et espérance conditionnelle 4.7 Fonctions de variables aléatoires 4.8 Somme de variables aléatoires 4.9 La loi normale multivariée, applications "],
["statistique-descriptive.html", "A Statistique descriptive A.1 Distribution univariée A.2 Distribution bivariée A.3 Séries temporelles", " A Statistique descriptive A.1 Distribution univariée A.2 Distribution bivariée A.3 Séries temporelles "],
["techniques-de-denombrement-en-probabilite.html", "B Techniques de dénombrement en probabilité", " B Techniques de dénombrement en probabilité "],
["fonctions-indicatrices.html", "C Fonctions indicatrices", " C Fonctions indicatrices Théorème C.1 L’application qui a un évènement \\(A\\) associe la fonction indicatrice \\[ \\mathbf{1}_A : x \\to \\left\\lbrace \\begin{array}{ccc} 1 &amp; \\mbox{si} &amp; x\\in A\\\\ 0 &amp; \\mbox{si} &amp; x \\notin A\\\\ \\end{array}\\right. \\] est bijective. En outre, si \\(A,B\\) sont deux évènements disjoints, on a: \\[\\mathbf{1}_{A \\cup B} = \\mathbf{1}_A+ \\mathbf{1}_B.\\] Par ailleurs, \\[ \\mathbf{1}_{A^c} = 1- \\mathbf{1}_A.\\] Enfin, si \\(A,B\\) sont deux évènements, \\[ \\mathbf{1}_{A \\cap B} = \\mathbf{1}_A \\mathbf{1}_B.\\] Cet outil permet (entre autres) une démonstration immédiate de l’égalité de Poincaré (formule d’inclusion-exclusion) : Si \\(B = \\bigcup_{i=1}^n A_i\\), \\[\\mathbf{1}_B =1- \\prod_{i=1}^n (1-\\mathbf{1}_{A_i})\\] Et en développant puis en prenant l’espérance, il vient: \\[\\mathbb{P}(B) = \\sum_i \\mathbb{P}(A_i) - \\sum_{i&lt;j} \\mathbb{P}(A_i \\cap A_j)+ \\ldots \\] "],
["echantillonage.html", "D Échantillonage", " D Échantillonage "]
]
