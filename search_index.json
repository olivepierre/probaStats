[
["index.html", "Probabilités et statistiques Introduction", " Probabilités et statistiques Pierre-Damien Olive Mis à jour le 2019-06-20 Introduction Le site avec la dernière version du cours est disponible sur : https://olivepierre.github.io/probaStats/ "],
["evenements-et-probabilites.html", "Chapitre 1 Évènements et probabilités 1.1 Évènements 1.2 Probabilités 1.3 Probabilités conditionnelles 1.4 Indépendance", " Chapitre 1 Évènements et probabilités 1.1 Évènements 1.2 Probabilités 1.3 Probabilités conditionnelles 1.4 Indépendance "],
["variables-aleatoires.html", "Chapitre 2 Variables aléatoires 2.1 Variables aléatoires 2.2 Variables discrètes et continues 2.3 Vecteurs aléatoires", " Chapitre 2 Variables aléatoires 2.1 Variables aléatoires 2.2 Variables discrètes et continues 2.3 Vecteurs aléatoires "],
["variables-aleatoires-discretes.html", "Chapitre 3 Variables aléatoires discrètes 3.1 Fonctions de masse 3.2 Indépendance 3.3 Espérance 3.4 Cas particuliers de variables discrètes aléatoires 3.5 Dépendance 3.6 Loi conditionnelle et espérance conditionnelle 3.7 Somme de variables aléatoires discrètes", " Chapitre 3 Variables aléatoires discrètes 3.1 Fonctions de masse Définition 3.1 La fonction de masse d’une variable aléatoire discrète \\(X\\) est la fonction \\(f : \\mathbb{R} \\mapsto \\left[0 ,1 \\right]\\) définie par: \\[ f(x) = \\mathbb{P}(X=x) . \\] Fonctions de répartitions et fonctions de masse sont reliées par les relations: \\[ F(x) = \\sum_{x_i \\leq x} f(x_i) ; f(x) = F(x) - F(x^{-}) . \\] Lemme 3.1 Une fonction \\(f : \\mathbb{R} \\mapsto \\left[0 ,1 \\right]\\) est une fonction de masse si et seulement si l’ensemble \\(\\lbrace x : f(x) &gt;0 \\rbrace\\) est dénombrable et \\(\\sum_{i} f(x_i) = 1\\), où les \\(x_1, \\ldots, x_n, \\ldots\\) sont les valeurs de \\(x\\) tel que \\(f(x)&gt;0\\). Exemple 3.1 On choisit un nombre entier entre \\(1\\) et \\(n\\), de façon équiprobable. Un tel choix définit une variable aléatoire discrète, de loi uniforme. Exemple 3.2 On lance une pièce \\(n\\) fois, et on tombe sur pile avec une probabilité \\(p\\). L’ensemble des évènements est \\(\\Omega = \\lbrace \\mbox{pile}, \\mbox{face} \\rbrace\\). Le nombre de fois que l’on obtient “pile” est une variable aléatoire discrète dont la fonction de masse vérifie: \\[ f(x) = \\left\\lbrace \\begin{array}{ccc} 0 &amp; \\mbox{si} &amp; x\\notin \\lbrace 0, \\ldots , n \\rbrace\\\\ \\binom{n}{k} p^k (1-p)^{n-k} &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] On dit que cette variable aléatoire suit une loi binômiale (notée \\(\\mathcal{B}(n,p)\\)). Lorsque \\(n=1\\), on dit que la variable aléatoire suit une loi de Bernoulli (notée \\(\\mathcal{B}(p)\\)). Exemple 3.3 On compte le nombre d’épreuves de Bernoulli nécessaires pour arriver sur “pile” pour la première fois. Ce nombre de lancer est une variables aléatoire discrète, à valeur dans \\(\\mathbb{N}^*\\) et de fonction de masse: \\[ f(k) = (1-p)^{k-1} p \\forall k \\in \\mathbb{N}^* \\] La loi correspondante s’appelle la loi géométrique, de paramètre \\(p\\), notée \\(\\mathcal{G}(p)\\). Exemple 3.4 Une variable aléatoire prenant ses valeurs dans \\(\\mathbb{N}\\) avec une fonction de masse \\[ f(k) = \\frac{\\lambda^k}{k!} e^{-\\lambda} \\forall k \\in \\mathbb{N}\\] suit une loi de Poisson de paramètre \\(\\lambda &gt;0\\), notée \\(\\mathcal{P}(\\lambda)\\). Exemple 3.5 Soit \\(0&lt;p&lt;1\\). On tire simultanément \\(n\\) boules dans une urne contenant \\(A\\) boules, dont \\(pA\\) gagnantes et \\((1-p)A\\) perdantes. On compte le nombre de boules gagnantes extraites. La loi de cette variable aléatoire discrète est la loi hypergéométrique, de fonction de masse: \\[ f(k) = \\frac{\\binom{pA}{k} \\binom{qA}{n-k}}{\\binom{A}{n}} \\] 3.2 Indépendance Définition 3.2 Deux variables aléatoires discrètes \\(X\\) et \\(Y\\) sont indépendantes si les évènements \\(\\lbrace X=x \\rbrace\\) et \\(\\lbrace Y=y \\rbrace\\) sont indépendants pour tout \\(x\\) et \\(y\\) : \\[\\mathbb{P}(X=x,Y=y) = \\mathbb{P}(X=x) \\mathbb{P}(Y=y) .\\] On peut étendre cette définition à un ensemble de variables aléatoires \\((X_i)_{i \\in I}\\).Dans ce cas, l’égalité ci-dessus devient: \\[ \\mathbb{P}(X_i = x_i \\forall i \\in J) = \\prod_{i \\in J} \\mathbb{P}(X_i = x_i)\\] pour toute partie finie \\(J\\) de \\(I\\) Théorème 3.1 Soient \\(X,Y\\) deux variables aléatoires discrètes indépendantes, \\(f,g\\) deux fonctions réelles. Alors les variables aléatoires discrètes \\(f(X)\\) et \\(g(Y)\\) sont encore indépendantes. 3.3 Espérance Définition 3.3 L’espérance d’une variables aléatoire discrète \\(X\\) avec une fonction de masse \\(f\\) est définie comme \\[ \\mathbb{E} X = \\sum_{x : f(x)&gt;0} x f(x) \\] lorsque cette somme est absolument convergente; la convergence absolue est requise afin que cette somme ait une même valeur quelque soit l’ordre dans lequel on somme les \\(x\\) (cf. cours analyse sur les familles sommables). En particulier, lorsque \\(A\\) est un évènement, on a \\[\\mathbb{E} \\mathbf{1}_A = \\mathbb{P}(A).\\] Afin de calculer les espérances de fonctions de variables aléatoires, on peut envisager de calculer la fonction de masse de cette nouvelle variable aléatoire. Cette approche est délicate et peu pertinente grâce au lemme: Lemme 3.2 Soit \\(X\\) une variable aléatoire discrète, \\(f\\) sa fonction de masse. Soit \\(g\\) une fonction réelle. Alors l’espérance de \\(g(X)\\) est définie par \\[ \\mathbb{E} g(X) = \\sum_{x} g(x) f(x) \\] lorsque cette somme est absolument convergente. On peut ainsi calculer les moments pour tout ordre (\\(m_k = \\mathbb{E} X^k\\)) ainsi que les moments centrés (\\(\\sigma_k = \\mathbb{E} (X - m_1)^k\\)). En particulier, on définit : Définition 3.4 Le moment d’ordre 1 s’appelle l’espérance de \\(X\\), le moment centré d’ordre 2 de \\(X\\) s’appelle la variance de \\(X\\) (notée \\(\\mathbb{V}X\\)) : c’est une quantité positive! On appelle \\(\\sigma = \\sqrt{\\sigma_2}\\) l’écart-type de \\(X\\). Lemme 3.3 On peut exprimer les moments centrés en fonction de moments d’ordres inférieurs ; en particulier, \\[\\sigma_2 = m_2 - m_1^2 \\] Théorème 3.2 L’opérateur \\(\\mathbb{E}\\) est une forme linéaire ( pour \\(\\lambda \\in \\mathbb{R}\\), \\(X,Y\\) deux variables aléatoires admettant une espérance, \\(\\mathbb{E} (X+\\lambda Y) = \\mathbb{X} + \\lambda \\mathbb{E} Y\\)), positive (\\(X \\geq 0\\) entraine \\(\\mathbb{E}X \\geq 0\\)) vérifiant \\(\\mathbb{E}1 = 1\\). Par contre, il n’est pas vrai (en général) que \\[\\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Lemme 3.4 Si \\(X,Y\\) sont des variables aléatoires indépendantes, alors \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Définition 3.5 On dit que les variables aléatoires \\(X,Y\\) sont non corrélées si \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Ainsi, les variables indépendantes sont non-corrélées, mais l’inverse n’est pas vrai ! Théorème 3.3 Soient \\(X,Y\\) deux variables aléatoires, \\(\\lambda \\in \\mathbb{R}\\). Alors \\(\\mathbb{V} \\lambda X = \\lambda^2 \\mathbb{V}X\\) et \\(\\mathbb{V}(X+Y) = \\mathbb{V}X+\\mathbb{V}Y\\) si ces variables sont non-corrélées. 3.4 Cas particuliers de variables discrètes aléatoires Théorème 3.4 Soit \\(X\\) une variable aléatoire suivant une loi uniforme. Alors \\[\\mathbb{E}X = \\frac{n+1}{2} , \\mathbb{V}X = \\frac{n^2-1}{12}\\] Théorème 3.5 Soit \\(X\\) une variable aléatoire suivant une loi de Bernoulli. Alors \\[\\mathbb{E}X = p , \\mathbb{V}X = p(1-p)\\] Théorème 3.6 Soit \\(X\\) une variable aléatoire suivant une loi binômiale \\(\\mathcal{B}(n,p)\\). Alors \\[\\mathbb{E}X = np , \\mathbb{V}X = np(1-p)\\] Théorème 3.7 Soit \\(X\\) une variable aléatoire suivant une loi géométrique \\(\\mathcal{G}(p)\\). Alors \\[\\mathbb{E}X = \\frac{1}{p} , \\mathbb{V}X = \\frac{1-p}{p^2}\\] Théorème 3.8 Soit \\(X\\) une variable aléatoire suivant une loi de Poisson \\(\\mathcal{P}(\\lambda)\\). Alors \\[\\mathbb{E}X = \\lambda , \\mathbb{V}X = \\lambda\\] 3.5 Dépendance Définition 3.6 Soient \\(X,Y\\) deux variables aléatoires discrètes. La fonction de répartition jointe est définie par \\[F(x,y) = \\mathbb{P}(X \\leq x , Y \\leq y).\\] La fonction de masse jointe est donnée par \\[ f(x,y) = \\mathbb{P}(X=x, Y=y).\\] Lemme 3.5 Les variables aléatoires discrètes \\(X,Y\\) sont indépendantes si et seulement si \\[ f_{X,Y}(x,y) = f_X(x) f_Y(y). \\] Plus généralement, \\(X,Y\\) sont indépendantes si et seulement si il existe des fonctions réelles \\(f,g\\) telles que \\[ f_{X,Y}(x,y) = f(x) g(y). \\] A partir de la connaissance de \\(f_{X,Y}\\), il est possible de calculer \\(f_X\\) ou \\(f_Y\\) grâce au théorème suivant : Théorème 3.9 On a: \\[f_X(x) = \\sum_y f_{X,Y}(x,y)\\] \\[f_Y(y) = \\sum_x f_{X,Y}(x,y)\\] Le calcul de l’espérance d’une fonction de variables aléatoires est rendu facile par le Lemme 3.6 Soit \\(g\\) une fonction telle que \\(g(X,Y)\\) soit une variable aléatoire. Alors \\[\\mathbb{E} g(X,Y) = \\sum_{x,y} g(x,y)f_{X,Y}(x,y)\\] si cette somme existe. Définition 3.7 Soient \\(X,Y\\) deux variables aléatoires. Leur covariance est définie par: \\[cov(X,Y) = \\mathbb{E} \\left[ (X- \\mathbb{E}X) (Y - \\mathbb{E}Y)\\right] = \\mathbb{E}XY - \\mathbb{E}X \\mathbb{E}Y.\\] Le coefficient de corrélation est : \\[cor(X,Y) = \\frac{cov(X,Y)}{\\sqrt{\\mathbb{V}X \\mathbb{V}Y}}.\\] Lemme 3.7 Le coefficient de corrélation est en module inférieur à 1 et égal à \\(\\pm 1\\) si seulement si il existe des réels \\(a,b,c\\) tels que \\[\\mathbb{P}(aX+bY+c=0)=1.\\] Théorème 3.10 Soit \\(X,Y\\) deux variables aléatoires discrètes. Alors \\[\\left( \\mathbb{E}XY \\right)^2 \\leq \\left( \\mathbb{E} X^2\\right) \\left( \\mathbb{E} Y^2\\right)\\] avec égalité si et seulement si il existe des réels \\(\\lambda,\\mu\\) non tous nuls tels que \\[\\mathbb{P}(\\lambda X + \\mu Y = 0) = 1.\\] 3.6 Loi conditionnelle et espérance conditionnelle Soient \\(X,Y\\) deux variables aléatoires discrètes. Définition 3.8 La fonction de répartion conditionnelle de \\(Y\\) sachant \\(X=x\\), notée \\(F_{Y|X}(\\bullet | x)\\) est définie par \\[ F_{Y|X}(y | x) = \\mathbb{P}(Y \\leq y | X=x) \\] pour tout \\(x\\) tel que \\(\\mathbb{P}(X=x)&gt;0\\). La fonction de masse conditionnelle de \\(Y\\) sachant \\(X=x\\), notée \\(f_{Y|X}(\\bullet | x)\\) est définie par \\[ f_{Y|X}(y | x) = \\mathbb{P}(Y = y | X=x) \\] pour tout \\(x\\) tel que \\(\\mathbb{P}(X=x)&gt;0\\). On a donc \\[ f_{Y|X} = f_{X,Y} / f_X \\] lorsque cette expression est définie. Encore, \\(f_{Y|X} = f_Y\\) si et seulement si \\(X\\) et \\(Y\\) sont indépendantes. Définition 3.9 L’espérance conditionnelle de \\(Y\\) par rapport à \\(X\\) est définie comme \\[\\mathbb{E}(Y | X=x) = \\sum_y y f_{Y|X}(y | x)\\] Théorème 3.11 On a : \\[\\mathbb{E} Y = \\mathbb{E} \\left( \\mathbb{E}(Y | X)\\right)\\] Théorème 3.12 Pour toute fonction \\(g\\) pour laquelle les sommes sont définies, on a : \\[\\mathbb{E} \\left(Y g(X)\\right) = \\mathbb{E} \\left( g(X) \\mathbb{E}(Y | X)\\right)\\] Définition 3.10 La variance conditionnelle de \\(Y\\) sachant \\(X\\) est définie par \\[\\mathbb{V}(Y | X) = \\mathbb{E} \\left( [Y- \\mathbb{E}(Y | X)]^2 | X\\right).\\] La variance conditionnelle est liée à l’espérance conditionnelle par le théorème : Théorème 3.13 \\[\\mathbb{V}(Y)=\\mathbb{E}(\\mathbb{V}[Y| X])+\\mathbb{V}(\\mathbb{E}[Y|X])\\] 3.7 Somme de variables aléatoires discrètes Soient \\(X,Y\\) deux variables aléatoires discrètes. Théorème 3.14 La loi de \\(X+Y\\) est définie par \\[ \\mathbb{P}(X+Y=z) = \\sum_x f_{X,Y} (x,z-x) \\] Si, en outre, \\(X\\) et \\(Y\\) sont indépendantes, alors \\[ \\mathbb{P}(X+Y=z) = \\sum_x f_X(x) f_Y(z-x) = \\sum_y f_X(z-y) f_Y(y) \\] La fonction masse de \\(X+Y\\) s’appelle alors le produit de convolution des fonctions masses de \\(X\\) et \\(Y\\), et est noté \\(f_X \\star f_Y\\). "],
["variables-aleatoires-continues.html", "Chapitre 4 Variables aléatoires continues 4.1 densités de probabilités 4.2 Indépendance 4.2 Espérance 4.3 Exemples de variables aléatoires continues 4.4 Dépendance 4.5 Somme de variables aléatoires 4.6 La loi normale multivariée, applications", " Chapitre 4 Variables aléatoires continues 4.1 densités de probabilités Définition 4.1 Une variable aléatoire réelle \\(X\\) est dite continue s’il existe une fonction f positive telle que \\[F_X(x) = \\int_{-\\infty}^x f(t) \\mbox{d}t\\] Une telle fonction est une densité de \\(X\\). Cette densité n’est pas unique. Une telle densité est généralement notée \\(f_X\\). Lorsque \\(F_X\\) est dérivable, \\(F_X&#39;\\) est une densité de \\(X\\). Lemme 4.1 Si \\(X\\) admet \\(f\\) comme densité, alors: \\(\\int_{-\\infty}^{+\\infty} f(x) \\mbox{d}x = 1\\); \\(\\mathbb{P}(X=x)=0, \\forall x \\in \\mathbb{R}\\); \\(\\mathbb{P}(X \\in \\mathcal{E}) = \\int_{\\mathcal{E}} f(x) \\mbox{d}x\\) pour tout évènement \\(\\mathcal{E}\\); \\(X\\) et \\(-X\\) ont les mêmes distributions si et seulement si \\(f_X\\) est paire; Si \\(f\\) et \\(g\\) sont des densités, il en est de même pour \\(\\lambda f + (1-\\lambda)g\\) pour \\(0 \\leq \\lambda \\leq 1\\). Exemple 4.1 Soient \\(a,b\\) deux réels avec \\(a&lt;b\\). La loi uniforme sur l’intervalle \\([a,b]\\) est définie par sa densité de probabilité \\[ f(x) = \\left\\lbrace \\begin{array}{ccc} \\frac{1}{b-a} &amp; \\mbox{si} &amp; a \\leq x \\leq b\\\\ 0 &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] On note cette loi \\(\\mathcal{U}(a,b)\\). Exemple 4.2 La loi exponentielle modélise la durée de vie d’un phénomène sans mémoire : la probabilité que le phénomène dure au moins \\(s+t\\) sachant qu’il a déjà duré \\(t\\) est la même que la probabilié de durer \\(s\\) à partir de la mise en fonction initiale. Soit \\(\\lambda &gt; 0\\). La densité associée à cette loi est : \\[ f(t) = \\left\\lbrace \\begin{array}{ccc} \\lambda e^{-\\lambda t} &amp; \\mbox{si} &amp; t&gt;0\\\\ 0 &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] On note la loi exponentielle de paramètre \\(\\lambda\\) : \\(\\mathcal{E}(\\lambda)\\). Lemme 4.2 Soient \\(X,Y\\) deux variables aléatoires qui suivent deux lois exponentielles, de paramètres \\(\\lambda, \\mu\\), alors \\(\\min (X,Y)\\) suit encore une loi exponentielle, de paramètre \\(\\lambda + \\mu\\). Exemple 4.3 La loi Gamma (qui généralise la loi exponentielle) modélise également des durées; elle est caractérisée par deux paramètres \\(k,\\theta\\) positifs. Sa densité est \\[ f(t) = \\left\\lbrace \\begin{array}{ccc} \\frac{t^{k-1} \\exp \\left(-\\frac{t}{\\theta} \\right)}{\\theta^k \\Gamma (k)} &amp; \\mbox{si} &amp; x&gt;0\\\\ 0 &amp; \\mbox{sinon} &amp; \\\\ \\end{array}\\right. \\] où \\(\\Gamma\\) désigne la fonction Gamma d’Euler (\\(\\Gamma : z \\mapsto \\int_0^{+\\infty} t^{z-1} e^{-t} \\mbox{d}t\\)). On note cette loi \\(\\Gamma(k,\\theta)\\). Lemme 4.3 Soient \\(X_1, \\ldots, X_n\\) des variables aléatoires indépendantes, de loi \\(\\Gamma(k_1,\\theta),\\ldots,\\Gamma(k_n,\\theta)\\). Alors \\(X_1+\\ldots+X_n\\) suit une loi \\(\\Gamma(\\sum_{i=1}^n k_i,\\theta)\\). Lemme 4.4 Soit \\(X \\sim \\Gamma(k,\\theta)\\) et \\(t&gt;0\\). Alors \\(tX \\sim \\Gamma(k,t \\theta)\\). Exemple 4.4 La loi normale est définie suivant deux paramètres (\\(\\mu \\in \\mathbb{R}, \\sigma &gt;0\\)) par sa densité: \\[ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x-\\mu)^2}{2 \\sigma^2}\\right) \\] On note la loi \\(\\mathcal{N}(\\mu,\\sigma^2)\\). Exemple 4.5 Soit \\(X\\) une variable aléatoire qui suit une loi normale \\(\\mathcal{N}(\\mu,\\sigma^2)\\). Alors \\(\\exp(X)\\) suit une loi log-normale. 4.2 Indépendance Définition 4.2 Soient \\(X,Y\\) deux variables aléatoires réelles. On dit que les variables aléatoires \\(X\\) et \\(Y\\) sont indépendantes si les évènements \\(\\lbrace X \\leq x \\rbrace\\) et \\(\\lbrace Y \\leq y \\rbrace\\) sont indépendants pour tous \\(x,y\\) réels. Théorème 4.1 Soient \\(X,Y\\) deux variables aléatoires, \\(f,g\\) deux fonctions telles que \\(f(X), g(Y)\\) soient des variables aléatoires. Si \\(X\\) et \\(Y\\) sont indépendantes, il en est de même pour \\(f(X)\\) et \\(g(Y)\\).``` 4.2 Espérance Définition 4.3 L’espérance d’une variable aléatoire \\(X\\) de densité \\(f\\) est définie par \\[\\mathbb{E}X = \\int_{-\\infty}^{+\\infty} x f(x) \\mbox{d}x\\] lorsque cette intégrale existe. Théorème 4.2 Si \\(X\\) et \\(g(X)\\) sont des variables aléatoires, alors \\[\\mathbb{E}g(X) = \\int_{-\\infty}^{+\\infty} g(x) f(x) \\mbox{d}x\\] Lemme 4.5 Si \\(X\\) admet une densité nulle sur \\(\\mathbb{R}^-\\) et une fonction de répartition \\(F\\), alors \\[\\mathbb{E}X = \\int_0^{+\\infty} (1-F(x)) \\mbox{d}x = \\int_0^{+\\infty} \\mathbb{P}(X&gt;x) \\mbox{d}x\\] Les autres propriétés de l’espérance, vues dans le chapitre sur les variables aléatoires discrètes, s’étendent sans difficultés aux variables continues (moments, moments centrés, variance, covariance, corrélation). 4.3 Exemples de variables aléatoires continues Théorème 4.3 L’espérance et la variance des lois continues de référence sont recensés dans le tableau ci-après : Loi Espérance Variance \\(\\mathcal{U}(a,b)\\) \\(\\frac{a+b}{2}\\) \\(\\frac{(b-a)^2}{12}\\) \\(\\mathcal{E}(\\lambda)\\) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{\\lambda^2}\\) \\(\\Gamma(k,\\theta)\\) \\(k \\theta\\) \\(k \\theta^2\\) \\(\\mathcal{N}(\\mu,\\sigma^2)\\) \\(\\mu\\) \\(\\sigma^2\\) 4.4 Dépendance Définition 4.4 Soient \\(X,Y\\) deux variables aléatoires réelles. Ces variables sont conjointement continues s’il existe une fonction positive \\(f\\) telle que: \\[F(x,y) = \\mathbb{P}(X \\leq x , Y \\leq y) = \\int_{-\\infty}^x \\int_{-\\infty}^y f(u,v) \\mbox{d}u \\mbox{d}v\\] f est une densité du couple \\((X,Y)\\). Si \\(F\\) est dérivable, on peut prendre \\[f = \\frac{\\partial^2 F}{\\partial x \\partial y}\\] Lemme 4.6 On a \\[ \\mathbb{P}(a \\leq X \\leq b ; c \\leq Y \\leq d) = F(b,d) - F(a,d) - F(b,c)+F(a,c) = \\int_{a}^b \\int_{c}^d f(u,v) \\mbox{d}u \\mbox{d}v \\] Plus généralement, si \\(\\mathcal{E}\\) est un évènement, on a: \\[ \\mathbb{P}\\left( (X,Y) \\in \\mathcal{E} \\right) = \\iint_{\\mathcal{E}} f(u,v) \\mbox{d}u \\mbox{d}v \\] Définition 4.5 Les fonctions de répartition marginales de \\(X\\) et \\(Y\\) sont définies par \\[ F_X(x) = \\mathbb{P}(X \\leq x) = \\int_{-\\infty}^x \\int_{-\\infty}^{+\\infty} f(u,v) \\mbox{d}u \\mbox{d}v ; F_Y(y) = \\mathbb{P}(Y \\leq y) = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^y f(u,v) \\mbox{d}u \\mbox{d}v \\] On en déduit les densités marginales selon \\(X\\) et \\(Y\\) : \\[ f_X(x) = \\int_{-\\infty}^{+\\infty} f(x,y) \\mbox{d}y ; f_Y(y) = \\int_{-\\infty}^{+\\infty} f(x,y) \\mbox{d}x \\] Théorème 4.4 Soit \\(g : \\mathbb{R}^2 \\to \\mathbb{R}\\) telle que \\(g(X,Y)\\) soit une variable aléatoire. Alors, \\[ \\mathbb{E} g(X,Y) = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} g(u,v) f(u,v) \\mbox{d}u \\mbox{d}v \\] dès que cette intégrale est définie. Théorème 4.5 Les variables \\(X,Y\\) sont indépendantes si et seulement si \\[F_{X,Y} = F_X F_Y \\iff f_{X,Y} = f_x f_y.\\] Théorème 4.6 Soit \\(X,Y\\) deux variables aléatoires continues. Alors \\[\\left( \\mathbb{E}XY \\right)^2 \\leq \\left( \\mathbb{E} X^2\\right) \\left( \\mathbb{E} Y^2\\right)\\] avec égalité si et seulement si il existe des réels \\(\\lambda,\\mu\\) non tous nuls tels que \\[\\mathbb{P}(\\lambda X + \\mu Y = 0) = 1.\\] ## Loi conditionnelle et espérance conditionnelle Soient \\(X,Y\\) deux variables aléatoires continues, de densité jointe \\(f\\). Définition 4.6 La fonction de répartition conditionnelle de \\(Y\\) sachant \\(X=x\\) est définie par \\[ F_{Y|X}(y|x) = \\int_{-\\infty}^y \\frac{f(x,v)}{f_X(x)} \\mbox{d}v \\] pour tout \\(x\\) tel que \\(f_X(x)&gt;0\\). Elle est également notée \\[\\mathbb{P}(Y \\leq y | X = x)\\] Définition 4.7 La densité conditionnelle de \\(Y\\) sachant \\(X=x\\) est définie par \\[ f_{Y|X}(y|x) = \\frac{f(x,y)}{f_X(x)} \\] pour tout \\(x\\) tel que \\(f_X(x)&gt;0\\). Encore, \\[ f_{Y|X} = \\frac{f_{X,Y}}{f_X} \\] Théorème 4.7 L’espérance conditionnelle de \\(Y\\) sachant \\(X\\) est définie comme \\[\\mathbb{E}(Y|X=x) = \\int_{-\\infty}^{+\\infty} y f_{Y|X}(y|x) \\mbox{d}y ;\\] Elle vérifie la relation \\[ \\mathbb{E} \\left(\\mathbb{E}(Y|X) \\right) = \\mathbb{E} Y . \\] Plus généralement, pour toute fonction \\(g\\) pour laquelle l’expression a un sens, on a : \\[ \\mathbb{E} \\left(g(X) \\mathbb{E}(Y|X) \\right) = \\mathbb{E} Y g(X) . \\] ## Fonctions de variables aléatoires Théorème 4.8 Soit \\(X\\) une variable aléatoire avec une densité \\(f\\), et soit \\(g\\) une fonction réelle telle que \\(Y = g(X)\\) soit encore une variable aléatoire. La fonction de répartition de \\(Y\\) est définie par : \\[ \\mathbb{P}(Y \\leq y) = \\mathbb{P} \\left(X \\in g^{-1}(]-\\infty , y]) \\right) . \\] Plus généralement, on a le théorème suivant: Théorème 4.9 Soient \\(\\mathcal{U}, \\mathcal{V}\\) deux ouverts de \\(\\mathbb{R}^d\\), Soit \\(X\\) une variable aléatoire à valeurs dans \\(\\mathcal{U}\\) de densité \\(f\\) et soit \\(T\\) un difféomorphisme de classe \\(\\mathcal{C}^1 : \\mathcal {U} \\to \\mathcal{V}\\). La densité de la variable aléatoire \\(Y = \\varphi(X)\\) est définie par : \\[ f_Y(y) = \\left\\lbrace \\begin{array}{ccc} f_X(\\varphi^{-1}(y)) |J_{\\varphi^{-1}}(y)| &amp; \\mbox{si} &amp; y \\in \\mathcal{V}\\\\ 0 &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] en notant \\(|J_{\\varphi^{-1}}(y)|\\) le déterminant du jacobien de \\(\\varphi^{-1}\\), c’est-à-dire de la matrice de terme général \\(\\left(\\frac{\\partial \\varphi^{-1}_j}{ \\partial y_i} \\right)_{i,j}\\). Lemme 4.7 Pour \\(d=2\\), le changement de variables “passage en coordonnées polaires” transforme une densité \\(f(x,y)\\) en \\(f(r \\cos \\theta, r \\sin \\theta) r\\). 4.5 Somme de variables aléatoires 4.6 La loi normale multivariée, applications "],
["statistique-descriptive.html", "A Statistique descriptive A.1 Distribution univariée A.2 Distribution bivariée A.3 Séries temporelles", " A Statistique descriptive A.1 Distribution univariée A.2 Distribution bivariée A.3 Séries temporelles "],
["techniques-de-denombrement-en-probabilite.html", "B Techniques de dénombrement en probabilité", " B Techniques de dénombrement en probabilité "],
["fonctions-indicatrices.html", "C Fonctions indicatrices", " C Fonctions indicatrices Théorème C.1 L’application qui a un évènement \\(A\\) associe la fonction indicatrice \\[ \\mathbf{1}_A : x \\to \\left\\lbrace \\begin{array}{ccc} 1 &amp; \\mbox{si} &amp; x\\in A\\\\ 0 &amp; \\mbox{si} &amp; x \\notin A\\\\ \\end{array}\\right. \\] est bijective. En outre, si \\(A,B\\) sont deux évènements disjoints, on a: \\[\\mathbf{1}_{A \\cup B} = \\mathbf{1}_A+ \\mathbf{1}_B.\\] Par ailleurs, \\[ \\mathbf{1}_{A^c} = 1- \\mathbf{1}_A.\\] Enfin, si \\(A,B\\) sont deux évènements, \\[ \\mathbf{1}_{A \\cap B} = \\mathbf{1}_A \\mathbf{1}_B.\\] Cet outil permet (entre autres) une démonstration immédiate de l’égalité de Poincaré (formule d’inclusion-exclusion) : Si \\(B = \\bigcup_{i=1}^n A_i\\), \\[\\mathbf{1}_B =1- \\prod_{i=1}^n (1-\\mathbf{1}_{A_i})\\] Et en développant puis en prenant l’espérance, il vient: \\[\\mathbb{P}(B) = \\sum_i \\mathbb{P}(A_i) - \\sum_{i&lt;j} \\mathbb{P}(A_i \\cap A_j)+ \\ldots \\] "],
["espaces-lp.html", "D Espaces \\(L^p\\)", " D Espaces \\(L^p\\) "],
["echantillonage.html", "E Échantillonage", " E Échantillonage "]
]
