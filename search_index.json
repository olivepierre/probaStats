[
["index.html", "Probabilités et statistiques Introduction", " Probabilités et statistiques Pierre-Damien Olive Mis à jour le 2019-06-16 Introduction Le site avec la dernière version du cours est disponible sur : https://olivepierre.github.io/probaStats/ "],
["evenements-et-probabilites.html", "Chapitre 1 Évènements et probabilités 1.1 Évènements 1.2 Probabilités 1.3 Probabilités conditionnelles 1.4 Indépendance", " Chapitre 1 Évènements et probabilités 1.1 Évènements 1.2 Probabilités 1.3 Probabilités conditionnelles 1.4 Indépendance "],
["variables-aleatoires.html", "Chapitre 2 Variables aléatoires 2.1 Variables aléatoires 2.2 Variables discrètes et continues 2.3 Vecteurs aléatoires", " Chapitre 2 Variables aléatoires 2.1 Variables aléatoires 2.2 Variables discrètes et continues 2.3 Vecteurs aléatoires "],
["variables-aleatoires-discretes.html", "Chapitre 3 Variables aléatoires discrètes 3.1 Fonctions de masse 3.2 Indépendance 3.3 Espérance", " Chapitre 3 Variables aléatoires discrètes 3.1 Fonctions de masse Définition 3.1 La fonction de masse d’une variable aléatoire discrète \\(X\\) est la fonction \\(f : \\mathbb{R} \\mapsto \\left[0 ,1 \\right]\\) définie par: \\[ f(x) = \\mathbb{P}(X=x) . \\] Fonctions de répartitions et fonctions de masse sont reliées par les relations: \\[ F(x) = \\sum_{x_i \\leq x} f(x_i) ; f(x) = F(x) - F(x^{-}) . \\] Lemme 3.1 Une fonction \\(f : \\mathbb{R} \\mapsto \\left[0 ,1 \\right]\\) est une fonction de masse si et seulement si l’ensemble \\(\\lbrace x : f(x) &gt;0 \\rbrace\\) est dénombrable et \\(\\sum_{i} f(x_i) = 1\\), où les \\(x_1, \\ldots, x_n, \\ldots\\) sont les valeurs de \\(x\\) tel que \\(f(x)&gt;0\\). TODO : exemples du programme 3.2 Indépendance Définition 3.2 Deux variables aléatoires discrètes \\(X\\) et \\(Y\\) sont indépendantes si les évènements \\(\\lbrace X=x \\rbrace\\) et \\(\\lbrace Y=y \\rbrace\\) sont indépendants pour tout \\(x\\) et \\(y\\) : \\[\\mathbb{P}(X=x,Y=y) = \\mathbb{P}(X=x) \\mathbb{P}(Y=y) .\\] On peut étendre cette définition à un ensemble de variables aléatoires \\((X_i)_{i \\in I}\\).Dans ce cas, l’égalité ci-dessus devient: \\[ \\mathbb{P}(X_i = x_i \\forall i \\in J) = \\prod_{i \\in J} \\mathbb{P}(X_i = x_i)\\] pour toute partie finie \\(J\\) de \\(I\\) Théorème 3.1 Soient \\(X,Y\\) deux variables aléatoires discrètes indépendantes, \\(f,g\\) deux fonctions réelles. Alors les variables aléatoires discrètes \\(f(X)\\) et \\(g(Y)\\) sont encore indépendantes. 3.3 Espérance Définition 3.3 L’espérance d’une variables aléatoire discrète \\(X\\) avec une fonction de masse \\(f\\) est définie comme \\[ \\mathbb{E} X = \\sum_{x : f(x)&gt;0} x f(x) \\] lorsque cette somme est absolument convergente; la convergence absolue est requise afin que cette somme ait une même valeur quelque soit l’ordre dans lequel on somme les \\(x\\) (cf. cours analyse sur les familles sommables). En particulier, lorsque \\(A\\) est un évènement, on a \\[\\mathbb{E} \\mathbf{1}_A = \\mathbb{P}(A).\\] Afin de calculer les espérances de fonctions de variables aléatoires, on peut envisager de calculer la fonction de masse de cette nouvelle variable aléatoire. Cette approche est délicate et peu pertinente grâce au lemme: Lemme 3.2 Soit \\(X\\) une variable aléatoire discrète, \\(f\\) sa fonction de masse. Soit \\(g\\) une fonction réelle. Alors l’espérance de \\(g(X)\\) est définie par \\[ \\mathbb{E} g(X) = \\sum_{x} g(x) f(x) \\] lorsque cette somme est absolument convergente. On peut ainsi calculer les moments pour tout ordre (\\(m_k = \\mathbb{E} X^k\\)) ainsi que les moments centrés (\\(\\sigma_k = \\mathbb{E} (X - m_1)^k\\)). En particulier, on définit : Définition 3.4 Le moment d’ordre 1 s’appelle l’espérance de \\(X\\), le moment centré d’ordre 2 de \\(X\\) s’appelle la variance de \\(X\\) (notée \\(\\mathbb{V}X\\)) : c’est une quantité positive! On appelle \\(\\sigma = \\sqrt{\\sigma_2}\\) l’écart-type de \\(X\\). Lemme 3.3 On peut exprimer les moments centrés en fonction de moments d’ordres inférieurs ; en particulier, \\[\\sigma_2 = m_2 - m_1^2 \\] Théorème 3.2 L’opérateur $ $ est une forme linéaire ( pour $ $, $ X,Y $ deux variables aléatoires admettant une espérance, $ (X+Y) = + Y \\(), positive (\\) X 0$ entraine \\(\\mathbb{E}X \\geq 0\\)) vérifiant \\(\\mathbb{E}1 = 1\\). Par contre, il n’est pas vrai (en général) que \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Lemme 3.4 Si \\(X,Y\\) sont des variables aléatoires indépendantes, alors \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Définition 3.5 On dit que les variables aléatoires \\(X,Y\\) sont non corrélées si \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Ainsi, les variables indépendantes sont non-corrélées, mais l’inverse n’est pas vrai ! Théorème 3.3 Soient \\(X,Y\\) deux variables aléatoires, \\(\\lambda \\in \\mathbb{R}\\). Alors \\(\\mathbb{V} \\lambda X = \\lambda^2 \\mathbb{V}X\\) et \\(\\mathbb{V}(X+Y) = \\mathbb{V}X+\\mathbb{V}Y\\) si ces variables sont non-corrélées. "]
]
