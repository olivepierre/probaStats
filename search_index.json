[
["index.html", "Probabilités et statistiques Introduction", " Probabilités et statistiques Pierre-Damien Olive Mis à jour le 2019-10-17 Introduction Le site avec la dernière version du cours est disponible sur : https://olivepierre.github.io/probaStats/ "],
["évènements-et-probabilités.html", "Chapitre 1 Évènements et probabilités 1.1 Évènements 1.2 Probabilités 1.3 Probabilités conditionnelles 1.4 Indépendance", " Chapitre 1 Évènements et probabilités 1.1 Évènements 1.2 Probabilités 1.3 Probabilités conditionnelles 1.4 Indépendance "],
["variables-aléatoires.html", "Chapitre 2 Variables aléatoires 2.1 Variables aléatoires 2.2 Variables discrètes et continues 2.3 Vecteurs aléatoires", " Chapitre 2 Variables aléatoires 2.1 Variables aléatoires 2.2 Variables discrètes et continues 2.3 Vecteurs aléatoires "],
["variables-aléatoires-discrètes.html", "Chapitre 3 Variables aléatoires discrètes 3.1 Fonctions de masse 3.2 Indépendance 3.3 Espérance 3.4 Cas particuliers de variables discrètes aléatoires 3.5 Dépendance 3.6 Loi conditionnelle et espérance conditionnelle 3.7 Somme de variables aléatoires discrètes", " Chapitre 3 Variables aléatoires discrètes 3.1 Fonctions de masse Définition 3.1 La fonction de masse d’une variable aléatoire discrète \\(X\\) est la fonction \\(f : \\mathbb{R} \\mapsto \\left[0 ,1 \\right]\\) définie par: \\[ f(x) = \\mathbb{P}(X=x) . \\] Fonctions de répartitions et fonctions de masse sont reliées par les relations: \\[ F(x) = \\sum_{x_i \\leq x} f(x_i) ; f(x) = F(x) - F(x^{-}) . \\] Lemme 3.1 Une fonction \\(f : \\mathbb{R} \\mapsto \\left[0 ,1 \\right]\\) est une fonction de masse si et seulement si l’ensemble \\(\\lbrace x : f(x) &gt;0 \\rbrace\\) est dénombrable et \\(\\sum_{i} f(x_i) = 1\\), où les \\(x_1, \\ldots, x_n, \\ldots\\) sont les valeurs de \\(x\\) tel que \\(f(x)&gt;0\\). Exemple 3.1 On choisit un nombre entier entre \\(1\\) et \\(n\\), de façon équiprobable. Un tel choix définit une variable aléatoire discrète, de loi uniforme. Exemple 3.2 On lance une pièce \\(n\\) fois, et on tombe sur pile avec une probabilité \\(p\\). L’ensemble des évènements est \\(\\Omega = \\lbrace \\mbox{pile}, \\mbox{face} \\rbrace\\). Le nombre de fois que l’on obtient “pile” est une variable aléatoire discrète dont la fonction de masse vérifie: \\[ f(x) = \\left\\lbrace \\begin{array}{ccc} 0 &amp; \\mbox{si} &amp; x\\notin \\lbrace 0, \\ldots , n \\rbrace\\\\ \\binom{n}{k} p^k (1-p)^{n-k} &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] On dit que cette variable aléatoire suit une loi binômiale (notée \\(\\mathcal{B}(n,p)\\)). Lorsque \\(n=1\\), on dit que la variable aléatoire suit une loi de Bernoulli (notée \\(\\mathcal{B}(p)\\)). Exemple 3.3 On compte le nombre d’épreuves de Bernoulli nécessaires pour arriver sur “pile” pour la première fois. Ce nombre de lancer est une variables aléatoire discrète, à valeur dans \\(\\mathbb{N}^*\\) et de fonction de masse: \\[ f(k) = (1-p)^{k-1} p \\forall k \\in \\mathbb{N}^* \\] La loi correspondante s’appelle la loi géométrique, de paramètre \\(p\\), notée \\(\\mathcal{G}(p)\\). Exemple 3.4 Une variable aléatoire prenant ses valeurs dans \\(\\mathbb{N}\\) avec une fonction de masse \\[ f(k) = \\frac{\\lambda^k}{k!} e^{-\\lambda} \\forall k \\in \\mathbb{N}\\] suit une loi de Poisson de paramètre \\(\\lambda &gt;0\\), notée \\(\\mathcal{P}(\\lambda)\\). Exemple 3.5 Soit \\(0&lt;p&lt;1\\). On tire simultanément \\(n\\) boules dans une urne contenant \\(A\\) boules, dont \\(pA\\) gagnantes et \\((1-p)A\\) perdantes. On compte le nombre de boules gagnantes extraites. La loi de cette variable aléatoire discrète est la loi hypergéométrique, de fonction de masse: \\[ f(k) = \\frac{\\binom{pA}{k} \\binom{qA}{n-k}}{\\binom{A}{n}} \\] 3.2 Indépendance Définition 3.2 Deux variables aléatoires discrètes \\(X\\) et \\(Y\\) sont indépendantes si les évènements \\(\\lbrace X=x \\rbrace\\) et \\(\\lbrace Y=y \\rbrace\\) sont indépendants pour tout \\(x\\) et \\(y\\) : \\[\\mathbb{P}(X=x,Y=y) = \\mathbb{P}(X=x) \\mathbb{P}(Y=y) .\\] On peut étendre cette définition à un ensemble de variables aléatoires \\((X_i)_{i \\in I}\\).Dans ce cas, l’égalité ci-dessus devient: \\[ \\mathbb{P}(X_i = x_i \\forall i \\in J) = \\prod_{i \\in J} \\mathbb{P}(X_i = x_i)\\] pour toute partie finie \\(J\\) de \\(I\\) Théorème 3.1 Soient \\(X,Y\\) deux variables aléatoires discrètes indépendantes, \\(f,g\\) deux fonctions réelles. Alors les variables aléatoires discrètes \\(f(X)\\) et \\(g(Y)\\) sont encore indépendantes. 3.3 Espérance Définition 3.3 L’espérance d’une variables aléatoire discrète \\(X\\) avec une fonction de masse \\(f\\) est définie comme \\[ \\mathbb{E} X = \\sum_{x : f(x)&gt;0} x f(x) \\] lorsque cette somme est absolument convergente; la convergence absolue est requise afin que cette somme ait une même valeur quelque soit l’ordre dans lequel on somme les \\(x\\) (cf. cours analyse sur les familles sommables). En particulier, lorsque \\(A\\) est un évènement, on a \\[\\mathbb{E} \\mathbf{1}_A = \\mathbb{P}(A).\\] Afin de calculer les espérances de fonctions de variables aléatoires, on peut envisager de calculer la fonction de masse de cette nouvelle variable aléatoire. Cette approche est délicate et peu pertinente grâce au lemme: Lemme 3.2 Soit \\(X\\) une variable aléatoire discrète, \\(f\\) sa fonction de masse. Soit \\(g\\) une fonction réelle. Alors l’espérance de \\(g(X)\\) est définie par \\[ \\mathbb{E} g(X) = \\sum_{x} g(x) f(x) \\] lorsque cette somme est absolument convergente. On peut ainsi calculer les moments pour tout ordre (\\(m_k = \\mathbb{E} X^k\\)) ainsi que les moments centrés (\\(\\sigma_k = \\mathbb{E} (X - m_1)^k\\)). En particulier, on définit : Définition 3.4 Le moment d’ordre 1 s’appelle l’espérance de \\(X\\), le moment centré d’ordre 2 de \\(X\\) s’appelle la variance de \\(X\\) (notée \\(\\mathbb{V}X\\)) : c’est une quantité positive! On appelle \\(\\sigma = \\sqrt{\\sigma_2}\\) l’écart-type de \\(X\\). Lemme 3.3 On peut exprimer les moments centrés en fonction de moments d’ordres inférieurs ; en particulier, \\[\\sigma_2 = m_2 - m_1^2 \\] Théorème 3.2 L’opérateur \\(\\mathbb{E}\\) est une forme linéaire ( pour \\(\\lambda \\in \\mathbb{R}\\), \\(X,Y\\) deux variables aléatoires admettant une espérance, \\(\\mathbb{E} (X+\\lambda Y) = \\mathbb{X} + \\lambda \\mathbb{E} Y\\)), positive (\\(X \\geq 0\\) entraine \\(\\mathbb{E}X \\geq 0\\)) vérifiant \\(\\mathbb{E}1 = 1\\). Par contre, il n’est pas vrai (en général) que \\[\\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Lemme 3.4 Si \\(X,Y\\) sont des variables aléatoires indépendantes, alors \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Définition 3.5 On dit que les variables aléatoires \\(X,Y\\) sont non corrélées si \\[ \\mathbb{E} XY = (\\mathbb{E}X) ( \\mathbb{E} Y) . \\] Ainsi, les variables indépendantes sont non-corrélées, mais l’inverse n’est pas vrai ! Théorème 3.3 Soient \\(X,Y\\) deux variables aléatoires, \\(\\lambda \\in \\mathbb{R}\\). Alors \\(\\mathbb{V} \\lambda X = \\lambda^2 \\mathbb{V}X\\) et \\(\\mathbb{V}(X+Y) = \\mathbb{V}X+\\mathbb{V}Y\\) si ces variables sont non-corrélées. 3.4 Cas particuliers de variables discrètes aléatoires Théorème 3.4 Soit \\(X\\) une variable aléatoire suivant une loi uniforme. Alors \\[\\mathbb{E}X = \\frac{n+1}{2} , \\mathbb{V}X = \\frac{n^2-1}{12}\\] Théorème 3.5 Soit \\(X\\) une variable aléatoire suivant une loi de Bernoulli. Alors \\[\\mathbb{E}X = p , \\mathbb{V}X = p(1-p)\\] Théorème 3.6 Soit \\(X\\) une variable aléatoire suivant une loi binômiale \\(\\mathcal{B}(n,p)\\). Alors \\[\\mathbb{E}X = np , \\mathbb{V}X = np(1-p)\\] Théorème 3.7 Soit \\(X\\) une variable aléatoire suivant une loi géométrique \\(\\mathcal{G}(p)\\). Alors \\[\\mathbb{E}X = \\frac{1}{p} , \\mathbb{V}X = \\frac{1-p}{p^2}\\] Théorème 3.8 Soit \\(X\\) une variable aléatoire suivant une loi de Poisson \\(\\mathcal{P}(\\lambda)\\). Alors \\[\\mathbb{E}X = \\lambda , \\mathbb{V}X = \\lambda\\] 3.5 Dépendance Définition 3.6 Soient \\(X,Y\\) deux variables aléatoires discrètes. La fonction de répartition jointe est définie par \\[F(x,y) = \\mathbb{P}(X \\leq x , Y \\leq y).\\] La fonction de masse jointe est donnée par \\[ f(x,y) = \\mathbb{P}(X=x, Y=y).\\] Lemme 3.5 Les variables aléatoires discrètes \\(X,Y\\) sont indépendantes si et seulement si \\[ f_{X,Y}(x,y) = f_X(x) f_Y(y). \\] Plus généralement, \\(X,Y\\) sont indépendantes si et seulement si il existe des fonctions réelles \\(f,g\\) telles que \\[ f_{X,Y}(x,y) = f(x) g(y). \\] A partir de la connaissance de \\(f_{X,Y}\\), il est possible de calculer \\(f_X\\) ou \\(f_Y\\) grâce au théorème suivant : Théorème 3.9 On a: \\[f_X(x) = \\sum_y f_{X,Y}(x,y)\\] \\[f_Y(y) = \\sum_x f_{X,Y}(x,y)\\] Le calcul de l’espérance d’une fonction de variables aléatoires est rendu facile par le Lemme 3.6 Soit \\(g\\) une fonction telle que \\(g(X,Y)\\) soit une variable aléatoire. Alors \\[\\mathbb{E} g(X,Y) = \\sum_{x,y} g(x,y)f_{X,Y}(x,y)\\] si cette somme existe. Définition 3.7 Soient \\(X,Y\\) deux variables aléatoires. Leur covariance est définie par: \\[cov(X,Y) = \\mathbb{E} \\left[ (X- \\mathbb{E}X) (Y - \\mathbb{E}Y)\\right] = \\mathbb{E}XY - \\mathbb{E}X \\mathbb{E}Y.\\] Le coefficient de corrélation est : \\[cor(X,Y) = \\frac{cov(X,Y)}{\\sqrt{\\mathbb{V}X \\mathbb{V}Y}}.\\] Lemme 3.7 Le coefficient de corrélation est en module inférieur à 1 et égal à \\(\\pm 1\\) si seulement si il existe des réels \\(a,b,c\\) tels que \\[\\mathbb{P}(aX+bY+c=0)=1.\\] Théorème 3.10 Soit \\(X,Y\\) deux variables aléatoires discrètes. Alors \\[\\left( \\mathbb{E}XY \\right)^2 \\leq \\left( \\mathbb{E} X^2\\right) \\left( \\mathbb{E} Y^2\\right)\\] avec égalité si et seulement si il existe des réels \\(\\lambda,\\mu\\) non tous nuls tels que \\[\\mathbb{P}(\\lambda X + \\mu Y = 0) = 1.\\] 3.6 Loi conditionnelle et espérance conditionnelle Soient \\(X,Y\\) deux variables aléatoires discrètes. Définition 3.8 La fonction de répartion conditionnelle de \\(Y\\) sachant \\(X=x\\), notée \\(F_{Y|X}(\\bullet | x)\\) est définie par \\[ F_{Y|X}(y | x) = \\mathbb{P}(Y \\leq y | X=x) \\] pour tout \\(x\\) tel que \\(\\mathbb{P}(X=x)&gt;0\\). La fonction de masse conditionnelle de \\(Y\\) sachant \\(X=x\\), notée \\(f_{Y|X}(\\bullet | x)\\) est définie par \\[ f_{Y|X}(y | x) = \\mathbb{P}(Y = y | X=x) \\] pour tout \\(x\\) tel que \\(\\mathbb{P}(X=x)&gt;0\\). On a donc \\[ f_{Y|X} = f_{X,Y} / f_X \\] lorsque cette expression est définie. Encore, \\(f_{Y|X} = f_Y\\) si et seulement si \\(X\\) et \\(Y\\) sont indépendantes. Définition 3.9 L’espérance conditionnelle de \\(Y\\) par rapport à \\(X\\) est définie comme \\[\\mathbb{E}(Y | X=x) = \\sum_y y f_{Y|X}(y | x)\\] Théorème 3.11 On a : \\[\\mathbb{E} Y = \\mathbb{E} \\left( \\mathbb{E}(Y | X)\\right)\\] Théorème 3.12 Pour toute fonction \\(g\\) pour laquelle les sommes sont définies, on a : \\[\\mathbb{E} \\left(Y g(X)\\right) = \\mathbb{E} \\left( g(X) \\mathbb{E}(Y | X)\\right)\\] Définition 3.10 La variance conditionnelle de \\(Y\\) sachant \\(X\\) est définie par \\[\\mathbb{V}(Y | X) = \\mathbb{E} \\left( [Y- \\mathbb{E}(Y | X)]^2 | X\\right).\\] La variance conditionnelle est liée à l’espérance conditionnelle par le théorème : Théorème 3.13 \\[\\mathbb{V}(Y)=\\mathbb{E}(\\mathbb{V}[Y| X])+\\mathbb{V}(\\mathbb{E}[Y|X])\\] 3.7 Somme de variables aléatoires discrètes Soient \\(X,Y\\) deux variables aléatoires discrètes. Théorème 3.14 La loi de \\(X+Y\\) est définie par \\[ \\mathbb{P}(X+Y=z) = \\sum_x f_{X,Y} (x,z-x) \\] Si, en outre, \\(X\\) et \\(Y\\) sont indépendantes, alors \\[ \\mathbb{P}(X+Y=z) = \\sum_x f_X(x) f_Y(z-x) = \\sum_y f_X(z-y) f_Y(y) \\] La fonction masse de \\(X+Y\\) s’appelle alors le produit de convolution des fonctions masses de \\(X\\) et \\(Y\\), et est noté \\(f_X \\star f_Y\\). "],
["variables-aléatoires-continues.html", "Chapitre 4 Variables aléatoires continues 4.1 densités de probabilités 4.2 Indépendance 4.2 Espérance 4.3 Exemples de variables aléatoires continues 4.4 Dépendance 4.5 Somme de variables aléatoires 4.6 La loi normale multivariée 4.7 Lois issues de la loi normale", " Chapitre 4 Variables aléatoires continues 4.1 densités de probabilités Définition 4.1 Une variable aléatoire réelle \\(X\\) est dite continue s’il existe une fonction f positive telle que \\[F_X(x) = \\int_{-\\infty}^x f(t) \\mbox{d}t\\] Une telle fonction est une densité de \\(X\\). Cette densité n’est pas unique. Une telle densité est généralement notée \\(f_X\\). Lorsque \\(F_X\\) est dérivable, \\(F_X&#39;\\) est une densité de \\(X\\). Lemme 4.1 Si \\(X\\) admet \\(f\\) comme densité, alors: \\(\\int_{-\\infty}^{+\\infty} f(x) \\mbox{d}x = 1\\); \\(\\mathbb{P}(X=x)=0, \\forall x \\in \\mathbb{R}\\); \\(\\mathbb{P}(X \\in \\mathcal{E}) = \\int_{\\mathcal{E}} f(x) \\mbox{d}x\\) pour tout évènement \\(\\mathcal{E}\\); \\(X\\) et \\(-X\\) ont les mêmes distributions si et seulement si \\(f_X\\) est paire; Si \\(f\\) et \\(g\\) sont des densités, il en est de même pour \\(\\lambda f + (1-\\lambda)g\\) pour \\(0 \\leq \\lambda \\leq 1\\). Exemple 4.1 Soient \\(a,b\\) deux réels avec \\(a&lt;b\\). La loi uniforme sur l’intervalle \\([a,b]\\) est définie par sa densité de probabilité \\[ f(x) = \\left\\lbrace \\begin{array}{ccc} \\frac{1}{b-a} &amp; \\mbox{si} &amp; a \\leq x \\leq b\\\\ 0 &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] On note cette loi \\(\\mathcal{U}(a,b)\\). Exemple 4.2 La loi exponentielle modélise la durée de vie d’un phénomène sans mémoire : la probabilité que le phénomène dure au moins \\(s+t\\) sachant qu’il a déjà duré \\(t\\) est la même que la probabilié de durer \\(s\\) à partir de la mise en fonction initiale. Soit \\(\\lambda &gt; 0\\). La densité associée à cette loi est : \\[ f(t) = \\left\\lbrace \\begin{array}{ccc} \\lambda e^{-\\lambda t} &amp; \\mbox{si} &amp; t&gt;0\\\\ 0 &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] On note la loi exponentielle de paramètre \\(\\lambda\\) : \\(\\mathcal{E}(\\lambda)\\). Lemme 4.2 Soient \\(X,Y\\) deux variables aléatoires qui suivent deux lois exponentielles, de paramètres \\(\\lambda, \\mu\\), alors \\(\\min (X,Y)\\) suit encore une loi exponentielle, de paramètre \\(\\lambda + \\mu\\). Exemple 4.3 La loi Gamma (qui généralise la loi exponentielle) modélise également des durées; elle est caractérisée par deux paramètres \\(k,\\theta\\) positifs. Sa densité est \\[ f(t) = \\left\\lbrace \\begin{array}{ccc} \\frac{t^{k-1} \\exp \\left(-\\frac{t}{\\theta} \\right)}{\\theta^k \\Gamma (k)} &amp; \\mbox{si} &amp; x&gt;0\\\\ 0 &amp; \\mbox{sinon} &amp; \\\\ \\end{array}\\right. \\] où \\(\\Gamma\\) désigne la fonction Gamma d’Euler (\\(\\Gamma : z \\mapsto \\int_0^{+\\infty} t^{z-1} e^{-t} \\mbox{d}t\\)). On note cette loi \\(\\Gamma(k,\\theta)\\). Lemme 4.3 Soient \\(X_1, \\ldots, X_n\\) des variables aléatoires indépendantes, de loi \\(\\Gamma(k_1,\\theta),\\ldots,\\Gamma(k_n,\\theta)\\). Alors \\(X_1+\\ldots+X_n\\) suit une loi \\(\\Gamma(\\sum_{i=1}^n k_i,\\theta)\\). Lemme 4.4 Soit \\(X \\sim \\Gamma(k,\\theta)\\) et \\(t&gt;0\\). Alors \\(tX \\sim \\Gamma(k,t \\theta)\\). Exemple 4.4 La loi normale est définie suivant deux paramètres (\\(\\mu \\in \\mathbb{R}, \\sigma &gt;0\\)) par sa densité: \\[ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x-\\mu)^2}{2 \\sigma^2}\\right) \\] On note la loi \\(\\mathcal{N}(\\mu,\\sigma^2)\\). Exemple 4.5 Soit \\(X\\) une variable aléatoire qui suit une loi normale \\(\\mathcal{N}(\\mu,\\sigma^2)\\). Alors \\(\\exp(X)\\) suit une loi log-normale. 4.2 Indépendance Définition 4.2 Soient \\(X,Y\\) deux variables aléatoires réelles. On dit que les variables aléatoires \\(X\\) et \\(Y\\) sont indépendantes si les évènements \\(\\lbrace X \\leq x \\rbrace\\) et \\(\\lbrace Y \\leq y \\rbrace\\) sont indépendants pour tous \\(x,y\\) réels. Théorème 4.1 Soient \\(X,Y\\) deux variables aléatoires, \\(f,g\\) deux fonctions telles que \\(f(X), g(Y)\\) soient des variables aléatoires. Si \\(X\\) et \\(Y\\) sont indépendantes, il en est de même pour \\(f(X)\\) et \\(g(Y)\\).``` 4.2 Espérance Définition 4.3 L’espérance d’une variable aléatoire \\(X\\) de densité \\(f\\) est définie par \\[\\mathbb{E}X = \\int_{-\\infty}^{+\\infty} x f(x) \\mbox{d}x\\] lorsque cette intégrale existe. Théorème 4.2 Si \\(X\\) et \\(g(X)\\) sont des variables aléatoires, alors \\[\\mathbb{E}g(X) = \\int_{-\\infty}^{+\\infty} g(x) f(x) \\mbox{d}x\\] Lemme 4.5 Si \\(X\\) admet une densité nulle sur \\(\\mathbb{R}^-\\) et une fonction de répartition \\(F\\), alors \\[\\mathbb{E}X = \\int_0^{+\\infty} (1-F(x)) \\mbox{d}x = \\int_0^{+\\infty} \\mathbb{P}(X&gt;x) \\mbox{d}x\\] Les autres propriétés de l’espérance, vues dans le chapitre sur les variables aléatoires discrètes, s’étendent sans difficultés aux variables continues (moments, moments centrés, variance, covariance, corrélation). 4.3 Exemples de variables aléatoires continues Théorème 4.3 L’espérance et la variance des lois continues de référence sont recensés dans le tableau ci-après : Loi Espérance Variance \\(\\mathcal{U}(a,b)\\) \\(\\frac{a+b}{2}\\) \\(\\frac{(b-a)^2}{12}\\) \\(\\mathcal{E}(\\lambda)\\) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{\\lambda^2}\\) \\(\\Gamma(k,\\theta)\\) \\(k \\theta\\) \\(k \\theta^2\\) \\(\\mathcal{N}(\\mu,\\sigma^2)\\) \\(\\mu\\) \\(\\sigma^2\\) 4.4 Dépendance Définition 4.4 Soient \\(X,Y\\) deux variables aléatoires réelles. Ces variables sont conjointement continues s’il existe une fonction positive \\(f\\) telle que: \\[F(x,y) = \\mathbb{P}(X \\leq x , Y \\leq y) = \\int_{-\\infty}^x \\int_{-\\infty}^y f(u,v) \\mbox{d}u \\mbox{d}v\\] f est une densité du couple \\((X,Y)\\). Si \\(F\\) est dérivable, on peut prendre \\[f = \\frac{\\partial^2 F}{\\partial x \\partial y}\\] Lemme 4.6 On a \\[ \\mathbb{P}(a \\leq X \\leq b ; c \\leq Y \\leq d) = F(b,d) - F(a,d) - F(b,c)+F(a,c) = \\int_{a}^b \\int_{c}^d f(u,v) \\mbox{d}u \\mbox{d}v \\] Plus généralement, si \\(\\mathcal{E}\\) est un évènement, on a: \\[ \\mathbb{P}\\left( (X,Y) \\in \\mathcal{E} \\right) = \\iint_{\\mathcal{E}} f(u,v) \\mbox{d}u \\mbox{d}v \\] Définition 4.5 Les fonctions de répartition marginales de \\(X\\) et \\(Y\\) sont définies par \\[ F_X(x) = \\mathbb{P}(X \\leq x) = \\int_{-\\infty}^x \\int_{-\\infty}^{+\\infty} f(u,v) \\mbox{d}u \\mbox{d}v ; F_Y(y) = \\mathbb{P}(Y \\leq y) = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^y f(u,v) \\mbox{d}u \\mbox{d}v \\] On en déduit les densités marginales selon \\(X\\) et \\(Y\\) : \\[ f_X(x) = \\int_{-\\infty}^{+\\infty} f(x,y) \\mbox{d}y ; f_Y(y) = \\int_{-\\infty}^{+\\infty} f(x,y) \\mbox{d}x \\] Théorème 4.4 Soit \\(g : \\mathbb{R}^2 \\to \\mathbb{R}\\) telle que \\(g(X,Y)\\) soit une variable aléatoire. Alors, \\[ \\mathbb{E} g(X,Y) = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} g(u,v) f(u,v) \\mbox{d}u \\mbox{d}v \\] dès que cette intégrale est définie. Théorème 4.5 Les variables \\(X,Y\\) sont indépendantes si et seulement si \\[F_{X,Y} = F_X F_Y \\iff f_{X,Y} = f_x f_y.\\] Théorème 4.6 Soit \\(X,Y\\) deux variables aléatoires continues. Alors \\[\\left( \\mathbb{E}XY \\right)^2 \\leq \\left( \\mathbb{E} X^2\\right) \\left( \\mathbb{E} Y^2\\right)\\] avec égalité si et seulement si il existe des réels \\(\\lambda,\\mu\\) non tous nuls tels que \\[\\mathbb{P}(\\lambda X + \\mu Y = 0) = 1.\\] ## Loi conditionnelle et espérance conditionnelle Soient \\(X,Y\\) deux variables aléatoires continues, de densité jointe \\(f\\). Définition 4.6 La fonction de répartition conditionnelle de \\(Y\\) sachant \\(X=x\\) est définie par \\[ F_{Y|X}(y|x) = \\int_{-\\infty}^y \\frac{f(x,v)}{f_X(x)} \\mbox{d}v \\] pour tout \\(x\\) tel que \\(f_X(x)&gt;0\\). Elle est également notée \\[\\mathbb{P}(Y \\leq y | X = x)\\] Définition 4.7 La densité conditionnelle de \\(Y\\) sachant \\(X=x\\) est définie par \\[ f_{Y|X}(y|x) = \\frac{f(x,y)}{f_X(x)} \\] pour tout \\(x\\) tel que \\(f_X(x)&gt;0\\). Encore, \\[ f_{Y|X} = \\frac{f_{X,Y}}{f_X} \\] Théorème 4.7 L’espérance conditionnelle de \\(Y\\) sachant \\(X\\) est définie comme \\[\\mathbb{E}(Y|X=x) = \\int_{-\\infty}^{+\\infty} y f_{Y|X}(y|x) \\mbox{d}y ;\\] Elle vérifie la relation \\[ \\mathbb{E} \\left(\\mathbb{E}(Y|X) \\right) = \\mathbb{E} Y . \\] Plus généralement, pour toute fonction \\(g\\) pour laquelle l’expression a un sens, on a : \\[ \\mathbb{E} \\left(g(X) \\mathbb{E}(Y|X) \\right) = \\mathbb{E} Y g(X) . \\] ## Fonctions de variables aléatoires Théorème 4.8 Soit \\(X\\) une variable aléatoire avec une densité \\(f\\), et soit \\(g\\) une fonction réelle telle que \\(Y = g(X)\\) soit encore une variable aléatoire. La fonction de répartition de \\(Y\\) est définie par : \\[ \\mathbb{P}(Y \\leq y) = \\mathbb{P} \\left(X \\in g^{-1}(]-\\infty , y]) \\right) . \\] Plus généralement, on a le théorème suivant: Théorème 4.9 Soient \\(\\mathcal{U}, \\mathcal{V}\\) deux ouverts de \\(\\mathbb{R}^d\\), Soit \\(X\\) une variable aléatoire à valeurs dans \\(\\mathcal{U}\\) de densité \\(f\\) et soit \\(T\\) un difféomorphisme de classe \\(\\mathcal{C}^1 : \\mathcal {U} \\to \\mathcal{V}\\). La densité de la variable aléatoire \\(Y = \\varphi(X)\\) est définie par : \\[ f_Y(y) = \\left\\lbrace \\begin{array}{ccc} f_X(\\varphi^{-1}(y)) |J_{\\varphi^{-1}}(y)| &amp; \\mbox{si} &amp; y \\in \\mathcal{V}\\\\ 0 &amp; \\mbox{sinon} &amp;\\\\ \\end{array}\\right. \\] en notant \\(|J_{\\varphi^{-1}}(y)|\\) le déterminant du jacobien de \\(\\varphi^{-1}\\), c’est-à-dire de la matrice de terme général \\(\\left(\\frac{\\partial \\varphi^{-1}_j}{ \\partial y_i} \\right)_{i,j}\\). Lemme 4.7 Pour \\(d=2\\), le changement de variables “passage en coordonnées polaires” transforme une densité \\(f(x,y)\\) en \\(f(r \\cos \\theta, r \\sin \\theta) r\\). 4.5 Somme de variables aléatoires Théorème 4.10 Si \\(X,Y\\) ont pour densité jointe \\(f\\), alors la densité de \\(Z = X+Y\\) est définie par: \\[ f_Z(z) = \\int_{-\\infty}^{+\\infty} f(x,z-x) \\mbox{d}x \\] Si, \\(X\\) et \\(Y\\) sont indépendantes, le résultat devient : \\[ f_Z(z) = \\int_{-\\infty}^{+\\infty} f_X(z-y) f_Y(y) \\mbox{d}y = \\int_{-\\infty}^{+\\infty} f_X(x) f_Y(z-x) \\mbox{d}x \\] On dit que \\(f_Z\\) est le produit de convolution de \\(f_X\\) et \\(f_Y\\), noté \\(f_X \\star f_Y\\). Lemme 4.8 Soit \\(X \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)\\) et \\(Y \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2)\\). Supposons que \\(X\\) et \\(Y\\) sont indépendantes. Alors \\(X+Y \\sim \\mathcal{N}(\\mu_1 + \\mu_2,\\sigma_1^2 + \\sigma_2^2)\\). 4.6 La loi normale multivariée Définition 4.8 Le vecteur \\(X = (X_1, \\ldots, X_n)\\) suit une loi normale multivariée (notée \\(\\mathcal{N}(\\mu,\\Sigma)\\)) s’il existe un vecteur \\(\\mu = (\\mu_1,\\ldots,\\mu_n)\\) de réels et une matrice symétrique définie positive \\(\\Sigma\\) telle que la densité jointe de \\(X\\) soit: \\[f_X(x) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} \\exp \\left( -\\frac12 (x-\\mu)\\Sigma^{-1}(x-\\mu)&#39; \\right)\\] De façon équivalente, on dit que le vecteur \\(X\\) suit une loi normale multivariée si et seulement si pour tout vecteur \\(a \\in \\mathbb{R}^n\\), \\(Xa&#39;\\) suit une loi normale. Théorème 4.11 Si \\(X\\) suit une loi \\(\\mathcal{N}(\\mu,\\Sigma)\\), alors \\(\\mathbb{E}X = \\mu\\) la matrice \\(\\Sigma\\) est la matrice de covariance de \\(X\\): \\[\\Sigma_{i,j} = \\text{cov} (X_i,X_j)\\] Si \\(A\\) est une matrice de rang \\(m \\leq n\\), alors \\(XA\\) suit la loi \\(\\mathcal{N}(\\mu A,A&#39; \\Sigma A)\\) 4.7 Lois issues de la loi normale On dispose de variables aléatoires correspondants aux résultats d’un même expérience aléatoire \\(X_1,\\ldots,X_n\\) dont on suppose qu’elle sont de loi normale \\(\\mathcal{N}(\\mu,\\sigma^2)\\) pour des quantités \\(\\mu,\\sigma\\) non observée. On cherche à estimer ces quantités. Pour celà, on va considérer la moyenne empirique \\[\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\] comme estimateur de \\(\\mu\\) et la variance empirique \\[S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2\\] comme estimateur de \\(\\sigma^2\\). Théorème 4.12 Soit \\(X_1,\\ldots,X_n\\) des variables aléatoires indépendantes, de même loi \\(\\mathcal{N}(\\mu,\\sigma^2)\\). \\(\\overline{X}\\) et \\(S^2\\) sont indépendants; \\(\\overline{X} \\sim \\mathcal{N}(\\mu,\\sigma^2 / n)\\) \\((n-1)S^2/\\sigma^2 \\sim \\Gamma(\\frac12, \\frac{n-1}{2})\\). Cette dernière loi s’appelle la loi du \\(\\chi^2\\) à \\(n-1\\) degrés de liberté, notée \\(\\chi^2(n-1)\\) ; c’est la loi de la somme des carrés de \\(n-1\\) lois normales centrées réduites indépendantes. Les variables aléatoires \\[ U = \\frac{n-1}{\\sigma^2}S^2 \\sim \\chi^2(n-1)\\] et \\[V = \\frac{\\sqrt{n}}{\\sigma}(\\overline{X}-\\mu) \\sim \\mathcal{N}(0,1).\\] Elles ne dépendent pas de \\(\\sigma\\). Le ratio \\[T = \\frac{V}{\\sqrt{U/(n-1)}}\\] dont le numérateur suit \\(\\mathcal{N}(0,1)\\) et le dénominateur est la racine d’un \\(\\chi^2(n-1)\\), divisé par \\(n-1\\) suit une loi de Student, de paramètre \\(n-1\\), notée \\(t(n-1)\\). Soient \\(U,V\\) deux variables aléatoires qui suivent respectivement les lois \\(\\chi^2(r)\\) et \\(\\chi^2(s)\\), alors \\[F = \\frac{U/r}{V/s}\\] suit une loi de Fisher de paramètres \\((r,s)\\), notée \\(F(r,s)\\). Dans ce cas, \\(1/F\\) suit la loi \\(F(s,r)\\) et si \\(T \\sim t(r)\\), \\(T^2 \\sim F(1,r)\\) "],
["convergence-de-variables-aléatoires.html", "Chapitre 5 Convergence de variables aléatoires 5.1 Modes de convergence 5.2 Inégalités en probabilités 5.3 Applications", " Chapitre 5 Convergence de variables aléatoires 5.1 Modes de convergence Définition 5.1 Soient \\(X, X_1, \\ldots, X_n\\) des variables aléatoires. On dit que : \\(X_n \\to X\\) presque sûrement (noté \\(X_n \\xrightarrow{p.s} X\\)) si \\[ \\mathbb{P}\\left( \\left\\{ \\omega \\in \\Omega : X_n (\\omega) \\xrightarrow[n \\to +\\infty]{} X (\\omega) \\right\\} \\right) = 1 \\] \\(X_n \\to X\\) dans \\(L^p\\) (\\(p \\geq 1\\)) (noté \\(X_n \\xrightarrow{L^p} X\\)) si \\(\\mathbb{E} |X_n|^p &lt; + \\infty\\) et \\[ \\mathbb{E} |X_n - X|^p \\xrightarrow[n \\to +\\infty]{} 0 \\] \\(X_n \\to X\\) en probabilité (noté \\(X_n \\xrightarrow{\\mathbb{P}} X\\)) si \\[ \\mathbb{P} \\left( |X_n-X|&gt; \\epsilon \\right) \\xrightarrow[n \\to +\\infty]{} 0 \\forall \\epsilon &gt;0 \\] \\(X_n \\to X\\) en loi, ou converge faiblement (noté \\(X_n \\xrightarrow{\\mathcal{L}} X\\)) si \\[ \\mathbb{P} \\left( X_n \\leq x \\right) \\xrightarrow[n \\to +\\infty]{} \\mathbb{P} \\left( X \\leq x \\right) \\text{pour tous les $x$ où la fonction $F_X$ est continue.} \\] Notons que si \\(X_n \\xrightarrow{\\mathcal{L}} X\\), \\(X_n \\xrightarrow{\\mathcal{L}} Y\\) pour tout \\(Y\\) avec la même loi que \\(X\\). En particulier, on a les résultats suivants pour la convergence faible. Théorème 5.1 Si \\(X_n \\xrightarrow{\\mathcal{L}} X\\) et \\(g\\) est une fonction continue, alors \\(g(X_n) \\xrightarrow{\\mathcal{L}} g(X)\\). Par ailleurs, on a les équivalences : \\(X_n \\xrightarrow{\\mathcal{L}} X\\) \\(\\mathbb{E} g\\left(X_n\\right) \\to \\mathbb{E} g\\left(X\\right)\\) pour toute fonction \\(g\\) continue bornée \\(\\mathbb{E} g\\left(X_n\\right) \\to \\mathbb{E} g\\left(X\\right)\\) pour toute fonction \\(g\\) de la forme \\(g(x) = f(x) \\mathbb{1}_{[a,b]}(x)\\) où \\(f\\) est continue sur \\([a,b]\\) et \\(a,b\\) sont des points de continuité de la fonction de répartition de \\(X\\). Théorème 5.2 Si \\(X_n \\xrightarrow{\\mathbb{P}} X\\) et \\(g\\) est une fonction continue, alors \\(g(X_n) \\xrightarrow{\\mathbb{P}} g(X)\\) Théorème 5.3 Ces modes de convergence ne sont pas équivalents: \\(\\left( X_n \\xrightarrow{p.s} X \\right) \\Rightarrow \\left( X_n \\xrightarrow{\\mathbb{P}} X \\right)\\) \\(\\left( X_n \\xrightarrow{L^p} X \\right) \\Rightarrow \\left( X_n \\xrightarrow{\\mathbb{P}} X \\right)\\) \\(\\left( X_n \\xrightarrow{\\mathbb{P}} X \\right) \\Rightarrow \\left( X_n \\xrightarrow{\\mathcal{L}} X \\right)\\) Les implications inverses sont fausses en général. De plus, si \\(q &gt; p \\geq 1\\), alors \\[ \\left( X_n \\xrightarrow{L^q} X \\right) \\Rightarrow \\left( X_n \\xrightarrow{L^p} X \\right) \\] Théorème 5.4 On a : Si \\(X_n \\xrightarrow{p.s} X\\) et \\(Y_n \\xrightarrow{p.s} Y\\), alors \\(X_n + Y_n \\xrightarrow{p.s} X + Y\\) Si \\(X_n \\xrightarrow{L^p} X\\) et \\(Y_n \\xrightarrow{L^p} Y\\), alors \\(X_n + Y_n \\xrightarrow{L^p} X + Y\\) Si \\(X_n \\xrightarrow{\\mathbb{P}} X\\) et \\(Y_n \\xrightarrow{\\mathbb{P}} Y\\), alors \\(X_n + Y_n \\xrightarrow{\\mathbb{P}} X + Y\\) : Ce résultat est faux en général pour la convergence faible (prendre \\(X\\) telle que \\(X\\) et \\(-X\\) ont la même loi, \\(X_n = Y_n\\) et \\(Y = -X\\)). Si \\(X_n \\xrightarrow{\\mathbb{P}} X\\) et \\(Y_n \\xrightarrow{\\mathbb{P}} Y\\), alors \\(X_n Y_n \\xrightarrow{\\mathbb{P}} X Y\\) 5.2 Inégalités en probabilités Théorème 5.5 Soit \\(X\\) une variable aléatoire, \\(h\\) une fonction positive telle que \\(h(X)\\) admette une espérance. Alors \\[ \\mathbb{P} \\left( h(X) \\geq a \\right) \\leq \\frac{\\mathbb{E} h(X)}{a} , \\forall a &gt; 0 \\] Preuve Soit \\(A = \\lbrace h(X) \\geq a \\rbrace\\) . Alors \\(X \\geq a \\mathbf{1}_A\\) et le résultat suit en passant à l’espérance. En particulier, on a: Théorème 5.6 (Inégalité de Markov) Soit \\(X\\) une variable aléatoire qui admet une moyenne. Alors \\[ \\mathbb{P} \\left( |X| \\geq a \\right) \\leq \\frac{\\mathbb{E} |X|}{a} , \\forall a &gt; 0 \\] Théorème 5.7 (Inégalité de Bienaymé-Tchébychev) Soit \\(X\\) une variable aléatoire qui admet une moyenne. Alors \\[ \\mathbb{P} \\left( |X|^2 \\geq a \\right) \\leq \\frac{\\mathbb{E} |X|^2}{a} , \\forall a &gt; 0 \\] Théorème 5.8 Soit \\(X\\) une variable aléatoire qui admet un moment d’ordre 2. Alors \\[ \\mathbb{P} \\left( |X - \\mathbb{E} X| \\geq a \\right) \\leq \\frac{\\mathbb{V} X}{a^2} , \\forall a &gt; 0 \\] Le théorème suivant permet de proposer une borne supérieure à \\(\\mathbb{P} \\left( |X| \\geq a \\right)\\) : Théorème 5.9 Si \\(g\\) est une fonction positive strictement croissante, alors \\[ \\mathbb{P} \\left( |X| \\geq a \\right) \\leq \\frac{\\mathbb{E} g(X)}{g(a)} , \\forall a &gt; 0 \\] Les bornes inférieures sont plus difficiles à trouver en général, mais on a le Théorème 5.10 Soit \\(h : \\mathbb{R} \\to [0,M]\\) une fonction positive à valeurs bornées. Alors \\[ \\mathbb{P} \\left( h(X) \\geq a \\right) \\leq \\frac{\\mathbb{E} h(X) - a}{M-a} , \\forall M &gt; a \\geq 0 \\] Preuve Soit \\[ A = \\lbrace h(X) \\geq a \\rbrace \\] . Alors \\[ h(X) \\leq M \\mathbf{1}_A + a \\mathbf{1}_{A^c} \\] et le résultat suit en passant à l’ espérance. Une inégalité comparable à l’inégalité de Markov, mais plus précise, est l’inégalité de Hoeffding : Théorème 5.11 (Inégalité de Hoeffding) Soient \\(Y_1, Y_2, \\ldots\\) des variables aléatoires indépendantes, d’espérance nulle et bornée (pour tout entier positif \\(i\\), il existe des réels \\(a_i, b_i\\) tels que \\(a_i \\leq Y_i \\leq b_i\\)). Soit \\(\\epsilon &gt; 0\\). Alors pour tout \\(t&gt;0\\), \\[ \\mathbb{P} \\left(\\sum_{i=1}^n Y_i \\geq \\epsilon \\right) \\leq e^{-t \\epsilon} \\prod_{i=1}^n exp \\left(\\frac{t^2 \\left(b_i^2 - a_i^2 \\right)}{8}\\right) \\] Théorème 5.12 (Inégalité de Cauchy-Schwarz) Soient \\(X,Y\\) deux variables aléatoires admettant des moments d’ordre 2, alors \\[ \\mathbb{E} XY \\leq \\sqrt{\\mathbb{E} \\left( X^2 \\right) \\mathbb{E} \\left( Y^2 \\right)} \\] Théorème 5.13 (Inégalité de Jensen) Soit \\(g\\) une fonction convexe, \\(X\\) une variable aléatoire dont l’espérance existe. Alors \\(g\\left(X\\right)\\) est encore une variable aléatoire, et \\[ \\mathbb{E} g\\left( X \\right) \\geq g \\left( \\mathbb{E} X \\right) \\] 5.3 Applications Soient \\(X_1,X_2,\\ldots\\) des variables aléatoires indépendantes identiquement distribuées. Soit \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) la moyenne empirique des \\(X_i\\). Théorème 5.14 (Loi faible des grands nombres) La moyenne empirique converge en probabilité vers \\(\\mu\\), l’espérance de \\(X_1\\). Théorème 5.15 (Loi forte des grands nombres) La moyenne empirique converge presque sûrement vers \\(\\mu\\), l’espérance de \\(X_1\\). Théorème 5.16 (Théorème central limite) Supposont que \\(X_1\\) admette une variance, notée \\(\\sigma^2\\). Alors la variable aléatoire \\(\\sqrt{n} \\frac{\\overline{X}_n - \\mu}{\\sigma}\\) converge en loi vers une variable aléatoire de loi normale centrée réduite \\(\\mathcal{N}(0,1)\\). "],
["statistique-inférentielle.html", "Chapitre 6 Statistique inférentielle 6.1 Introduction 6.2 Estimation ponctuelle 6.3 Intervalles de confiance 6.4 Tests d’hypothèses", " Chapitre 6 Statistique inférentielle 6.1 Introduction En statistiques, on est confronté au problème suivant : observant les résultats d’une expérience aléatoire \\(X_1, \\ldots,X_n\\) que peut-on dire de la loi du processus associé ? Une approche courante consiste à proposer un modèle décrivant le processus observé, et à inférer les caractéristiques de ce modèle à partir des données observées. Quand ces caractéristiques sont paramétrées par un nombre fini de paramètres, on dit qu’il s’agit d’un modèle paramétrique, sinon il s’agit d’un modèle non-paramétrique. 6.2 Estimation ponctuelle Définition 6.1 Soit \\(\\theta\\) une quantité à estimer. Un estimateur de \\(\\theta\\), noté \\(\\hat{\\theta}_n\\) est une variable aléatoire \\[ \\hat{\\theta}_n = g(X_1, \\ldots , X_n) \\] Ce paramètre n’est pas nécessairement un des paramètres qui définit la loi, mais peut en être la somme, le produit… L’idée est de construire \\(\\hat{\\theta}_n \\simeq \\theta\\) (\\(\\theta\\) étant une quantité non observable, mais fixée). Définition 6.2 Le biais de l’estimateur \\(\\hat{\\theta}_n\\) est défini par \\[\\mathbb{E} \\hat{\\theta}_n - \\theta\\]. L’espérance étant prise suivant la distribution qui a généré les données (donc avec \\(\\theta\\) observé). Lorsqu’il est nul, on dit que l’estimateur est sans biais. Lorsqu’il tend vers 0 lorsque \\(n\\) tend vers \\(+\\infty\\), on dit que l’estimateur est asymptotiquement sans biais. Il est souhaitable que l’estimateur s’approche de la valeur à estimer lorsq’il y a de plus en plus de données disponibles. Définition 6.3 On dit que l’estimateur \\(\\hat{\\theta}_n\\) est consistant s’il converge en probabilité vers \\(\\theta\\). Théorème 6.1 La qualité de la précision d’un estimateur est mesurée par son erreur quadratique moyenne \\[ \\mathbb{E} \\left[ \\hat{\\theta}_n - \\theta\\right]^2 \\] Cette quantité se décompose en \\[ \\mathbb{E} \\left[ \\hat{\\theta}_n - \\theta\\right]^2 = \\text{biais}^2(\\hat{\\theta}) + \\mathbb{V} \\hat{\\theta} \\] Théorème 6.2 Si le biais et la variance d’un estimateur converge vers 0 lorsque \\(n\\) tend vers \\(+\\infty\\), alors cet estimateur est consistent. Définition 6.4 On dit que l’estimateur \\(\\hat{\\theta}_n\\) est asymptotiquement normal si \\[ \\frac{\\hat{\\theta}_n - \\theta}{\\sqrt{\\mathbb{V} \\hat{\\theta}_n}} \\xrightarrow{\\mathcal{L}} \\mathcal{N}(0,1) \\] Il est facile de construire des estimateurs avec la méthode des moments: Définition 6.5 Soit \\(\\alpha_k(\\theta) = \\mathbb{E} X^k\\) et \\(\\hat{\\alpha}_k = \\frac{1}{n} \\sum_{i=1}^n X_i^k\\). On définit \\(\\hat{\\theta}_n\\) comme la valeur de \\(\\theta\\) qui vérifie: \\[ \\alpha_1(\\hat{\\theta}_n) = \\hat{\\alpha}_1, \\ldots , \\alpha_k(\\hat{\\theta}_n) = \\hat{\\alpha}_k \\]. Cette formule définit un système de \\(k\\) équations à \\(k\\) inconnues. Il suffit donc de choisir un \\(k\\) adapté. Cette estimateur s’appelle l’estimateur de \\(\\theta\\) selon la méthode des moments. Théorème 6.3 Soit \\(\\hat{\\theta}_n\\) l’estimateur de \\(\\theta\\) selon la méthode des moments. Alors L’estimateur existe avec une probabilité qui tend vers 1. L’estimateur est consistent. L’estimateur est asymptotiquement normal: \\[ \\sqrt{n}(\\hat{\\theta}_n - \\theta) \\xrightarrow{\\mathcal{L}} \\mathcal{N}(0,\\Sigma) \\] où \\(\\Sigma = g \\mathbb{E} YY&#39; g&#39;\\), avec \\(Y = (X, X^2,\\ldots,X^k)&#39;, g = (\\partial \\alpha^{-1}_1(\\theta) / \\partial \\theta , \\ldots , \\partial \\alpha^{-1}_k(\\theta) / \\partial \\theta)\\). 6.3 Intervalles de confiance Définition 6.6 Soit \\(\\alpha \\in ] 0 , 1 [\\). Un intervalle de confiance au niveau de confiance \\(1- \\alpha\\) pour un paramètre réel \\(\\theta\\) est un intervalle \\(\\mathcal{I}_n = \\left] a_n(X_1, \\ldots, X_n) , b_n(X_1, \\ldots, X_n) \\right[\\) tel que \\[ \\mathbb{P} \\left( \\theta \\in \\mathcal{I}_n \\right) \\geq 1-\\alpha \\] A noter : \\(\\theta\\) est fixé et \\(\\mathcal{I}_n\\) a des bornes aléatoires. Théorème 6.4 Supposons que l’estimateur \\(\\hat{\\theta}_n\\) soit asymptotiquement normal. Soit \\(\\Phi\\) la fonction de répartition de la loi normale (centrée, réduite), \\(\\alpha \\in ] 0 , 1 [\\) et \\(z_{\\alpha /2} = \\Phi^{-1}(1-\\frac12 \\alpha)\\) (ce qui est équivalent à \\(\\mathbb{P}(-z_{\\alpha /2} &lt; Z &lt; z_{\\alpha /2}) = 1- \\alpha\\) lorsque \\(Z \\sim \\mathcal{N}(0,1)\\)). Alors l’intervalle \\[\\mathcal{I}_n = \\left] \\hat{\\theta}_n - z_{\\alpha /2} \\sqrt{\\mathbb{V}\\hat{\\theta}_n } , \\hat{\\theta}_n + z_{\\alpha /2} \\sqrt{\\mathbb{V}\\hat{\\theta}_n } \\right[ \\] vérifie \\[ \\mathcal{P}(\\theta \\in \\mathcal{I}_n) \\to 1-\\alpha \\] 6.4 Tests d’hypothèses Lorsque l’on teste une hypothèse, on part d’une théorie par défaut, appelée hypothèse nulle \\(H_0\\) qui est comparée à une hypothèse alternative \\(H_a\\). On regarde si les données fournissent des éléments suffisamment convaincants pour rejeter l’hypothèse nulle. Sinon, on ne rejette pas l’hypothèse nulle. La plupart du temps, la validation de l’hypothèse équivaut l’appartenance à un région de rejet \\(R\\) pour une certaine statistique : Si \\(X\\) est une variable aléatoire, on rejette l’hypothèse nulle si \\(X \\in R\\), sinon on ne rejette pas l’hypothèse nulle. La région de rejet s’écrit \\[ \\lbrace x : T(x) &gt;c \\rbrace .\\] \\(T\\) s’appelle la statistique de test et \\(c\\) est la valeur critique. Le problème consiste donc à trouver une statistique de test et une valeur critique. 6.4.1 Exemple : le test de Wald Définition 6.7 Considérons le test de \\(\\theta = \\theta_0\\) contre l’hypothèse alternative \\(\\theta \\not= \\theta_0\\). Supposons que \\(\\hat{\\theta}\\) est asymptotiquement normal. Le test de Wald consiste à rejeter \\(H_0\\) lorsque \\(|W| &gt; z_{\\alpha / 2}\\) avec \\(W = \\frac{\\hat{\\theta} - \\theta_0}{\\sqrt{\\mathbb{V} \\hat{\\theta}}}.\\) Théorème 6.5 On a: \\[ \\mathbb{P} (|W| &gt; z_{\\alpha / 2}) \\to \\alpha.\\] "],
["statistique-descriptive.html", "A Statistique descriptive A.1 Distribution univariée A.2 Distribution bivariée A.3 Séries temporelles", " A Statistique descriptive A.1 Distribution univariée A.2 Distribution bivariée A.3 Séries temporelles "],
["techniques-de-dénombrement-en-probabilité.html", "B Techniques de dénombrement en probabilité", " B Techniques de dénombrement en probabilité "],
["espaces-lp.html", "C Espaces \\(L^p\\) C.1 Définitions", " C Espaces \\(L^p\\) C.1 Définitions "],
["échantillonage.html", "D Échantillonage", " D Échantillonage "],
["fonctions-indicatrices.html", "E Fonctions indicatrices", " E Fonctions indicatrices Théorème E.1 L’application qui a un évènement \\(A\\) associe la fonction indicatrice \\[ \\mathbf{1}_A : x \\to \\left\\lbrace \\begin{array}{ccc} 1 &amp; \\mbox{si} &amp; x\\in A\\\\ 0 &amp; \\mbox{si} &amp; x \\notin A\\\\ \\end{array}\\right. \\] est bijective. En outre, si \\(A,B\\) sont deux évènements disjoints, on a: \\[\\mathbf{1}_{A \\cup B} = \\mathbf{1}_A+ \\mathbf{1}_B.\\] Par ailleurs, \\[ \\mathbf{1}_{A^c} = 1- \\mathbf{1}_A.\\] Enfin, si \\(A,B\\) sont deux évènements, \\[ \\mathbf{1}_{A \\cap B} = \\mathbf{1}_A \\mathbf{1}_B.\\] Cet outil permet (entre autres) une démonstration immédiate de l’égalité de Poincaré (formule d’inclusion-exclusion) : Si \\(B = \\bigcup_{i=1}^n A_i\\), \\[\\mathbf{1}_B =1- \\prod_{i=1}^n (1-\\mathbf{1}_{A_i})\\] Et en développant puis en prenant l’espérance, il vient: \\[\\mathbb{P}(B) = \\sum_i \\mathbb{P}(A_i) - \\sum_{i&lt;j} \\mathbb{P}(A_i \\cap A_j)+ \\ldots \\] "]
]
