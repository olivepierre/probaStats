---
title: "Probabilités et statistiques"
author: "Pierre-Damien Olive"
date: "Mis à jour le `r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Ce document fournit des éléments en probabilités et statistiques, illustrés en R."

---

# Introduction  {-}
Le site avec la dernière version du cours est disponible sur : https://olivepierre.github.io/probaStats/

<!--chapter:end:index.Rmd-->

# (PART) Probabilités {-} 

# Évènements et probabilités

## Évènements

## Probabilités

## Probabilités conditionnelles

## Indépendance

# Variables aléatoires

## Variables aléatoires

## Variables discrètes et continues

## Vecteurs aléatoires

# Variables aléatoires discrètes

## Fonctions de masse 
```{definition}
La **fonction de masse** d'une variable aléatoire discrète $X$ est la fonction $f : \mathbb{R} \mapsto \left[0 ,1 \right]$ définie par:
$$
f(x) = \mathbb{P}(X=x) .
$$
```
Fonctions de répartitions et fonctions de masse sont reliées par les relations:
$$
F(x) = \sum_{x_i \leq x} f(x_i) ; f(x) = F(x) - F(x^{-}) .
$$

```{lemma}
Une fonction  $f : \mathbb{R} \mapsto \left[0 ,1 \right]$ est une fonction de masse si et seulement si  l'ensemble $\lbrace x : f(x) >0 \rbrace$ est dénombrable et $\sum_{i} f(x_i) = 1$, où les $x_1, \ldots, x_n, \ldots$ sont les valeurs de $x$ tel que $f(x)>0$.
```
TODO : exemples du programme

## Indépendance

```{definition}
Deux variables aléatoires discrètes $X$ et $Y$ sont **indépendantes** si les évènements $\lbrace X=x \rbrace$ et $\lbrace Y=y \rbrace$ sont indépendants pour tout $x$ et $y$ :
$$\mathbb{P}(X=x,Y=y) = \mathbb{P}(X=x) \mathbb{P}(Y=y) .$$  
On peut étendre cette définition à un ensemble de variables aléatoires $(X_i)_{i \in I}$.Dans ce cas, l'égalité ci-dessus devient:
$$ \mathbb{P}(X_i = x_i \forall i \in J) = \prod_{i \in J} \mathbb{P}(X_i = x_i)$$
pour toute partie finie $J$ de $I$
```
```{theorem}
Soient $X,Y$ deux variables aléatoires discrètes indépendantes, $f,g$ deux fonctions réelles. Alors les variables aléatoires discrètes $f(X)$ et $g(Y)$ sont encore indépendantes.
```

## Espérance

```{definition}
L'**espérance** d'une variables aléatoire discrète $X$ avec une fonction de masse $f$ est définie comme
$$ \mathbb{E} X = \sum_{x : f(x)>0} x f(x) $$
  lorsque cette somme est *absolument* convergente; la convergence absolue est requise afin que cette somme ait une même valeur quelque soit l'ordre dans lequel on somme les $x$ (cf. cours analyse sur les familles sommables).
```
En particulier, lorsque $A$ est un évènement, on a
$$\mathbb{E} \mathbf{1}_A = \mathbb{P}(A).$$


Afin de calculer les espérances de fonctions de variables aléatoires, on peut envisager de calculer la fonction de masse de cette nouvelle variable aléatoire. Cette approche est délicate et peu pertinente grâce au lemme:
```{lemma}
Soit $X$ une variable aléatoire discrète, $f$ sa fonction de masse. Soit $g$ une fonction réelle. Alors l'espérance de $g(X)$ est définie par
$$
\mathbb{E} g(X) = \sum_{x} g(x) f(x)
$$
lorsque cette somme est absolument convergente.
```


On peut ainsi calculer les moments pour tout ordre ($m_k = \mathbb{E} X^k$) ainsi que les moments centrés ($\sigma_k = \mathbb{E} (X - m_1)^k$). En particulier, on définit :
```{definition}
 Le moment d'ordre 1 s'appelle l'espérance de $X$, le moment centré d'ordre 2 de $X$ s'appelle la variance de $X$ (notée $\mathbb{V}X$) : c'est une quantité positive!
 On appelle $\sigma = \sqrt{\sigma_2}$ l'écart-type de $X$.
 ```

```{lemma}
On peut exprimer les moments centrés en fonction de moments d'ordres inférieurs ; en particulier,
$$\sigma_2 = m_2 - m_1^2 $$
```

```{theorem}
L'opérateur $ \mathbb{E} $ est une forme linéaire ( pour $ \lambda \in \mathbb{R} $,
$ X,Y $ deux variables aléatoires admettant une espérance,
$ \mathbb{E} (X+\lambda Y) = \mathbb{X} + \lambda \mathbb{E} Y $), positive ($ X \geq 0$ entraine $\mathbb{E}X \geq 0$) vérifiant $\mathbb{E}1 = 1$.
```
Par contre, il n'est pas vrai (en général) que $$ \mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . $$

```{lemma}
Si $X,Y$ sont des variables aléatoires indépendantes, alors 
$$ \mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . $$
```


```{definition}
On dit que les variables aléatoires $X,Y$ sont **non corrélées** si $$ \mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . $$
```
Ainsi, les variables indépendantes sont non-corrélées, mais l'inverse n'est pas vrai !

```{theorem}
Soient $X,Y$ deux variables aléatoires, $\lambda \in \mathbb{R}$. Alors $\mathbb{V} \lambda X = \lambda^2 \mathbb{V}X$ et $\mathbb{V}(X+Y) = \mathbb{V}X+\mathbb{V}Y$ si ces variables sont non-corrélées.
```




<!--chapter:end:01-proba.Rmd-->

# (PART) Statistiques {-}

<!-- # Modèle linéaire -->

<!-- On peut ajouter des blocs (cf https://github.com/rstudio/bookdown/blob/master/inst/examples/css/style.css +  https://github.com/rstudio/bookdown/blob/master/inst/examples/02-components.Rmd) -->

<!-- il faut penser à modifier le css -->

<!-- # Références -->

<!--chapter:end:02-stats.Rmd-->

# (APPENDIX) Annexes {-}

<!-- # Transformée de Laplace -->

<!-- # Autres annexes -->

<!--chapter:end:03-annexe.Rmd-->

<!-- `r if (knitr:::is_html_output()) ' -->
<!-- # Références {-} -->
<!-- '` -->

<!--chapter:end:09-references.Rmd-->

