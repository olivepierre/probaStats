# Variables aléatoires discrètes

## Fonctions de masse 
```{definition}
La **fonction de masse** d'une variable aléatoire discrète $X$ est la fonction $f : \mathbb{R} \mapsto \left[0 ,1 \right]$ définie par:
$$
f(x) = \mathbb{P}(X=x) .
$$
```
Fonctions de répartitions et fonctions de masse sont reliées par les relations:
$$
F(x) = \sum_{x_i \leq x} f(x_i) ; f(x) = F(x) - F(x^{-}) .
$$

```{lemma}
Une fonction  $f : \mathbb{R} \mapsto \left[0 ,1 \right]$ est une fonction de masse si et seulement si  l'ensemble $\lbrace x : f(x) >0 \rbrace$ est dénombrable et $\sum_{i} f(x_i) = 1$, où les $x_1, \ldots, x_n, \ldots$ sont les valeurs de $x$ tel que $f(x)>0$.
```

```{example}
On choisit un nombre entier entre $1$ et $n$, de façon équiprobable. Un tel choix définit une variable aléatoire discrète, de loi uniforme.
```


```{example}
On lance une pièce $n$ fois, et on tombe sur pile avec une probabilité $p$.
L'ensemble des évènements est $\Omega = \lbrace \mbox{pile}, \mbox{face} \rbrace$.
Le nombre de fois que l'on obtient "pile" est une variable aléatoire discrète dont la fonction de masse vérifie:
$$
  f(x) = \left\lbrace
\begin{array}{ccc}
0  & \mbox{si} & x\notin \lbrace 0, \ldots , n \rbrace\\
\binom{n}{k} p^k (1-p)^{n-k} & \mbox{sinon} &\\
\end{array}\right.
$$
On dit que cette variable aléatoire suit une loi binômiale (notée $\mathcal{B}(n,p)$).  
Lorsque $n=1$, on dit que la variable aléatoire suit une loi de Bernoulli (notée $\mathcal{B}(p)$). 
```
```{example}
On compte le nombre d'épreuves de Bernoulli nécessaires pour arriver sur "pile" pour la première fois. Ce nombre de lancer est une variables aléatoire discrète, à valeur dans $\mathbb{N}^*$ et de fonction de masse:
$$
f(k) = (1-p)^{k-1} p \forall k \in \mathbb{N}^*
$$
La loi correspondante s'appelle la loi géométrique, de paramètre $p$, notée $\mathcal{G}(p)$. 
```


```{example}
Une variable aléatoire prenant ses valeurs dans $\mathbb{N}$ avec une fonction de masse $$
  f(k) = \frac{\lambda^k}{k!} e^{-\lambda} \forall k \in \mathbb{N}$$
  suit une loi de Poisson de paramètre $\lambda >0$, notée $\mathcal{P}(\lambda)$.
```

```{example}
Soit $0<p<1$. On tire simultanément $n$ boules dans une urne contenant $A$ boules, dont $pA$ gagnantes et $(1-p)A$ perdantes. On compte le nombre de boules gagnantes extraites. La loi de cette variable aléatoire discrète est la loi hypergéométrique, de fonction de masse:
  $$
  f(k) = \frac{\binom{pA}{k} \binom{qA}{n-k}}{\binom{A}{n}}
  $$
```


## Indépendance

```{definition}
Deux variables aléatoires discrètes $X$ et $Y$ sont **indépendantes** si les évènements $\lbrace X=x \rbrace$ et $\lbrace Y=y \rbrace$ sont indépendants pour tout $x$ et $y$ :
$$\mathbb{P}(X=x,Y=y) = \mathbb{P}(X=x) \mathbb{P}(Y=y) .$$  
On peut étendre cette définition à un ensemble de variables aléatoires $(X_i)_{i \in I}$.Dans ce cas, l'égalité ci-dessus devient:
$$ \mathbb{P}(X_i = x_i \forall i \in J) = \prod_{i \in J} \mathbb{P}(X_i = x_i)$$
pour toute partie finie $J$ de $I$
```
```{theorem}
Soient $X,Y$ deux variables aléatoires discrètes indépendantes, $f,g$ deux fonctions réelles. Alors les variables aléatoires discrètes $f(X)$ et $g(Y)$ sont encore indépendantes.
```

## Espérance

```{definition}
L'**espérance** d'une variables aléatoire discrète $X$ avec une fonction de masse $f$ est définie comme
$$ \mathbb{E} X = \sum_{x : f(x)>0} x f(x) $$
  lorsque cette somme est *absolument* convergente; la convergence absolue est requise afin que cette somme ait une même valeur quelque soit l'ordre dans lequel on somme les $x$ (cf. cours analyse sur les familles sommables).
```
En particulier, lorsque $A$ est un évènement, on a
$$\mathbb{E} \mathbf{1}_A = \mathbb{P}(A).$$


Afin de calculer les espérances de fonctions de variables aléatoires, on peut envisager de calculer la fonction de masse de cette nouvelle variable aléatoire. Cette approche est délicate et peu pertinente grâce au lemme:
```{lemma}
Soit $X$ une variable aléatoire discrète, $f$ sa fonction de masse. Soit $g$ une fonction réelle. Alors l'espérance de $g(X)$ est définie par
$$
\mathbb{E} g(X) = \sum_{x} g(x) f(x)
$$
lorsque cette somme est absolument convergente.
```


On peut ainsi calculer les moments pour tout ordre ($m_k = \mathbb{E} X^k$) ainsi que les moments centrés ($\sigma_k = \mathbb{E} (X - m_1)^k$). En particulier, on définit :
```{definition}
 Le **moment d'ordre 1** s'appelle l'espérance de $X$, le **moment centré d'ordre 2** de $X$ s'appelle la variance de $X$ (notée $\mathbb{V}X$) : c'est une quantité positive!
 On appelle $\sigma = \sqrt{\sigma_2}$ l'écart-type de $X$.
 ```

```{lemma}
On peut exprimer les moments centrés en fonction de moments d'ordres inférieurs ; en particulier,
$$\sigma_2 = m_2 - m_1^2 $$
```

```{theorem}
L'opérateur $\mathbb{E}$ est une forme linéaire ( pour $\lambda \in \mathbb{R}$,
$X,Y$ deux variables aléatoires admettant une espérance,
$\mathbb{E} (X+\lambda Y) = \mathbb{X} + \lambda \mathbb{E} Y$), positive ($X \geq 0$ entraine $\mathbb{E}X \geq 0$) vérifiant $\mathbb{E}1 = 1$.
```
Par contre, il n'est pas vrai (en général) que 
$$\mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . $$

```{lemma}
Si $X,Y$ sont des variables aléatoires indépendantes, alors 
$$ \mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . $$
```


```{definition}
On dit que les variables aléatoires $X,Y$ sont **non corrélées** si $$ \mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . $$
```
Ainsi, les variables indépendantes sont non-corrélées, mais l'inverse n'est pas vrai !

```{theorem}
Soient $X,Y$ deux variables aléatoires, $\lambda \in \mathbb{R}$. Alors $\mathbb{V} \lambda X = \lambda^2 \mathbb{V}X$ et $\mathbb{V}(X+Y) = \mathbb{V}X+\mathbb{V}Y$ si ces variables sont non-corrélées.
```

## Cas particuliers de variables discrètes aléatoires

```{theorem}
Soit $X$ une variable aléatoire suivant une loi uniforme. Alors 
$$\mathbb{E}X = \frac{n+1}{2} , \mathbb{V}X = \frac{n^2-1}{12}$$
```

```{theorem}
Soit $X$ une variable aléatoire suivant une loi de Bernoulli. Alors 
$$\mathbb{E}X = p , \mathbb{V}X = p(1-p)$$
```

```{theorem}
Soit $X$ une variable aléatoire suivant une loi binômiale $\mathcal{B}(n,p)$. Alors 
$$\mathbb{E}X = np , \mathbb{V}X = np(1-p)$$
```

```{theorem}
Soit $X$ une variable aléatoire suivant une loi géométrique $\mathcal{G}(p)$. Alors 
$$\mathbb{E}X = \frac{1}{p} , \mathbb{V}X = \frac{1-p}{p^2}$$
```

```{theorem}
Soit $X$ une variable aléatoire suivant une loi de Poisson $\mathcal{P}(\lambda)$. Alors 
$$\mathbb{E}X = \lambda , \mathbb{V}X = \lambda$$
```

## Dépendance

```{definition}
Soient $X,Y$ deux variables aléatoires discrètes. La **fonction de répartition jointe** est définie par
$$F(x,y) = \mathbb{P}(X \leq x , Y \leq y).$$

La **fonction de masse jointe** est donnée par
$$ f(x,y) = \mathbb{P}(X=x, Y=y).$$
```

```{lemma}
Les variables aléatoires discrètes $X,Y$ sont indépendantes si et seulement si
$$
f_{X,Y}(x,y) = f_X(x) f_Y(y).  
$$
  Plus généralement, $X,Y$ sont indépendantes si et seulement si il existe des fonctions réelles $f,g$ telles que 
$$
  f_{X,Y}(x,y) = f(x) g(y).  
$$
```
A partir de la connaissance de $f_{X,Y}$, il est possible de calculer $f_X$ ou $f_Y$ grâce au théorème suivant :
```{theorem}
On a:
  $$f_X(x) = \sum_y f_{X,Y}(x,y)$$
  $$f_Y(y) = \sum_x f_{X,Y}(x,y)$$
```

Le calcul de l'espérance d'une fonction de variables aléatoires est rendu facile par le 
```{lemma}
Soit $g$ une fonction telle que $g(X,Y)$ soit une variable aléatoire. Alors
  $$\mathbb{E} g(X,Y) = \sum_{x,y} g(x,y)f_{X,Y}(x,y)$$
  si cette somme existe.
```

```{definition}
Soient $X,Y$ deux variables aléatoires. Leur **covariance** est définie par:
  $$cov(X,Y) = \mathbb{E} \left[ (X- \mathbb{E}X) (Y - \mathbb{E}Y)\right] = \mathbb{E}XY - \mathbb{E}X \mathbb{E}Y.$$
  Le **coefficient de corrélation** est :
  $$cor(X,Y) = \frac{cov(X,Y)}{\sqrt{\mathbb{V}X \mathbb{V}Y}}.$$
```

```{lemma}
Le coefficient de corrélation est en module inférieur à 1 et égal à $\pm 1$ si seulement si il existe des réels $a,b,c$ tels que $$\mathbb{P}(aX+bY+c=0)=1.$$
  
```

```{theorem}
Soit $X,Y$ deux variables aléatoires discrètes. Alors 
$$\left( \mathbb{E}XY \right)^2 \leq \left( \mathbb{E} X^2\right) \left( \mathbb{E} Y^2\right)$$
  avec égalité si et seulement si il existe des réels $\lambda,\mu$ non tous nuls tels que 
$$\mathbb{P}(\lambda X + \mu Y = 0) = 1.$$
```


## Loi conditionnelle et espérance conditionnelle

Soient $X,Y$ deux variables aléatoires discrètes.
```{definition}
La **fonction de répartion conditionnelle** de $Y$ sachant $X=x$, notée $F_{Y|X}(\bullet | x)$ est définie par 
$$
 F_{Y|X}(y | x) = \mathbb{P}(Y \leq y | X=x)
$$ pour tout $x$ tel que $\mathbb{P}(X=x)>0$.

La **fonction de masse conditionnelle** de $Y$ sachant $X=x$, notée $f_{Y|X}(\bullet | x)$ est définie par 
$$
 f_{Y|X}(y | x) = \mathbb{P}(Y = y | X=x)
$$ pour tout $x$ tel que $\mathbb{P}(X=x)>0$.
```
On a donc
$$
 f_{Y|X} = f_{X,Y} / f_X
 $$
 lorsque cette expression est définie. Encore, $f_{Y|X} = f_Y$ si et seulement si $X$ et $Y$ sont indépendantes.
 
 ```{definition}
 **L'espérance conditionnelle** de $Y$ par rapport à $X$ est définie comme
 $$\mathbb{E}(Y | X=x) = \sum_y y f_{Y|X}(y | x)$$
 ```

```{theorem}
On a :
$$\mathbb{E} Y = \mathbb{E} \left( \mathbb{E}(Y | X)\right)$$
 ```

```{theorem}
Pour toute fonction $g$ pour laquelle les sommes sont définies, on a :
  $$\mathbb{E} \left(Y g(X)\right) = \mathbb{E} \left( g(X) \mathbb{E}(Y | X)\right)$$
```
```{definition}
La **variance conditionnelle** de $Y$ sachant $X$ est définie par
$$\mathbb{V}(Y | X) = \mathbb{E} \left( [Y- \mathbb{E}(Y | X)]^2  | X\right).$$
```
La variance conditionnelle est liée à l'espérance conditionnelle par le théorème :

```{theorem}
$$\mathbb{V}(Y)=\mathbb{E}(\mathbb{V}[Y| X])+\mathbb{V}(\mathbb{E}[Y|X])$$\mathbb{V}
```

## Somme de variables aléatoires discrètes
Soient $X,Y$ deux variables aléatoires discrètes.
```{theorem}
La loi de $X+Y$ est définie par
$$
  \mathbb{P}(X+Y=z) = \sum_x f_{X,Y} (x,z-x)
$$
Si, en outre, $X$ et $Y$ sont indépendantes, alors
$$
  \mathbb{P}(X+Y=z) = \sum_x f_X(x) f_Y(z-x) = \sum_y f_X(z-y) f_Y(y)
$$
La fonction masse de $X+Y$ s'appelle alors le **produit de convolution** des fonctions masses de $X$ et $Y$, et est noté $f_X \star f_Y$. 
```
