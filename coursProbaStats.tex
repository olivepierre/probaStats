\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Probabilités et statistiques},
            pdfauthor={Pierre-Damien Olive},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Probabilités et statistiques}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Pierre-Damien Olive}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Mis à jour le 2019-10-17}

\usepackage{booktabs}

\usepackage{amsthm}
\newtheorem{theorem}{Théorème}[chapter]
\newtheorem{lemma}{Lemme}[chapter]
\newtheorem{corollary}{Corollaire}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Définition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Exemple}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercice}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remarque }
\newtheorem*{solution}{Solution }
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Le site avec la dernière version du cours est disponible sur : \url{https://olivepierre.github.io/probaStats/}

\hypertarget{part-probabilituxe9s}{%
\part{Probabilités}\label{part-probabilituxe9s}}

\hypertarget{uxe9vuxe8nements-et-probabilituxe9s}{%
\chapter{Évènements et probabilités}\label{uxe9vuxe8nements-et-probabilituxe9s}}

\hypertarget{uxe9vuxe8nements}{%
\section{Évènements}\label{uxe9vuxe8nements}}

\hypertarget{probabilituxe9s}{%
\section{Probabilités}\label{probabilituxe9s}}

\hypertarget{probabilituxe9s-conditionnelles}{%
\section{Probabilités conditionnelles}\label{probabilituxe9s-conditionnelles}}

\hypertarget{induxe9pendance}{%
\section{Indépendance}\label{induxe9pendance}}

\hypertarget{variables-aluxe9atoires}{%
\chapter{Variables aléatoires}\label{variables-aluxe9atoires}}

\hypertarget{variables-aluxe9atoires-1}{%
\section{Variables aléatoires}\label{variables-aluxe9atoires-1}}

\hypertarget{variables-discruxe8tes-et-continues}{%
\section{Variables discrètes et continues}\label{variables-discruxe8tes-et-continues}}

\hypertarget{vecteurs-aluxe9atoires}{%
\section{Vecteurs aléatoires}\label{vecteurs-aluxe9atoires}}

\hypertarget{variables-aluxe9atoires-discruxe8tes}{%
\chapter{Variables aléatoires discrètes}\label{variables-aluxe9atoires-discruxe8tes}}

\hypertarget{fonctions-de-masse}{%
\section{Fonctions de masse}\label{fonctions-de-masse}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-1}{}{\label{def:unnamed-chunk-1} }La \textbf{fonction de masse} d'une variable aléatoire discrète \(X\) est la fonction \(f : \mathbb{R} \mapsto \left[0 ,1 \right]\) définie par:
\[
f(x) = \mathbb{P}(X=x) .
\]
\EndKnitrBlock{definition}

Fonctions de répartitions et fonctions de masse sont reliées par les relations:
\[
F(x) = \sum_{x_i \leq x} f(x_i) ; f(x) = F(x) - F(x^{-}) .
\]

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-2}{}{\label{lem:unnamed-chunk-2} }Une fonction \(f : \mathbb{R} \mapsto \left[0 ,1 \right]\) est une fonction de masse si et seulement si l'ensemble \(\lbrace x : f(x) >0 \rbrace\) est dénombrable et \(\sum_{i} f(x_i) = 1\), où les \(x_1, \ldots, x_n, \ldots\) sont les valeurs de \(x\) tel que \(f(x)>0\).
\EndKnitrBlock{lemma}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-3}{}{\label{exm:unnamed-chunk-3} }On choisit un nombre entier entre \(1\) et \(n\), de façon équiprobable. Un tel choix définit une variable aléatoire discrète, de loi uniforme.
\EndKnitrBlock{example}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-4}{}{\label{exm:unnamed-chunk-4} }On lance une pièce \(n\) fois, et on tombe sur pile avec une probabilité \(p\).
L'ensemble des évènements est \(\Omega = \lbrace \mbox{pile}, \mbox{face} \rbrace\).
Le nombre de fois que l'on obtient ``pile'' est une variable aléatoire discrète dont la fonction de masse vérifie:
\[
  f(x) = \left\lbrace
\begin{array}{ccc}
0  & \mbox{si} & x\notin \lbrace 0, \ldots , n \rbrace\\
\binom{n}{k} p^k (1-p)^{n-k} & \mbox{sinon} &\\
\end{array}\right.
\]
On dit que cette variable aléatoire suit une loi binômiale (notée \(\mathcal{B}(n,p)\)).\\
Lorsque \(n=1\), on dit que la variable aléatoire suit une loi de Bernoulli (notée \(\mathcal{B}(p)\)).
\EndKnitrBlock{example}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-5}{}{\label{exm:unnamed-chunk-5} }On compte le nombre d'épreuves de Bernoulli nécessaires pour arriver sur ``pile'' pour la première fois. Ce nombre de lancer est une variables aléatoire discrète, à valeur dans \(\mathbb{N}^*\) et de fonction de masse:
\[
f(k) = (1-p)^{k-1} p \forall k \in \mathbb{N}^*
\]
La loi correspondante s'appelle la loi géométrique, de paramètre \(p\), notée \(\mathcal{G}(p)\).
\EndKnitrBlock{example}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-6}{}{\label{exm:unnamed-chunk-6} }Une variable aléatoire prenant ses valeurs dans \(\mathbb{N}\) avec une fonction de masse \[
  f(k) = \frac{\lambda^k}{k!} e^{-\lambda} \forall k \in \mathbb{N}\]
suit une loi de Poisson de paramètre \(\lambda >0\), notée \(\mathcal{P}(\lambda)\).
\EndKnitrBlock{example}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-7}{}{\label{exm:unnamed-chunk-7} }Soit \(0<p<1\). On tire simultanément \(n\) boules dans une urne contenant \(A\) boules, dont \(pA\) gagnantes et \((1-p)A\) perdantes. On compte le nombre de boules gagnantes extraites. La loi de cette variable aléatoire discrète est la loi hypergéométrique, de fonction de masse:
\[
  f(k) = \frac{\binom{pA}{k} \binom{qA}{n-k}}{\binom{A}{n}}
  \]
\EndKnitrBlock{example}

\hypertarget{induxe9pendance-1}{%
\section{Indépendance}\label{induxe9pendance-1}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-8}{}{\label{def:unnamed-chunk-8} }Deux variables aléatoires discrètes \(X\) et \(Y\) sont \textbf{indépendantes} si les évènements \(\lbrace X=x \rbrace\) et \(\lbrace Y=y \rbrace\) sont indépendants pour tout \(x\) et \(y\) :
\[\mathbb{P}(X=x,Y=y) = \mathbb{P}(X=x) \mathbb{P}(Y=y) .\]\\
On peut étendre cette définition à un ensemble de variables aléatoires \((X_i)_{i \in I}\).Dans ce cas, l'égalité ci-dessus devient:
\[ \mathbb{P}(X_i = x_i \forall i \in J) = \prod_{i \in J} \mathbb{P}(X_i = x_i)\]
pour toute partie finie \(J\) de \(I\)
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-9}{}{\label{thm:unnamed-chunk-9} }Soient \(X,Y\) deux variables aléatoires discrètes indépendantes, \(f,g\) deux fonctions réelles. Alors les variables aléatoires discrètes \(f(X)\) et \(g(Y)\) sont encore indépendantes.
\EndKnitrBlock{theorem}

\hypertarget{espuxe9rance}{%
\section{Espérance}\label{espuxe9rance}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-10}{}{\label{def:unnamed-chunk-10} }L'\textbf{espérance} d'une variables aléatoire discrète \(X\) avec une fonction de masse \(f\) est définie comme
\[ \mathbb{E} X = \sum_{x : f(x)>0} x f(x) \]
lorsque cette somme est \emph{absolument} convergente; la convergence absolue est requise afin que cette somme ait une même valeur quelque soit l'ordre dans lequel on somme les \(x\) (cf.~cours analyse sur les familles sommables).
\EndKnitrBlock{definition}

En particulier, lorsque \(A\) est un évènement, on a
\[\mathbb{E} \mathbf{1}_A = \mathbb{P}(A).\]

Afin de calculer les espérances de fonctions de variables aléatoires, on peut envisager de calculer la fonction de masse de cette nouvelle variable aléatoire. Cette approche est délicate et peu pertinente grâce au lemme:
\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-11}{}{\label{lem:unnamed-chunk-11} }Soit \(X\) une variable aléatoire discrète, \(f\) sa fonction de masse. Soit \(g\) une fonction réelle. Alors l'espérance de \(g(X)\) est définie par
\[
\mathbb{E} g(X) = \sum_{x} g(x) f(x)
\]
lorsque cette somme est absolument convergente.
\EndKnitrBlock{lemma}

On peut ainsi calculer les moments pour tout ordre (\(m_k = \mathbb{E} X^k\)) ainsi que les moments centrés (\(\sigma_k = \mathbb{E} (X - m_1)^k\)). En particulier, on définit :
\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-12}{}{\label{def:unnamed-chunk-12} } Le \textbf{moment d'ordre 1} s'appelle l'espérance de \(X\), le \textbf{moment centré d'ordre 2} de \(X\) s'appelle la variance de \(X\) (notée \(\mathbb{V}X\)) : c'est une quantité positive!
On appelle \(\sigma = \sqrt{\sigma_2}\) l'écart-type de \(X\).
\EndKnitrBlock{definition}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-13}{}{\label{lem:unnamed-chunk-13} }On peut exprimer les moments centrés en fonction de moments d'ordres inférieurs ; en particulier,
\[\sigma_2 = m_2 - m_1^2 \]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-14}{}{\label{thm:unnamed-chunk-14} }L'opérateur \(\mathbb{E}\) est une forme linéaire ( pour \(\lambda \in \mathbb{R}\),
\(X,Y\) deux variables aléatoires admettant une espérance,
\(\mathbb{E} (X+\lambda Y) = \mathbb{X} + \lambda \mathbb{E} Y\)), positive (\(X \geq 0\) entraine \(\mathbb{E}X \geq 0\)) vérifiant \(\mathbb{E}1 = 1\).
\EndKnitrBlock{theorem}

Par contre, il n'est pas vrai (en général) que
\[\mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . \]

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-15}{}{\label{lem:unnamed-chunk-15} }Si \(X,Y\) sont des variables aléatoires indépendantes, alors
\[ \mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . \]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-16}{}{\label{def:unnamed-chunk-16} }On dit que les variables aléatoires \(X,Y\) sont \textbf{non corrélées} si \[ \mathbb{E} XY = (\mathbb{E}X) ( \mathbb{E} Y) . \]
\EndKnitrBlock{definition}

Ainsi, les variables indépendantes sont non-corrélées, mais l'inverse n'est pas vrai !

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-17}{}{\label{thm:unnamed-chunk-17} }Soient \(X,Y\) deux variables aléatoires, \(\lambda \in \mathbb{R}\). Alors \(\mathbb{V} \lambda X = \lambda^2 \mathbb{V}X\) et \(\mathbb{V}(X+Y) = \mathbb{V}X+\mathbb{V}Y\) si ces variables sont non-corrélées.
\EndKnitrBlock{theorem}

\hypertarget{cas-particuliers-de-variables-discruxe8tes-aluxe9atoires}{%
\section{Cas particuliers de variables discrètes aléatoires}\label{cas-particuliers-de-variables-discruxe8tes-aluxe9atoires}}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-18}{}{\label{thm:unnamed-chunk-18} }Soit \(X\) une variable aléatoire suivant une loi uniforme. Alors
\[\mathbb{E}X = \frac{n+1}{2} , \mathbb{V}X = \frac{n^2-1}{12}\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-19}{}{\label{thm:unnamed-chunk-19} }Soit \(X\) une variable aléatoire suivant une loi de Bernoulli. Alors
\[\mathbb{E}X = p , \mathbb{V}X = p(1-p)\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-20}{}{\label{thm:unnamed-chunk-20} }Soit \(X\) une variable aléatoire suivant une loi binômiale \(\mathcal{B}(n,p)\). Alors
\[\mathbb{E}X = np , \mathbb{V}X = np(1-p)\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-21}{}{\label{thm:unnamed-chunk-21} }Soit \(X\) une variable aléatoire suivant une loi géométrique \(\mathcal{G}(p)\). Alors
\[\mathbb{E}X = \frac{1}{p} , \mathbb{V}X = \frac{1-p}{p^2}\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-22}{}{\label{thm:unnamed-chunk-22} }Soit \(X\) une variable aléatoire suivant une loi de Poisson \(\mathcal{P}(\lambda)\). Alors
\[\mathbb{E}X = \lambda , \mathbb{V}X = \lambda\]
\EndKnitrBlock{theorem}

\hypertarget{duxe9pendance}{%
\section{Dépendance}\label{duxe9pendance}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-23}{}{\label{def:unnamed-chunk-23} }Soient \(X,Y\) deux variables aléatoires discrètes. La \textbf{fonction de répartition jointe} est définie par
\[F(x,y) = \mathbb{P}(X \leq x , Y \leq y).\]

La \textbf{fonction de masse jointe} est donnée par
\[ f(x,y) = \mathbb{P}(X=x, Y=y).\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-24}{}{\label{lem:unnamed-chunk-24} }Les variables aléatoires discrètes \(X,Y\) sont indépendantes si et seulement si
\[
f_{X,Y}(x,y) = f_X(x) f_Y(y).  
\]
Plus généralement, \(X,Y\) sont indépendantes si et seulement si il existe des fonctions réelles \(f,g\) telles que
\[
  f_{X,Y}(x,y) = f(x) g(y).  
\]
\EndKnitrBlock{lemma}

A partir de la connaissance de \(f_{X,Y}\), il est possible de calculer \(f_X\) ou \(f_Y\) grâce au théorème suivant :
\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-25}{}{\label{thm:unnamed-chunk-25} }On a:
\[f_X(x) = \sum_y f_{X,Y}(x,y)\]
\[f_Y(y) = \sum_x f_{X,Y}(x,y)\]
\EndKnitrBlock{theorem}

Le calcul de l'espérance d'une fonction de variables aléatoires est rendu facile par le
\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-26}{}{\label{lem:unnamed-chunk-26} }Soit \(g\) une fonction telle que \(g(X,Y)\) soit une variable aléatoire. Alors
\[\mathbb{E} g(X,Y) = \sum_{x,y} g(x,y)f_{X,Y}(x,y)\]
si cette somme existe.
\EndKnitrBlock{lemma}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-27}{}{\label{def:unnamed-chunk-27} }Soient \(X,Y\) deux variables aléatoires. Leur \textbf{covariance} est définie par:
\[cov(X,Y) = \mathbb{E} \left[ (X- \mathbb{E}X) (Y - \mathbb{E}Y)\right] = \mathbb{E}XY - \mathbb{E}X \mathbb{E}Y.\]
Le \textbf{coefficient de corrélation} est :
\[cor(X,Y) = \frac{cov(X,Y)}{\sqrt{\mathbb{V}X \mathbb{V}Y}}.\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-28}{}{\label{lem:unnamed-chunk-28} }Le coefficient de corrélation est en module inférieur à 1 et égal à \(\pm 1\) si seulement si il existe des réels \(a,b,c\) tels que \[\mathbb{P}(aX+bY+c=0)=1.\]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-29}{}{\label{thm:unnamed-chunk-29} }Soit \(X,Y\) deux variables aléatoires discrètes. Alors
\[\left( \mathbb{E}XY \right)^2 \leq \left( \mathbb{E} X^2\right) \left( \mathbb{E} Y^2\right)\]
avec égalité si et seulement si il existe des réels \(\lambda,\mu\) non tous nuls tels que
\[\mathbb{P}(\lambda X + \mu Y = 0) = 1.\]
\EndKnitrBlock{theorem}

\hypertarget{loi-conditionnelle-et-espuxe9rance-conditionnelle}{%
\section{Loi conditionnelle et espérance conditionnelle}\label{loi-conditionnelle-et-espuxe9rance-conditionnelle}}

Soient \(X,Y\) deux variables aléatoires discrètes.
\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-30}{}{\label{def:unnamed-chunk-30} }La \textbf{fonction de répartion conditionnelle} de \(Y\) sachant \(X=x\), notée \(F_{Y|X}(\bullet | x)\) est définie par
\[
 F_{Y|X}(y | x) = \mathbb{P}(Y \leq y | X=x)
\] pour tout \(x\) tel que \(\mathbb{P}(X=x)>0\).

La \textbf{fonction de masse conditionnelle} de \(Y\) sachant \(X=x\), notée \(f_{Y|X}(\bullet | x)\) est définie par
\[
 f_{Y|X}(y | x) = \mathbb{P}(Y = y | X=x)
\] pour tout \(x\) tel que \(\mathbb{P}(X=x)>0\).
\EndKnitrBlock{definition}

On a donc
\[
 f_{Y|X} = f_{X,Y} / f_X
 \]
lorsque cette expression est définie. Encore, \(f_{Y|X} = f_Y\) si et seulement si \(X\) et \(Y\) sont indépendantes.

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-31}{}{\label{def:unnamed-chunk-31} }\textbf{L'espérance conditionnelle} de \(Y\) par rapport à \(X\) est définie comme
\[\mathbb{E}(Y | X=x) = \sum_y y f_{Y|X}(y | x)\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-32}{}{\label{thm:unnamed-chunk-32} }On a :
\[\mathbb{E} Y = \mathbb{E} \left( \mathbb{E}(Y | X)\right)\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-33}{}{\label{thm:unnamed-chunk-33} }Pour toute fonction \(g\) pour laquelle les sommes sont définies, on a :
\[\mathbb{E} \left(Y g(X)\right) = \mathbb{E} \left( g(X) \mathbb{E}(Y | X)\right)\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-34}{}{\label{def:unnamed-chunk-34} }La \textbf{variance conditionnelle} de \(Y\) sachant \(X\) est définie par
\[\mathbb{V}(Y | X) = \mathbb{E} \left( [Y- \mathbb{E}(Y | X)]^2  | X\right).\]
\EndKnitrBlock{definition}

La variance conditionnelle est liée à l'espérance conditionnelle par le théorème :

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-35}{}{\label{thm:unnamed-chunk-35} }\[\mathbb{V}(Y)=\mathbb{E}(\mathbb{V}[Y| X])+\mathbb{V}(\mathbb{E}[Y|X])\]
\EndKnitrBlock{theorem}

\hypertarget{somme-de-variables-aluxe9atoires-discruxe8tes}{%
\section{Somme de variables aléatoires discrètes}\label{somme-de-variables-aluxe9atoires-discruxe8tes}}

Soient \(X,Y\) deux variables aléatoires discrètes.
\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-36}{}{\label{thm:unnamed-chunk-36} }La loi de \(X+Y\) est définie par
\[
  \mathbb{P}(X+Y=z) = \sum_x f_{X,Y} (x,z-x)
\]
Si, en outre, \(X\) et \(Y\) sont indépendantes, alors
\[
  \mathbb{P}(X+Y=z) = \sum_x f_X(x) f_Y(z-x) = \sum_y f_X(z-y) f_Y(y)
\]
La fonction masse de \(X+Y\) s'appelle alors le \textbf{produit de convolution} des fonctions masses de \(X\) et \(Y\), et est noté \(f_X \star f_Y\).
\EndKnitrBlock{theorem}

\hypertarget{variables-aluxe9atoires-continues}{%
\chapter{Variables aléatoires continues}\label{variables-aluxe9atoires-continues}}

\hypertarget{densituxe9s-de-probabilituxe9s}{%
\section{densités de probabilités}\label{densituxe9s-de-probabilituxe9s}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-37}{}{\label{def:unnamed-chunk-37} }Une variable aléatoire réelle \(X\) est dite \textbf{continue} s'il existe une fonction f positive telle que
\[F_X(x) = \int_{-\infty}^x f(t) \mbox{d}t\]
Une telle fonction est une \textbf{densité} de \(X\). Cette densité n'est pas unique.
Une telle densité est généralement notée \(f_X\).

Lorsque \(F_X\) est dérivable, \(F_X'\) est une densité de \(X\).
\EndKnitrBlock{definition}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-38}{}{\label{lem:unnamed-chunk-38} }Si \(X\) admet \(f\) comme densité, alors:

\begin{itemize}
\item
  \(\int_{-\infty}^{+\infty} f(x) \mbox{d}x = 1\);
\item
  \(\mathbb{P}(X=x)=0, \forall x \in \mathbb{R}\);
\item
  \(\mathbb{P}(X \in \mathcal{E}) = \int_{\mathcal{E}} f(x) \mbox{d}x\) pour tout évènement \(\mathcal{E}\);
\item
  \(X\) et \(-X\) ont les mêmes distributions si et seulement si \(f_X\) est paire;
\item
  Si \(f\) et \(g\) sont des densités, il en est de même pour \(\lambda f + (1-\lambda)g\) pour \(0 \leq \lambda \leq 1\).
\end{itemize}
\EndKnitrBlock{lemma}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-39}{}{\label{exm:unnamed-chunk-39} }Soient \(a,b\) deux réels avec \(a<b\). La \textbf{loi uniforme} sur l'intervalle \([a,b]\) est définie par sa densité de probabilité
\[
f(x) = \left\lbrace
\begin{array}{ccc}
\frac{1}{b-a}  & \mbox{si} & a \leq x \leq b\\
0 & \mbox{sinon} &\\
\end{array}\right.
\]
On note cette loi \(\mathcal{U}(a,b)\).
\EndKnitrBlock{example}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-40}{}{\label{exm:unnamed-chunk-40} }La \textbf{loi exponentielle} modélise la durée de vie d'un phénomène sans mémoire : la probabilité que le phénomène dure au moins \(s+t\) sachant qu'il a déjà duré \(t\) est la même que la probabilié de durer \(s\) à partir de la mise en fonction initiale.

Soit \(\lambda > 0\). La densité associée à cette loi est :
\[
f(t) = \left\lbrace
\begin{array}{ccc}
\lambda e^{-\lambda t}  & \mbox{si} & t>0\\
0 & \mbox{sinon} &\\
\end{array}\right.
\]
On note la loi exponentielle de paramètre \(\lambda\) : \(\mathcal{E}(\lambda)\).
\EndKnitrBlock{example}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-41}{}{\label{lem:unnamed-chunk-41} }Soient \(X,Y\) deux variables aléatoires qui suivent deux lois exponentielles, de paramètres \(\lambda, \mu\), alors \(\min (X,Y)\) suit encore une loi exponentielle, de paramètre \(\lambda + \mu\).
\EndKnitrBlock{lemma}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-42}{}{\label{exm:unnamed-chunk-42} }La loi \textbf{Gamma} (qui généralise la loi exponentielle) modélise également des durées; elle est caractérisée par deux paramètres \(k,\theta\) positifs.

Sa densité est
\[ f(t) = \left\lbrace
\begin{array}{ccc}
\frac{t^{k-1} \exp \left(-\frac{t}{\theta} \right)}{\theta^k \Gamma (k)}   & \mbox{si} & x>0\\
0 & \mbox{sinon} & \\
\end{array}\right.
\]
où \(\Gamma\) désigne la fonction Gamma d'Euler (\(\Gamma : z \mapsto \int_0^{+\infty} t^{z-1} e^{-t} \mbox{d}t\)).

On note cette loi \(\Gamma(k,\theta)\).
\EndKnitrBlock{example}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-43}{}{\label{lem:unnamed-chunk-43} }Soient \(X_1, \ldots, X_n\) des variables aléatoires indépendantes, de loi \(\Gamma(k_1,\theta),\ldots,\Gamma(k_n,\theta)\). Alors \(X_1+\ldots+X_n\) suit une loi \(\Gamma(\sum_{i=1}^n k_i,\theta)\).
\EndKnitrBlock{lemma}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-44}{}{\label{lem:unnamed-chunk-44} }Soit \(X \sim \Gamma(k,\theta)\) et \(t>0\). Alors \(tX \sim \Gamma(k,t \theta)\).
\EndKnitrBlock{lemma}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-45}{}{\label{exm:unnamed-chunk-45} }La \textbf{loi normale} est définie suivant deux paramètres (\(\mu \in \mathbb{R}, \sigma >0\)) par sa densité:

\[
  f(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)
  \]
On note la loi \(\mathcal{N}(\mu,\sigma^2)\).
\EndKnitrBlock{example}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-46}{}{\label{exm:unnamed-chunk-46} }Soit \(X\) une variable aléatoire qui suit une loi normale \(\mathcal{N}(\mu,\sigma^2)\). Alors \(\exp(X)\) suit une loi \textbf{log-normale}.
\EndKnitrBlock{example}

\hypertarget{induxe9pendance-2}{%
\section{Indépendance}\label{induxe9pendance-2}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-47}{}{\label{def:unnamed-chunk-47} }Soient \(X,Y\) deux variables aléatoires réelles.
On dit que les variables aléatoires \(X\) et \(Y\) sont \textbf{indépendantes} si les évènements \(\lbrace X \leq x \rbrace\) et \(\lbrace Y \leq y \rbrace\) sont indépendants pour tous \(x,y\) réels.
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-48}{}{\label{thm:unnamed-chunk-48} }Soient \(X,Y\) deux variables aléatoires, \(f,g\) deux fonctions telles que \(f(X), g(Y)\) soient des variables aléatoires.

Si \(X\) et \(Y\) sont indépendantes, il en est de même pour \(f(X)\) et \(g(Y)\).```

\hypertarget{espuxe9rance-1}{%
\section{Espérance}\label{espuxe9rance-1}}
\EndKnitrBlock{theorem}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-49}{}{\label{def:unnamed-chunk-49} }\textbf{L'espérance} d'une variable aléatoire \(X\) de densité \(f\) est définie par
\[\mathbb{E}X = \int_{-\infty}^{+\infty} x f(x) \mbox{d}x\]
lorsque cette intégrale existe.
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-50}{}{\label{thm:unnamed-chunk-50} }Si \(X\) et \(g(X)\) sont des variables aléatoires, alors
\[\mathbb{E}g(X) = \int_{-\infty}^{+\infty} g(x) f(x) \mbox{d}x\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-51}{}{\label{lem:unnamed-chunk-51} }Si \(X\) admet une densité nulle sur \(\mathbb{R}^-\) et une fonction de répartition \(F\), alors
\[\mathbb{E}X = \int_0^{+\infty} (1-F(x)) \mbox{d}x = \int_0^{+\infty} \mathbb{P}(X>x) \mbox{d}x\]
\EndKnitrBlock{lemma}

Les autres propriétés de l'espérance, vues dans le chapitre sur les variables aléatoires discrètes, s'étendent sans difficultés aux variables continues (moments, moments centrés, variance, covariance, corrélation).

\hypertarget{exemples-de-variables-aluxe9atoires-continues}{%
\section{Exemples de variables aléatoires continues}\label{exemples-de-variables-aluxe9atoires-continues}}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-52}{}{\label{thm:unnamed-chunk-52} }L'espérance et la variance des lois continues de référence sont recensés dans le tableau ci-après :

\begin{longtable}[]{@{}ccc@{}}
\toprule
Loi & Espérance & Variance\tabularnewline
\midrule
\endhead
\(\mathcal{U}(a,b)\) & \(\frac{a+b}{2}\) & \(\frac{(b-a)^2}{12}\)\tabularnewline
\(\mathcal{E}(\lambda)\) & \(\frac{1}{\lambda}\) & \(\frac{1}{\lambda^2}\)\tabularnewline
\(\Gamma(k,\theta)\) & \(k \theta\) & \(k \theta^2\)\tabularnewline
\(\mathcal{N}(\mu,\sigma^2)\) & \(\mu\) & \(\sigma^2\)\tabularnewline
\bottomrule
\end{longtable}
\EndKnitrBlock{theorem}

\hypertarget{duxe9pendance-1}{%
\section{Dépendance}\label{duxe9pendance-1}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-53}{}{\label{def:unnamed-chunk-53} }Soient \(X,Y\) deux variables aléatoires réelles. Ces variables sont \textbf{conjointement continues} s'il existe une fonction positive \(f\) telle que:

\[F(x,y) = \mathbb{P}(X \leq x , Y \leq y) = \int_{-\infty}^x \int_{-\infty}^y f(u,v) \mbox{d}u \mbox{d}v\]
f est une densité du couple \((X,Y)\). Si \(F\) est dérivable, on peut prendre
\[f = \frac{\partial^2 F}{\partial x \partial y}\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-54}{}{\label{lem:unnamed-chunk-54} }On a
\[
  \mathbb{P}(a \leq X \leq b ; c \leq Y \leq d) = F(b,d) - F(a,d) - F(b,c)+F(a,c)
  = \int_{a}^b \int_{c}^d f(u,v) \mbox{d}u \mbox{d}v
\]
Plus généralement, si \(\mathcal{E}\) est un évènement, on a:

\[
  \mathbb{P}\left( (X,Y) \in \mathcal{E} \right) = \iint_{\mathcal{E}} f(u,v) \mbox{d}u \mbox{d}v
  \]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-55}{}{\label{def:unnamed-chunk-55} }Les \textbf{fonctions de répartition marginales} de \(X\) et \(Y\) sont définies par
\[
  F_X(x) = \mathbb{P}(X  \leq x) = \int_{-\infty}^x \int_{-\infty}^{+\infty} f(u,v) \mbox{d}u \mbox{d}v ; F_Y(y) = \mathbb{P}(Y  \leq y) = \int_{-\infty}^{+\infty} \int_{-\infty}^y  f(u,v) \mbox{d}u \mbox{d}v 
  \]
On en déduit les \textbf{densités marginales} selon \(X\) et \(Y\) :
\[
  f_X(x) = \int_{-\infty}^{+\infty} f(x,y) \mbox{d}y ; f_Y(y) = \int_{-\infty}^{+\infty} f(x,y) \mbox{d}x
  \]
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-56}{}{\label{thm:unnamed-chunk-56} }Soit \(g : \mathbb{R}^2 \to \mathbb{R}\) telle que \(g(X,Y)\) soit une variable aléatoire. Alors,
\[
  \mathbb{E} g(X,Y) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(u,v) f(u,v) \mbox{d}u \mbox{d}v
  \] dès que cette intégrale est définie.
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-57}{}{\label{thm:unnamed-chunk-57} }Les variables \(X,Y\) sont indépendantes si et seulement si
\[F_{X,Y} = F_X F_Y \iff f_{X,Y} = f_x f_y.\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-58}{}{\label{thm:unnamed-chunk-58} }Soit \(X,Y\) deux variables aléatoires continues. Alors
\[\left( \mathbb{E}XY \right)^2 \leq \left( \mathbb{E} X^2\right) \left( \mathbb{E} Y^2\right)\]
avec égalité si et seulement si il existe des réels \(\lambda,\mu\) non tous nuls tels que
\[\mathbb{P}(\lambda X + \mu Y = 0) = 1.\]
\EndKnitrBlock{theorem}

\hypertarget{loi-conditionnelle-et-espuxe9rance-conditionnelle-1}{%
\section{Loi conditionnelle et espérance conditionnelle}\label{loi-conditionnelle-et-espuxe9rance-conditionnelle-1}}

Soient \(X,Y\) deux variables aléatoires continues, de densité jointe \(f\).
\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-59}{}{\label{def:unnamed-chunk-59} }La \textbf{fonction de répartition conditionnelle} de \(Y\) sachant \(X=x\) est définie par

\[
F_{Y|X}(y|x) = \int_{-\infty}^y \frac{f(x,v)}{f_X(x)} \mbox{d}v  
\] pour tout \(x\) tel que \(f_X(x)>0\). Elle est également notée
\[\mathbb{P}(Y \leq y | X = x)\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-60}{}{\label{def:unnamed-chunk-60} }La \textbf{densité conditionnelle} de \(Y\) sachant \(X=x\) est définie par

\[
f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)} 
\] pour tout \(x\) tel que \(f_X(x)>0\). Encore,
\[
  f_{Y|X} = \frac{f_{X,Y}}{f_X} 
\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-61}{}{\label{thm:unnamed-chunk-61} }\textbf{L'espérance conditionnelle} de \(Y\) sachant \(X\) est définie comme
\[\mathbb{E}(Y|X=x) = \int_{-\infty}^{+\infty} y f_{Y|X}(y|x) \mbox{d}y ;\]
Elle vérifie la relation
\[
\mathbb{E} \left(\mathbb{E}(Y|X) \right) = \mathbb{E} Y .
\]
Plus généralement, pour toute fonction \(g\) pour laquelle l'expression a un sens, on a :
\[
\mathbb{E}  \left(g(X) \mathbb{E}(Y|X) \right) = \mathbb{E} Y g(X) .
\]
\EndKnitrBlock{theorem}

\hypertarget{fonctions-de-variables-aluxe9atoires}{%
\section{Fonctions de variables aléatoires}\label{fonctions-de-variables-aluxe9atoires}}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-62}{}{\label{thm:unnamed-chunk-62} }Soit \(X\) une variable aléatoire avec une densité \(f\), et soit \(g\) une fonction réelle telle que \(Y = g(X)\) soit encore une variable aléatoire.
La fonction de répartition de \(Y\) est définie par :
\[
  \mathbb{P}(Y \leq y) = \mathbb{P} \left(X \in g^{-1}(]-\infty , y]) \right) .
  \]
\EndKnitrBlock{theorem}

Plus généralement, on a le théorème suivant:
\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-63}{}{\label{thm:unnamed-chunk-63} }Soient \(\mathcal{U}, \mathcal{V}\) deux ouverts de \(\mathbb{R}^d\),
Soit \(X\) une variable aléatoire à valeurs dans \(\mathcal{U}\) de densité \(f\) et soit \(T\) un difféomorphisme de classe \(\mathcal{C}^1 : \mathcal {U} \to \mathcal{V}\).
La densité de la variable aléatoire \(Y = \varphi(X)\) est définie par :

\[
f_Y(y) = \left\lbrace
\begin{array}{ccc}
f_X(\varphi^{-1}(y)) |J_{\varphi^{-1}}(y)|  & \mbox{si} & y \in \mathcal{V}\\
0 & \mbox{sinon} &\\
\end{array}\right.
\]
en notant \(|J_{\varphi^{-1}}(y)|\) le déterminant du jacobien de \(\varphi^{-1}\), c'est-à-dire de la matrice de terme général \(\left(\frac{\partial \varphi^{-1}_j}{ \partial y_i} \right)_{i,j}\).
\EndKnitrBlock{theorem}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-64}{}{\label{lem:unnamed-chunk-64} }Pour \(d=2\), le changement de variables ``passage en coordonnées polaires'' transforme une densité \(f(x,y)\) en \(f(r \cos \theta, r \sin \theta) r\).
\EndKnitrBlock{lemma}

\hypertarget{somme-de-variables-aluxe9atoires}{%
\section{Somme de variables aléatoires}\label{somme-de-variables-aluxe9atoires}}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-65}{}{\label{thm:unnamed-chunk-65} }Si \(X,Y\) ont pour densité jointe \(f\), alors la densité de \(Z = X+Y\) est définie par:

\[
  f_Z(z) = \int_{-\infty}^{+\infty} f(x,z-x) \mbox{d}x
  \]
Si, \(X\) et \(Y\) sont indépendantes, le résultat devient :
\[
  f_Z(z) = \int_{-\infty}^{+\infty} f_X(z-y) f_Y(y) \mbox{d}y = \int_{-\infty}^{+\infty} f_X(x) f_Y(z-x) \mbox{d}x
  \]
On dit que \(f_Z\) est le \textbf{produit de convolution} de \(f_X\) et \(f_Y\), noté
\(f_X \star f_Y\).
\EndKnitrBlock{theorem}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-66}{}{\label{lem:unnamed-chunk-66} }Soit \(X \sim \mathcal{N}(\mu_1,\sigma_1^2)\) et \(Y \sim \mathcal{N}(\mu_2,\sigma_2^2)\). Supposons que \(X\) et \(Y\) sont indépendantes. Alors
\(X+Y \sim \mathcal{N}(\mu_1 + \mu_2,\sigma_1^2 + \sigma_2^2)\).
\EndKnitrBlock{lemma}

\hypertarget{la-loi-normale-multivariuxe9e}{%
\section{La loi normale multivariée}\label{la-loi-normale-multivariuxe9e}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-67}{}{\label{def:unnamed-chunk-67} }Le vecteur \(X = (X_1, \ldots, X_n)\) suit une \textbf{loi normale multivariée} (notée \(\mathcal{N}(\mu,\Sigma)\)) s'il existe un vecteur \(\mu = (\mu_1,\ldots,\mu_n)\) de réels et une matrice symétrique définie positive \(\Sigma\) telle que la densité jointe de \(X\) soit:

\[f_X(x) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp \left( 
-\frac12 (x-\mu)\Sigma^{-1}(x-\mu)'
\right)\]

De façon équivalente, on dit que le vecteur \(X\) suit une loi normale multivariée si et seulement si pour tout vecteur \(a \in \mathbb{R}^n\), \(Xa'\) suit une loi normale.
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-68}{}{\label{thm:unnamed-chunk-68} }Si \(X\) suit une loi \(\mathcal{N}(\mu,\Sigma)\), alors

\begin{itemize}
\item
  \(\mathbb{E}X = \mu\)
\item
  la matrice \(\Sigma\) est la matrice de covariance de \(X\):
  \[\Sigma_{i,j} = \text{cov} (X_i,X_j)\]
\item
  Si \(A\) est une matrice de rang \(m \leq n\), alors \(XA\) suit la loi \(\mathcal{N}(\mu A,A' \Sigma A)\)
\end{itemize}
\EndKnitrBlock{theorem}

\hypertarget{lois-issues-de-la-loi-normale}{%
\section{Lois issues de la loi normale}\label{lois-issues-de-la-loi-normale}}

On dispose de variables aléatoires correspondants aux résultats d'un même expérience aléatoire \(X_1,\ldots,X_n\) dont on suppose qu'elle sont de loi normale \(\mathcal{N}(\mu,\sigma^2)\) pour des quantités \(\mu,\sigma\) non observée. On cherche à estimer ces quantités. Pour celà, on va considérer la moyenne empirique
\[\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i\]
comme estimateur de \(\mu\) et la variance empirique
\[S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2\]
comme estimateur de \(\sigma^2\).

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-69}{}{\label{thm:unnamed-chunk-69} }Soit \(X_1,\ldots,X_n\) des variables aléatoires indépendantes, de même loi \(\mathcal{N}(\mu,\sigma^2)\).

\begin{itemize}
\item
  \(\overline{X}\) et \(S^2\) sont indépendants;
\item
  \(\overline{X} \sim \mathcal{N}(\mu,\sigma^2 / n)\)
\item
  \((n-1)S^2/\sigma^2 \sim \Gamma(\frac12, \frac{n-1}{2})\). Cette dernière loi s'appelle la \textbf{loi du \(\chi^2\) à \(n-1\) degrés de liberté}, notée \(\chi^2(n-1)\) ;
  c'est la loi de la somme des carrés de \(n-1\) lois normales centrées réduites indépendantes.
\item
  Les variables aléatoires
  \[ U = \frac{n-1}{\sigma^2}S^2 \sim \chi^2(n-1)\] et
  \[V = \frac{\sqrt{n}}{\sigma}(\overline{X}-\mu) \sim \mathcal{N}(0,1).\]
  Elles ne dépendent pas de \(\sigma\). Le ratio
  \[T = \frac{V}{\sqrt{U/(n-1)}}\]
  dont le numérateur suit \(\mathcal{N}(0,1)\) et le dénominateur est la racine d'un \(\chi^2(n-1)\), divisé par \(n-1\) suit une \textbf{loi de Student}, de paramètre \(n-1\), notée \(t(n-1)\).
\item
  Soient \(U,V\) deux variables aléatoires qui suivent respectivement les lois \(\chi^2(r)\) et \(\chi^2(s)\), alors
  \[F = \frac{U/r}{V/s}\]
  suit une \textbf{loi de Fisher} de paramètres \((r,s)\), notée \(F(r,s)\). Dans ce cas, \(1/F\) suit la loi \(F(s,r)\) et si \(T \sim t(r)\), \(T^2 \sim F(1,r)\)
\end{itemize}
\EndKnitrBlock{theorem}

\hypertarget{convergence-de-variables-aluxe9atoires}{%
\chapter{Convergence de variables aléatoires}\label{convergence-de-variables-aluxe9atoires}}

\hypertarget{modes-de-convergence}{%
\section{Modes de convergence}\label{modes-de-convergence}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-70}{}{\label{def:unnamed-chunk-70} }Soient \(X, X_1, \ldots, X_n\) des variables aléatoires. On dit que :

\begin{itemize}
\tightlist
\item
  \(X_n \to X\) \textbf{presque sûrement} (noté \(X_n \xrightarrow{p.s} X\)) si
  \[ \mathbb{P}\left( \left\{ \omega \in \Omega : X_n (\omega) \xrightarrow[n \to +\infty]{} X (\omega) \right\} \right) = 1
    \]
\item
  \(X_n \to X\) \textbf{dans \(L^p\) (\(p \geq 1\))} (noté \(X_n \xrightarrow{L^p} X\)) si \(\mathbb{E} |X_n|^p < + \infty\) et
  \[ \mathbb{E} |X_n - X|^p \xrightarrow[n \to +\infty]{} 0
    \]
\item
  \(X_n \to X\) \textbf{en probabilité} (noté \(X_n \xrightarrow{\mathbb{P}} X\)) si
  \[ \mathbb{P} \left( |X_n-X|> \epsilon \right) \xrightarrow[n \to +\infty]{} 0 \forall \epsilon >0
    \]
\item
  \(X_n \to X\) \textbf{en loi}, ou \textbf{converge faiblement} (noté \(X_n \xrightarrow{\mathcal{L}} X\)) si
  \[ \mathbb{P} \left(  X_n  \leq x \right) \xrightarrow[n \to +\infty]{} \mathbb{P} \left(  X \leq x \right) \text{pour tous les $x$ où la fonction $F_X$ est continue.}
    \] Notons que si \(X_n \xrightarrow{\mathcal{L}} X\), \(X_n \xrightarrow{\mathcal{L}} Y\) pour tout \(Y\) avec la même loi que \(X\).
\end{itemize}
\EndKnitrBlock{definition}

En particulier, on a les résultats suivants pour la convergence faible.

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-71}{}{\label{thm:unnamed-chunk-71} }Si \(X_n \xrightarrow{\mathcal{L}} X\) et \(g\) est une fonction continue, alors \(g(X_n) \xrightarrow{\mathcal{L}} g(X)\). Par ailleurs,

on a les équivalences :

\begin{itemize}
\tightlist
\item
  \(X_n \xrightarrow{\mathcal{L}} X\)
\item
  \(\mathbb{E} g\left(X_n\right) \to \mathbb{E} g\left(X\right)\) pour toute fonction \(g\) continue bornée
\item
  \(\mathbb{E} g\left(X_n\right) \to \mathbb{E} g\left(X\right)\) pour toute fonction \(g\) de la forme \(g(x) = f(x) \mathbb{1}_{[a,b]}(x)\) où \(f\) est continue sur \([a,b]\) et \(a,b\) sont des points de continuité de la fonction de répartition de \(X\).
\end{itemize}
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-72}{}{\label{thm:unnamed-chunk-72} }Si \(X_n \xrightarrow{\mathbb{P}} X\) et \(g\) est une fonction continue, alors \(g(X_n) \xrightarrow{\mathbb{P}} g(X)\)
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-73}{}{\label{thm:unnamed-chunk-73} }Ces modes de convergence ne sont pas équivalents:

\begin{itemize}
\tightlist
\item
  \(\left( X_n \xrightarrow{p.s} X \right) \Rightarrow \left( X_n \xrightarrow{\mathbb{P}} X \right)\)
\item
  \(\left( X_n \xrightarrow{L^p} X \right) \Rightarrow \left( X_n \xrightarrow{\mathbb{P}} X \right)\)
\item
  \(\left( X_n \xrightarrow{\mathbb{P}} X \right) \Rightarrow \left( X_n \xrightarrow{\mathcal{L}} X \right)\)
\end{itemize}

Les implications inverses sont fausses en général.

De plus, si \(q > p \geq 1\), alors
\[
  \left( X_n \xrightarrow{L^q} X \right) \Rightarrow \left( X_n \xrightarrow{L^p} X \right)
  \]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-74}{}{\label{thm:unnamed-chunk-74} }On a :

\begin{itemize}
\tightlist
\item
  Si \(X_n \xrightarrow{p.s} X\) et \(Y_n \xrightarrow{p.s} Y\), alors \(X_n + Y_n \xrightarrow{p.s} X + Y\)
\item
  Si \(X_n \xrightarrow{L^p} X\) et \(Y_n \xrightarrow{L^p} Y\), alors \(X_n + Y_n \xrightarrow{L^p} X + Y\)
\item
  Si \(X_n \xrightarrow{\mathbb{P}} X\) et \(Y_n \xrightarrow{\mathbb{P}} Y\), alors \(X_n + Y_n \xrightarrow{\mathbb{P}} X + Y\) : Ce résultat est faux en général pour la convergence faible (prendre \(X\) telle que \(X\) et \(-X\) ont la même loi, \(X_n = Y_n\) et \(Y = -X\)).
\item
  Si \(X_n \xrightarrow{\mathbb{P}} X\) et \(Y_n \xrightarrow{\mathbb{P}} Y\), alors \(X_n Y_n \xrightarrow{\mathbb{P}} X Y\)\\
\end{itemize}
\EndKnitrBlock{theorem}

\hypertarget{inuxe9galituxe9s-en-probabilituxe9s}{%
\section{Inégalités en probabilités}\label{inuxe9galituxe9s-en-probabilituxe9s}}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-75}{}{\label{thm:unnamed-chunk-75} }Soit \(X\) une variable aléatoire, \(h\) une fonction positive telle que \(h(X)\) admette une espérance.
Alors
\[
  \mathbb{P} \left( h(X) \geq a \right) \leq \frac{\mathbb{E} h(X)}{a} , \forall a > 0
\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}
\iffalse{} {Preuve } \fi{}
Soit \(A = \lbrace h(X) \geq a \rbrace\) .

Alors \(X \geq a \mathbf{1}_A\) et le résultat suit en passant à l'espérance.
\EndKnitrBlock{proof}

En particulier, on a:

\BeginKnitrBlock{theorem}[Inégalité de Markov]
\protect\hypertarget{thm:unnamed-chunk-77}{}{\label{thm:unnamed-chunk-77} \iffalse (Inégalité de Markov) \fi{} }Soit \(X\) une variable aléatoire qui admet une moyenne. Alors
\[
  \mathbb{P} \left( |X| \geq a \right) \leq \frac{\mathbb{E} |X|}{a} , \forall a > 0
\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}[Inégalité de Bienaymé-Tchébychev]
\protect\hypertarget{thm:unnamed-chunk-78}{}{\label{thm:unnamed-chunk-78} \iffalse (Inégalité de Bienaymé-Tchébychev) \fi{} }Soit \(X\) une variable aléatoire qui admet une moyenne. Alors
\[
  \mathbb{P} \left( |X|^2 \geq a \right) \leq \frac{\mathbb{E} |X|^2}{a} , \forall a > 0
\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-79}{}{\label{thm:unnamed-chunk-79} }Soit \(X\) une variable aléatoire qui admet un moment d'ordre 2. Alors
\[
  \mathbb{P} \left( |X - \mathbb{E} X| \geq a \right) \leq \frac{\mathbb{V} X}{a^2} , \forall a > 0
\]
\EndKnitrBlock{theorem}

Le théorème suivant permet de proposer une borne supérieure à \(\mathbb{P} \left( |X| \geq a \right)\) :
\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-80}{}{\label{thm:unnamed-chunk-80} }Si \(g\) est une fonction positive strictement croissante, alors
\[
  \mathbb{P} \left( |X| \geq a \right) \leq \frac{\mathbb{E} g(X)}{g(a)} , \forall a > 0
  \]
\EndKnitrBlock{theorem}

Les bornes inférieures sont plus difficiles à trouver en général, mais on a le

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-81}{}{\label{thm:unnamed-chunk-81} }Soit \(h : \mathbb{R} \to [0,M]\) une fonction positive à valeurs bornées.
Alors
\[
  \mathbb{P} \left( h(X) \geq a \right) \leq \frac{\mathbb{E} h(X) - a}{M-a} , \forall  M > a \geq 0
  \]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}
\iffalse{} {Preuve } \fi{}
Soit \[ A = \lbrace h(X) \geq a \rbrace \] .

Alors \[ h(X) \leq M \mathbf{1}_A + a \mathbf{1}_{A^c} \] et le résultat suit en passant à l' espérance.
\EndKnitrBlock{proof}

Une inégalité comparable à l'inégalité de Markov, mais plus précise, est l'inégalité de Hoeffding :

\BeginKnitrBlock{theorem}[Inégalité de Hoeffding]
\protect\hypertarget{thm:unnamed-chunk-83}{}{\label{thm:unnamed-chunk-83} \iffalse (Inégalité de Hoeffding) \fi{} }Soient \(Y_1, Y_2, \ldots\) des variables aléatoires indépendantes, d'espérance nulle et bornée (pour tout entier positif \(i\), il existe des réels \(a_i, b_i\) tels que \(a_i \leq Y_i \leq b_i\)). Soit \(\epsilon > 0\). Alors pour tout \(t>0\),

\[
\mathbb{P} \left(\sum_{i=1}^n Y_i \geq \epsilon \right) \leq e^{-t \epsilon} \prod_{i=1}^n exp \left(\frac{t^2 \left(b_i^2 - a_i^2 \right)}{8}\right)
\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}[Inégalité de Cauchy-Schwarz]
\protect\hypertarget{thm:unnamed-chunk-84}{}{\label{thm:unnamed-chunk-84} \iffalse (Inégalité de Cauchy-Schwarz) \fi{} }Soient \(X,Y\) deux variables aléatoires admettant des moments d'ordre 2, alors
\[
\mathbb{E} XY \leq \sqrt{\mathbb{E} \left( X^2 \right) \mathbb{E} \left( Y^2 \right)}
\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}[Inégalité de Jensen]
\protect\hypertarget{thm:unnamed-chunk-85}{}{\label{thm:unnamed-chunk-85} \iffalse (Inégalité de Jensen) \fi{} }Soit \(g\) une fonction convexe, \(X\) une variable aléatoire dont l'espérance existe.
Alors \(g\left(X\right)\) est encore une variable aléatoire, et
\[
\mathbb{E} g\left( X \right) \geq g \left( \mathbb{E} X \right)
\]
\EndKnitrBlock{theorem}

\hypertarget{applications}{%
\section{Applications}\label{applications}}

Soient \(X_1,X_2,\ldots\) des variables aléatoires indépendantes identiquement distribuées.
Soit \(\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\) la \emph{moyenne empirique} des \(X_i\).

\BeginKnitrBlock{theorem}[Loi faible des grands nombres]
\protect\hypertarget{thm:unnamed-chunk-86}{}{\label{thm:unnamed-chunk-86} \iffalse (Loi faible des grands nombres) \fi{} }La moyenne empirique converge \emph{en probabilité} vers \(\mu\), l'espérance de \(X_1\).
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}[Loi forte des grands nombres]
\protect\hypertarget{thm:unnamed-chunk-87}{}{\label{thm:unnamed-chunk-87} \iffalse (Loi forte des grands nombres) \fi{} }La moyenne empirique converge \emph{presque sûrement} vers \(\mu\), l'espérance de \(X_1\).
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}[Théorème central limite]
\protect\hypertarget{thm:unnamed-chunk-88}{}{\label{thm:unnamed-chunk-88} \iffalse (Théorème central limite) \fi{} }Supposont que \(X_1\) admette une variance, notée \(\sigma^2\).
Alors la variable aléatoire \(\sqrt{n} \frac{\overline{X}_n - \mu}{\sigma}\) converge \emph{en loi} vers une variable aléatoire de loi normale centrée réduite \(\mathcal{N}(0,1)\).
\EndKnitrBlock{theorem}

\hypertarget{part-statistiques}{%
\part{Statistiques}\label{part-statistiques}}

\hypertarget{statistique-infuxe9rentielle}{%
\chapter{Statistique inférentielle}\label{statistique-infuxe9rentielle}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

En statistiques, on est confronté au problème suivant : observant les résultats d'une expérience aléatoire \(X_1, \ldots,X_n\) que peut-on dire de la loi du processus associé ?

Une approche courante consiste à proposer un modèle décrivant le processus observé, et à inférer les caractéristiques de ce modèle à partir des données observées. Quand ces caractéristiques sont paramétrées par un nombre fini de paramètres, on dit qu'il s'agit d'un \emph{modèle paramétrique}, sinon il s'agit d'un \emph{modèle non-paramétrique}.

\hypertarget{estimation-ponctuelle}{%
\section{Estimation ponctuelle}\label{estimation-ponctuelle}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-89}{}{\label{def:unnamed-chunk-89} }Soit \(\theta\) une quantité à estimer. Un estimateur de \(\theta\), noté \(\hat{\theta}_n\) est une variable aléatoire
\[
  \hat{\theta}_n = g(X_1, \ldots , X_n)
  \]
\EndKnitrBlock{definition}

Ce paramètre n'est pas nécessairement un des paramètres qui définit la loi, mais peut en être la somme, le produit\ldots{}

L'idée est de construire \(\hat{\theta}_n \simeq \theta\) (\(\theta\) étant une quantité non observable, mais fixée).

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-90}{}{\label{def:unnamed-chunk-90} }Le biais de l'estimateur \(\hat{\theta}_n\) est défini par
\[\mathbb{E} \hat{\theta}_n - \theta\].
L'espérance étant prise suivant la distribution qui a généré les données (donc avec \(\theta\) observé).
Lorsqu'il est nul, on dit que l'estimateur est \emph{sans biais}. Lorsqu'il tend vers 0 lorsque \(n\) tend vers \(+\infty\), on dit que l'estimateur est \emph{asymptotiquement sans biais}.
\EndKnitrBlock{definition}

Il est souhaitable que l'estimateur s'approche de la valeur à estimer lorsq'il y a de plus en plus de données disponibles.

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-91}{}{\label{def:unnamed-chunk-91} }On dit que l'estimateur \(\hat{\theta}_n\) est \emph{consistant} s'il converge en probabilité vers \(\theta\).
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-92}{}{\label{thm:unnamed-chunk-92} }La qualité de la précision d'un estimateur est mesurée par son \emph{erreur quadratique moyenne}
\[
\mathbb{E} \left[ \hat{\theta}_n - \theta\right]^2
\]
Cette quantité se décompose en
\[
\mathbb{E} \left[ \hat{\theta}_n - \theta\right]^2 = \text{biais}^2(\hat{\theta}) + \mathbb{V} \hat{\theta}
\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-93}{}{\label{thm:unnamed-chunk-93} }Si le biais et la variance d'un estimateur converge vers 0 lorsque \(n\) tend vers \(+\infty\), alors cet estimateur est consistent.
\EndKnitrBlock{theorem}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-94}{}{\label{def:unnamed-chunk-94} }On dit que l'estimateur \(\hat{\theta}_n\) est \emph{asymptotiquement normal} si
\[
\frac{\hat{\theta}_n - \theta}{\sqrt{\mathbb{V} \hat{\theta}_n}} \xrightarrow{\mathcal{L}} \mathcal{N}(0,1)
\]
\EndKnitrBlock{definition}

Il est facile de construire des estimateurs avec la méthode des moments:
\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-95}{}{\label{def:unnamed-chunk-95} }Soit \(\alpha_k(\theta) = \mathbb{E} X^k\) et \(\hat{\alpha}_k = \frac{1}{n} \sum_{i=1}^n X_i^k\).

On définit \(\hat{\theta}_n\) comme la valeur de \(\theta\) qui vérifie:
\[
  \alpha_1(\hat{\theta}_n) = \hat{\alpha}_1, \ldots , \alpha_k(\hat{\theta}_n) = \hat{\alpha}_k
  \].
Cette formule définit un système de \(k\) équations à \(k\) inconnues. Il suffit donc de choisir un \(k\) adapté.
Cette estimateur s'appelle l'estimateur de \(\theta\) selon \emph{la méthode des moments}.
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-96}{}{\label{thm:unnamed-chunk-96} }Soit \(\hat{\theta}_n\) l'estimateur de \(\theta\) selon la méthode des moments.
Alors

\begin{itemize}
\tightlist
\item
  L'estimateur existe avec une probabilité qui tend vers 1.
\item
  L'estimateur est consistent.
\item
  L'estimateur est asymptotiquement normal:
  \[
  \sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{\mathcal{L}} \mathcal{N}(0,\Sigma)
  \]
  où \(\Sigma = g \mathbb{E} YY' g'\), avec \(Y = (X, X^2,\ldots,X^k)', g = (\partial \alpha^{-1}_1(\theta) / \partial \theta , \ldots , \partial \alpha^{-1}_k(\theta) / \partial \theta)\).

  \EndKnitrBlock{theorem}
\end{itemize}

\hypertarget{intervalles-de-confiance}{%
\section{Intervalles de confiance}\label{intervalles-de-confiance}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-97}{}{\label{def:unnamed-chunk-97} }Soit \(\alpha \in ] 0 , 1 [\). Un \emph{intervalle de confiance au niveau de confiance \(1- \alpha\)} pour un paramètre réel \(\theta\) est un intervalle \(\mathcal{I}_n = \left] a_n(X_1, \ldots, X_n) , b_n(X_1, \ldots, X_n) \right[\) tel que
\[
\mathbb{P} \left( \theta \in \mathcal{I}_n \right) \geq 1-\alpha 
\]
A noter : \(\theta\) est fixé et \(\mathcal{I}_n\) a des bornes aléatoires.
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-98}{}{\label{thm:unnamed-chunk-98} }Supposons que l'estimateur \(\hat{\theta}_n\) soit asymptotiquement normal. Soit \(\Phi\) la fonction de répartition de la loi normale (centrée, réduite), \(\alpha \in ] 0 , 1 [\) et \(z_{\alpha /2} = \Phi^{-1}(1-\frac12 \alpha)\) (ce qui est équivalent à \(\mathbb{P}(-z_{\alpha /2} < Z < z_{\alpha /2}) = 1- \alpha\) lorsque \(Z \sim \mathcal{N}(0,1)\)).
Alors l'intervalle
\[\mathcal{I}_n = \left]
\hat{\theta}_n - z_{\alpha /2} \sqrt{\mathbb{V}\hat{\theta}_n }
,
\hat{\theta}_n + z_{\alpha /2} \sqrt{\mathbb{V}\hat{\theta}_n }
\right[
  \]
vérifie
\[
  \mathcal{P}(\theta \in \mathcal{I}_n) \to 1-\alpha
  \]
\EndKnitrBlock{theorem}

\hypertarget{tests-dhypothuxe8ses}{%
\section{Tests d'hypothèses}\label{tests-dhypothuxe8ses}}

Lorsque l'on teste une hypothèse, on part d'une théorie par défaut, appelée \emph{hypothèse nulle} \(H_0\) qui est comparée à une \emph{hypothèse alternative} \(H_a\). On regarde si les données fournissent des éléments suffisamment convaincants pour \emph{rejeter l'hypothèse nulle}. Sinon, \emph{on ne rejette pas} l'hypothèse nulle.

La plupart du temps, la validation de l'hypothèse équivaut l'appartenance à un région de rejet \(R\) pour une certaine statistique : Si \(X\) est une variable aléatoire, on rejette l'hypothèse nulle si \(X \in R\), sinon on ne rejette pas l'hypothèse nulle.

La région de rejet s'écrit \[ \lbrace x : T(x) >c \rbrace .\]
\(T\) s'appelle la \emph{statistique de test} et \(c\) est la valeur critique. Le problème consiste donc à trouver une statistique de test et une valeur critique.

\hypertarget{exemple-le-test-de-wald}{%
\subsection{Exemple : le test de Wald}\label{exemple-le-test-de-wald}}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-99}{}{\label{def:unnamed-chunk-99} }Considérons le test de \(\theta = \theta_0\) contre l'hypothèse alternative \(\theta \not= \theta_0\).
Supposons que \(\hat{\theta}\) est asymptotiquement normal. Le test de Wald consiste à rejeter \(H_0\) lorsque \(|W| > z_{\alpha / 2}\) avec
\(W = \frac{\hat{\theta} - \theta_0}{\sqrt{\mathbb{V} \hat{\theta}}}.\)
\EndKnitrBlock{definition}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-100}{}{\label{thm:unnamed-chunk-100} }On a:
\[ \mathbb{P} (|W| > z_{\alpha / 2}) \to \alpha.\]
\EndKnitrBlock{theorem}

\hypertarget{appendix-annexes}{%
\appendix}


\hypertarget{statistique-descriptive}{%
\chapter{Statistique descriptive}\label{statistique-descriptive}}

\hypertarget{distribution-univariuxe9e}{%
\section{Distribution univariée}\label{distribution-univariuxe9e}}

\hypertarget{distribution-bivariuxe9e}{%
\section{Distribution bivariée}\label{distribution-bivariuxe9e}}

\hypertarget{suxe9ries-temporelles}{%
\section{Séries temporelles}\label{suxe9ries-temporelles}}

\hypertarget{techniques-de-duxe9nombrement-en-probabilituxe9}{%
\chapter{Techniques de dénombrement en probabilité}\label{techniques-de-duxe9nombrement-en-probabilituxe9}}

\hypertarget{espaces-lp}{%
\chapter{\texorpdfstring{Espaces \(L^p\)}{Espaces L\^{}p}}\label{espaces-lp}}

\hypertarget{duxe9finitions}{%
\section{Définitions}\label{duxe9finitions}}

\hypertarget{uxe9chantillonage}{%
\chapter{Échantillonage}\label{uxe9chantillonage}}

\hypertarget{fonctions-indicatrices}{%
\chapter{Fonctions indicatrices}\label{fonctions-indicatrices}}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-101}{}{\label{thm:unnamed-chunk-101} }L'application qui a un évènement \(A\) associe la fonction indicatrice \[
\mathbf{1}_A : x \to  
\left\lbrace
\begin{array}{ccc}
1  & \mbox{si} & x\in A\\
0 & \mbox{si} & x \notin A\\
\end{array}\right.
\]
est bijective.
En outre, si \(A,B\) sont deux évènements disjoints, on a:
\[\mathbf{1}_{A \cup B} = \mathbf{1}_A+ \mathbf{1}_B.\]
Par ailleurs,
\[ \mathbf{1}_{A^c} = 1- \mathbf{1}_A.\]

Enfin, si \(A,B\) sont deux évènements, \[
\mathbf{1}_{A \cap B} = \mathbf{1}_A \mathbf{1}_B.\]
\EndKnitrBlock{theorem}

Cet outil permet (entre autres) une démonstration immédiate de l'égalité de Poincaré (formule d'inclusion-exclusion) :
Si \(B = \bigcup_{i=1}^n A_i\),
\[\mathbf{1}_B =1- \prod_{i=1}^n (1-\mathbf{1}_{A_i})\]
Et en développant puis en prenant l'espérance, il vient:
\[\mathbb{P}(B) = \sum_i \mathbb{P}(A_i) - \sum_{i<j} \mathbb{P}(A_i \cap A_j)+  \ldots  \]

\bibliography{book.bib,packages.bib}


\end{document}
